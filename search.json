[
  {
    "objectID": "lecture_notes/becoming_backprop_ninja.html",
    "href": "lecture_notes/becoming_backprop_ninja.html",
    "title": "Lesson 5: Becoming a Backprop Ninja",
    "section": "",
    "text": "swole doge style"
  },
  {
    "objectID": "lecture_notes/becoming_backprop_ninja.html#starter-code",
    "href": "lecture_notes/becoming_backprop_ninja.html#starter-code",
    "title": "Lesson 5: Becoming a Backprop Ninja",
    "section": "Starter code",
    "text": "Starter code\n\nBolierplate\n\nImports\n\nimport random\nimport numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n\n\nRead in all the words\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nBuild the vocabulary of characters and mappings to/from integers\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\nBuild the dataset\n\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n    \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\n\nrandom.seed(2)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1]) # 80%\nXdev, Ydev = build_dataset(words[n1: n2]) # 10%\nXte, Yte = build_dataset(words[n2:]) # 10%\n\ntorch.Size([182481, 3]) torch.Size([182481])\ntorch.Size([22849, 3]) torch.Size([22849])\ntorch.Size([22816, 3]) torch.Size([22816])\n\n\nok boiler done, now get to the action\n\n\n\nLets get started\n\nUtility function\n\nTo compare manual gradients to PyTorch gradients\n\n\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n\n\n\nInitialize the layers\n\nn_embd = 10 # the dimensionality for the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((vocab_size, n_embd),             generator = g)\n\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size) ** 0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 \n# using b1 just for fun, it's useless because of batchnorm\n\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\nbnbias = torch.randn((1, n_hidden)) * 0.1\n\n# Note: The parameters here are initialized in non-standard ways\n# because sometimes initializing with e.g. all zeros could mask an incorrect\n# implementation of the backward pass\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad =True\n\n4137\n\n\n\n\nConstruct a minibatch\n\nbs = 32; n = bs\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (bs,) , generator = g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n\n\n\nAn Epoch\n\nForward Pass\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n\n# Linear layer 1\nhprebn = embcat @ W1 + b1\n\n# Batchnorm layer\nbnmeani = 1/n * hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff ** 2\nbnvar = 1/(n - 1) * (bndiff2).sum(0, keepdim=True) # note: bessel's correction (dividing by n - 1, not n)\nbnvar_inv = (bnvar + 1e-5) ** -0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n\n# Non linearity\nh = torch.tanh(hpreact)\n\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n\n# cross entropy loss ( same as F.cross_entropy(logits, Yb) )\nlogit_maxes = logits.max(1, keepdim = True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum ** -1 # if I use (1.0 / counts_sum ) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters: p.grad = None\n\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no clear way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n          embcat, emb]:\n    t.retain_grad()\nloss.backward()\nloss\n\ntensor(3.4738, grad_fn=&lt;NegBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/becoming_backprop_ninja.html#exercise-1",
    "href": "lecture_notes/becoming_backprop_ninja.html#exercise-1",
    "title": "Lesson 5: Becoming a Backprop Ninja",
    "section": "Exercise 1:",
    "text": "Exercise 1:\n\nbackprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one\n\n\nC.shape, Xb.shape, emb.shape\n\n(torch.Size([27, 10]), torch.Size([32, 3]), torch.Size([32, 3, 10]))\n\n\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ncmp('logprobs', dlogprobs, logprobs)\n\ndprobs = torch.zeros_like(probs)\ndprobs = (1/probs) * dlogprobs\ncmp('probs', dprobs, probs)\n\n\ndcounts_sum_inv =  (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n\ndcounts_sum = (-1 * counts_sum ** -2) * dcounts_sum_inv\ncmp('counts_sum', dcounts_sum, counts_sum)\n\ndcounts += dcounts_sum\ncmp('counts', dcounts, counts)\n\ndnorm_logits = norm_logits.exp() * dcounts\ncmp('norm_logits', dnorm_logits, norm_logits)\n\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\n\ndlogits_temp = torch.zeros_like(logits)\ndlogits_temp[range(n), torch.argmax(logits, 1)] = 1\ndlogits += dlogits_temp * dlogit_maxes\ncmp('logits', dlogits, logits)\n\ndh = dlogits @ W2.T\ncmp('h', dh, h)\n\ndW2 = h.T @ dlogits\ncmp('W2', dW2, W2)\n\ndb2 = dlogits.sum(0)\ncmp('b2', db2, b2)\n\ndhpreact = ((1 - h ** 2) * dh)\ncmp('hpreact', dhpreact, hpreact)\n\ndbngain = (bnraw * dhpreact).sum(0, keepdims=True)\ncmp('bngain', dbngain, bngain)\n\ndbnbias = dhpreact.sum(0, keepdims=True)\ncmp('bnbias', dbnbias, bnbias)\n\ndbnraw = bngain * dhpreact\ncmp('bnraw', dbnraw, bnraw)\n\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n\ndbndiff = bnvar_inv * dbnraw\ncmp('bndiff', dbndiff, bndiff)\n\ndbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\ncmp('bnvar', dbnvar, bnvar)\n\ndbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\ncmp('bndiff2', dbndiff2, bndiff2)\n\ndbndiff += (2 * bndiff) * dbndiff2\ncmp('bndiff', dbndiff, bndiff)\n\ndbnmeani = (- 1 * dbndiff).sum(0, keepdims=True) \ncmp('bnmeani', dbnmeani, bnmeani)\n\ndhprebn = dbndiff\ndhprebn += 1/n * (torch.ones_like(hprebn)) * dbnmeani\ncmp('hprebn', dhprebn, hprebn)\n\ndembcat = dhprebn @ W1.T\ncmp('embcat', dembcat, embcat)\n\ndW1 = embcat.T @ dhprebn\ncmp('W1', dW1, W1)\n\ndb1 = dhprebn.sum(0)\ncmp('b1', db1, b1)\n\ndemb = dembcat.view(emb.shape)\ncmp('emb', demb, emb)\n\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n    for j in range(Xb.shape[1]):\n        ix = Xb[k, j]\n        dC[ix] += demb[k, j]\ncmp('C', dC, C)\n\nlogprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: True  | approximate: True  | maxdiff: 0.0\nbngain          | exact: True  | approximate: True  | maxdiff: 0.0\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff          | exact: False | approximate: False | maxdiff: 0.0013724520104005933\nbnvar           | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff          | exact: True  | approximate: True  | maxdiff: 0.0\nbnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0\nembcat          | exact: True  | approximate: True  | maxdiff: 0.0\nW1              | exact: True  | approximate: True  | maxdiff: 0.0\nb1              | exact: True  | approximate: True  | maxdiff: 0.0\nemb             | exact: True  | approximate: True  | maxdiff: 0.0\nC               | exact: True  | approximate: True  | maxdiff: 0.0\n\n\n\ndprobs.shape\n\ntorch.Size([32, 27])\n\n\n\nprobs.shape\n\ntorch.Size([32, 27])\n\n\n\ncounts.shape\n\ntorch.Size([32, 27])\n\n\n\ndcounts_sum.shape\n\ntorch.Size([32, 1])\n\n\n\ncounts.shape\n\ntorch.Size([32, 27])"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html",
    "href": "lecture_notes/building_makemore_mlp2.html",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "",
    "text": "#!pip install tqdm\n\n\nfrom tqdm import tqdm, tqdm_notebook\nimport numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#imports",
    "href": "lecture_notes/building_makemore_mlp2.html#imports",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "",
    "text": "#!pip install tqdm\n\n\nfrom tqdm import tqdm, tqdm_notebook\nimport numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#setup",
    "href": "lecture_notes/building_makemore_mlp2.html#setup",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Setup",
    "text": "Setup\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n\n\nlen(words)\n\n32033\n\n\n\ndef generate_training_set(words, block_size, print_disabled=False):\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i+1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i:s for s, i in stoi.items()}\n    \n    X, Y = [], []\n    \n    for w in words:\n        if print_disabled: print(w)\n        \n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            if print_disabled: print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n            context = context[1:] + [ix] # crop and append\n            \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y\n\n\nX, Y = generate_training_set(words, 3)\n\n\nX.shape, Y.shape\n\n(torch.Size([228146, 3]), torch.Size([228146]))\n\n\n\ndef generate_train_valid_test_split(words, block_size=3):\n    random.seed(42)\n    random.shuffle(words)\n    n1 = int(0.8*len(words))\n    n2 = int(0.9*len(words))\n\n    Xtr, Ytr = generate_training_set(words[:n1], block_size)\n    Xdev, Ydev = generate_training_set(words[n1:n2], block_size)\n    Xte, Yte = generate_training_set(words[n2:], block_size)\n    \n    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n\n\nXtr, Ytr, Xdev, Ydev, Xte, Yte = generate_train_valid_test_split(words, block_size=3)\n\n\nXtr.shape, Ytr.shape\n\n(torch.Size([182625, 3]), torch.Size([182625]))\n\n\n\nXdev.shape, Ydev.shape\n\n(torch.Size([22655, 3]), torch.Size([22655]))\n\n\n\nXte.shape, Yte.shape\n\n(torch.Size([22866, 3]), torch.Size([22866]))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#train",
    "href": "lecture_notes/building_makemore_mlp2.html#train",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Train",
    "text": "Train\n\ndef compute_logits(parameters, X):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, W1.shape[0]) @ W1 + b1)\n    return h @ W2 + b2\n\n\ndef _regularization_loss(parameters, lambdas):\n    return sum(l * (p**2).mean() for p, l in zip(parameters, lambdas))\n\n\ndef initilialize_parameters(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g)\n    b1 = torch.randn(hidden_neuron, generator=g)\n    W2 = torch.randn((hidden_neuron, 27), generator=g)\n    b2 = torch.randn(27, generator=g)\n    return [C, W1, b1, W2, b2]\n\n\ndef train(parameters,\n          epochs,\n          X, \n          Y, \n          bs=32, \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        raise Exception(\"No initial parameters passed\")\n    \n    for p in parameters: p.requires_grad = True \n    \n    losses = []\n    for epoch in tqdm(range(epochs)):\n        \n        lr = 0.1 if epoch &lt; 100_000 else 0.01\n        \n        ix = torch.randint(0, X.shape[0], (bs, ))\n        batch_x, batch_y = X[ix], Y[ix]\n        logits = compute_logits(parameters, batch_x)\n        loss = F.cross_entropy(logits, batch_y)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n            \n        losses.append(loss.item())\n        \n    if enable_print:  print(epoch, loss.item())   \n    return losses\n\n\nparams1 = initilialize_parameters(3, 10, 200)\n\n\nlosses = train(params1, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                  | 50/200000 [00:00&lt;14:51, 224.22it/s]  5%|████████                                                                                                                                                       | 10086/200000 [00:16&lt;05:14, 602.91it/s] 10%|███████████████▉                                                                                                                                               | 19993/200000 [00:36&lt;06:22, 470.09it/s] 15%|███████████████████████▉                                                                                                                                       | 30086/200000 [00:56&lt;05:16, 536.84it/s] 20%|███████████████████████████████▉                                                                                                                               | 40107/200000 [01:15&lt;04:28, 595.70it/s] 25%|███████████████████████████████████████▊                                                                                                                       | 50117/200000 [01:34&lt;04:09, 601.27it/s] 30%|███████████████████████████████████████████████▊                                                                                                               | 60073/200000 [01:50&lt;03:55, 593.51it/s] 35%|███████████████████████████████████████████████████████▋                                                                                                       | 70114/200000 [02:08&lt;03:43, 581.43it/s] 40%|███████████████████████████████████████████████████████████████▋                                                                                               | 80089/200000 [02:26&lt;03:26, 579.70it/s] 45%|███████████████████████████████████████████████████████████████████████▋                                                                                       | 90115/200000 [02:43&lt;03:00, 607.76it/s] 50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 100092/200000 [03:00&lt;02:46, 598.58it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 110109/200000 [03:16&lt;02:30, 599.05it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 120106/200000 [03:33&lt;02:08, 619.76it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 130061/200000 [03:53&lt;04:29, 259.66it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 140117/200000 [04:14&lt;01:39, 600.35it/s] 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 150090/200000 [04:30&lt;01:20, 620.44it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 160095/200000 [04:47&lt;01:02, 639.88it/s] 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 170112/200000 [05:02&lt;00:46, 637.91it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 180125/200000 [05:18&lt;00:31, 634.12it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 190104/200000 [05:34&lt;00:15, 644.02it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [05:50&lt;00:00, 571.05it/s]\n\n\n0 27.642881393432617\n10000 2.6563515663146973\n20000 2.4569218158721924\n30000 2.4299867153167725\n40000 2.2356979846954346\n50000 2.222148895263672\n60000 1.997029423713684\n70000 2.013292074203491\n80000 2.4996888637542725\n90000 2.8243627548217773\n100000 2.815431833267212\n110000 2.333662509918213\n120000 2.5455234050750732\n130000 2.3017194271087646\n140000 2.1503050327301025\n150000 1.8232505321502686\n160000 2.411186456680298\n170000 2.52275013923645\n180000 2.173576593399048\n190000 2.165059804916382\n199999 2.3922524452209473\n\n\n\nplot.plot(losses)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#evaluate-loss",
    "href": "lecture_notes/building_makemore_mlp2.html#evaluate-loss",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Evaluate Loss",
    "text": "Evaluate Loss\n\n#torch.no_grad()\ndef evaluate_loss(parameters, X, Y):\n    logits = compute_logits(parameters, X)\n    return F.cross_entropy(logits, Y)\n\n\ndef loss_split(parameters, dataset='train'):\n    dataset_choices = {\n        'train': (Xtr, Ytr),\n        'valid': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }\n    return evaluate_loss(parameters, *dataset_choices[dataset])\n\n\nloss_split(params1), loss_split(params1, 'valid')\n\n(tensor(2.1137, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1547, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#sampling",
    "href": "lecture_notes/building_makemore_mlp2.html#sampling",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Sampling",
    "text": "Sampling\n\ndef generate_words(parameters, count, block_size=3):\n    for _ in range(count):\n        out = []\n        context = [0] * block_size # initialize with all ...\n        while True:\n            logits = compute_logits(parameters, torch.tensor([context]))\n            probs = F.softmax(logits, dim=1)\n            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n            context = context[1:] + [ix]\n            out.append(ix)\n            if ix == 0: break\n\n        print(''.join(itos[i] for i in out))\n\n\ngenerate_words(params1, 10)\n\njacklyny.\nnita.\nsano.\nmaketissariydah.\njama.\ncoanley.\nzemyni.\nkhreen.\nsis.\ncin."
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#fixing-the-initial-loss",
    "href": "lecture_notes/building_makemore_mlp2.html#fixing-the-initial-loss",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Fixing the initial loss",
    "text": "Fixing the initial loss\nIn the above training, at the 1st epoch the loss is 33.7 then it drops to 2.23\nif all the probabilities are uniform then we will expect a loss of\n\n- torch.tensor(1/27.0).log()\n\ntensor(3.2958)\n\n\n\nSample issue\n\nlogits = torch.tensor([0.0, 0.0, 0.0, 0.0])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.2500, 0.2500, 0.2500, 0.2500]), tensor(1.3863))\n\n\n\nlogits = torch.tensor([0.0, 0.0, 5.0, 0.0])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.0066, 0.0066, 0.9802, 0.0066]), tensor(0.0200))\n\n\n\nlogits = torch.tensor([0.0, 5.0, 0.0, 0.0])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.0066, 0.9802, 0.0066, 0.0066]), tensor(5.0200))\n\n\n\nlogits = torch.randn(4)\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.0808, 0.5283, 0.2293, 0.1616]), tensor(1.4727))\n\n\n\nlogits = torch.randn(4) * 10\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([1.8579e-01, 8.1123e-01, 1.6882e-04, 2.8133e-03]), tensor(8.6867))\n\n\n\nlogits = torch.rand(4) \nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.3815, 0.2294, 0.2123, 0.1768]), tensor(1.5497))\n\n\n\nlogits = torch.tensor([1.0, 1.0, 1.0, 1.0])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nprobs, loss\n\n(tensor([0.2500, 0.2500, 0.2500, 0.2500]), tensor(1.3863))\n\n\n\nparams2 = initilialize_parameters(3, 10, 200)\n\n\nlosses = train(params2, 1, Xtr, Ytr)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 357.51it/s]\n\n\n0 28.708385467529297\n0 28.708385467529297\n\n\n\ncompute_logits(params2, Xtr)[0] # the logits are not uniform\n\ntensor([  9.6100,   0.7546,  -4.9247,  -7.5269, -27.5197,  -7.1780,  -9.5191,\n         -6.9432, -11.4050,  15.3572,   3.7384,  24.8570,   5.2003,  -9.1091,\n          8.3202,   2.2977,  13.8022,   8.5462, -10.4909,  15.6155,  10.7404,\n        -10.5370,   4.4306,  22.4479,  21.0907,  13.4340,   5.8010],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ndef initilialize_parameters_v2(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g)\n    b1 = torch.randn(hidden_neuron, generator=g)\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    return [C, W1, b1, W2, b2]\n\n\nparams3 = initilialize_parameters_v2(3, 10, 200)\n\n\nlosses = train(params3, 1, Xtr, Ytr)\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 66.04it/s]\n\n\n0 3.2968194484710693\n0 3.2968194484710693\n\n\n\ncompute_logits(params3, Xtr)[0]\n\ntensor([-0.1350,  0.2579, -0.1748,  0.3775, -0.1732, -0.2264, -0.0493, -0.3319,\n         0.0467, -0.0121, -0.0923, -0.3330, -0.0462,  0.5159,  0.3215,  0.0554,\n        -0.0875, -0.2089, -0.1959,  0.1785,  0.1165, -0.2548, -0.2711, -0.1847,\n        -0.3341,  0.3078, -0.2509], grad_fn=&lt;SelectBackward0&gt;)\n\n\nThe logits are closer to zero now\n\nparams4 = initilialize_parameters_v2(3, 10, 200)\n\n\nlosses = train(params4, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                  | 23/200000 [00:00&lt;14:34, 228.75it/s]  5%|████████                                                                                                                                                       | 10114/200000 [00:18&lt;05:16, 599.28it/s] 10%|███████████████▉                                                                                                                                               | 20079/200000 [00:37&lt;05:10, 579.29it/s] 15%|███████████████████████▉                                                                                                                                       | 30114/200000 [00:55&lt;04:55, 575.28it/s] 20%|███████████████████████████████▉                                                                                                                               | 40108/200000 [01:13&lt;04:50, 550.33it/s] 25%|███████████████████████████████████████▊                                                                                                                       | 50088/200000 [01:30&lt;04:06, 607.78it/s] 30%|███████████████████████████████████████████████▊                                                                                                               | 60080/200000 [01:47&lt;03:51, 604.43it/s] 35%|███████████████████████████████████████████████████████▋                                                                                                       | 70121/200000 [02:03&lt;03:34, 604.67it/s] 40%|███████████████████████████████████████████████████████████████▋                                                                                               | 80125/200000 [02:19&lt;03:10, 629.99it/s] 45%|███████████████████████████████████████████████████████████████████████▌                                                                                       | 90074/200000 [02:36&lt;03:05, 591.36it/s] 50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 100078/200000 [02:52&lt;02:44, 608.54it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 110082/200000 [03:08&lt;02:24, 624.28it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 120123/200000 [03:25&lt;02:08, 622.65it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                       | 130079/200000 [03:42&lt;02:21, 495.27it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 140100/200000 [03:58&lt;01:34, 632.41it/s] 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 150100/200000 [04:15&lt;01:24, 593.63it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 160112/200000 [04:32&lt;01:12, 552.32it/s] 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 170099/200000 [04:49&lt;00:50, 588.99it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 180120/200000 [05:05&lt;00:32, 618.06it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 190054/200000 [05:22&lt;00:17, 554.68it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [05:39&lt;00:00, 589.22it/s]\n\n\n0 3.3012616634368896\n10000 2.0390188694000244\n20000 2.519038200378418\n30000 1.9892827272415161\n40000 1.973912239074707\n50000 2.0713963508605957\n60000 1.9657005071640015\n70000 2.3061559200286865\n80000 1.693084478378296\n90000 2.190971851348877\n100000 2.581700563430786\n110000 1.8936327695846558\n120000 2.3227176666259766\n130000 1.8893438577651978\n140000 2.0941903591156006\n150000 2.1335291862487793\n160000 2.551553964614868\n170000 1.945476770401001\n180000 2.069230318069458\n190000 1.791576862335205\n199999 2.231049060821533\n\n\n\nloss_split(params4, 'train'), loss_split(params4, 'valid')\n\n(tensor(2.0690, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1281, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#fix-the-saturated-tanh",
    "href": "lecture_notes/building_makemore_mlp2.html#fix-the-saturated-tanh",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Fix the saturated tanh",
    "text": "Fix the saturated tanh\n\nPlot tanh\n\nx = torch.linspace(-10, 10, 1000)\n\n\nplot.plot(x, torch.tanh(x))\n\n\n\n\n\n\nVisualize h\n\nh is the output of tanh in the above neural network\n\n\ndef compute_h(parameters, X):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    return torch.tanh(emb.view(-1, W1.shape[0]) @ W1 + b1)\n\n\nbatch_x = Xtr[torch.randint(0, Xtr.shape[0], (32, ))]\n\n\nh = compute_h(params4, batch_x)\n\n\nh\n\ntensor([[ 1.0000, -1.0000,  0.2621,  ..., -0.9742,  0.9999, -1.0000],\n        [ 1.0000, -1.0000,  0.9999,  ...,  0.2515,  0.1090, -0.8337],\n        [ 1.0000, -1.0000,  0.6779,  ..., -0.8491, -0.9900,  0.9737],\n        ...,\n        [ 0.9999,  0.9009, -0.9950,  ..., -1.0000,  0.9464,  0.9997],\n        [ 1.0000,  1.0000, -0.9781,  ...,  0.9608,  0.9965,  0.9994],\n        [-0.9998,  0.8074, -0.9989,  ...,  1.0000,  0.9892,  0.9999]],\n       grad_fn=&lt;TanhBackward0&gt;)\n\n\n\nh.shape\n\ntorch.Size([32, 200])\n\n\n\nh.view(-1).shape\n\ntorch.Size([6400])\n\n\n\nplot.hist(h.view(-1).tolist(), 50);\n\n\n\n\nAs we can see most values are -1 and 1\n\ndef compute_pre_activation(parameters, X):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    return emb.view(-1, W1.shape[0]) @ W1 + b1\n\n\nh_pre_act = compute_pre_activation(params4, batch_x)\n\n\nplot.hist(h_pre_act.view(-1).tolist(), 50);\n\n\n\n\n\nplot.figure(figsize=(20, 10))\nplot.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest')\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nThe white portion are the ones satisfying h.abs() &gt; 0.99 and black are the ones which are not satisfying the same\nAs we can see there are lots of whites, and the activations are lying mostly in the region of squashed\nThe gradient of tanh is (1 - t**2) * out.grad which will be 0 if tanh is +/-1, so there will be no gradients flowing through the network\nIf in the above image there is a single column of whites then that neuron will not learn anything for the batch of data\nThe h_pre_act is too off from zero therefore the activations are mostly -1 and 1. Lets change the parameters contributing to h_pre_act\n\ndef initilialize_parameters_v3(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g) * 0.1\n    b1 = torch.randn(hidden_neuron, generator=g) * 0.01\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    return [C, W1, b1, W2, b2]\n\n\nparams5 = initilialize_parameters_v3(3, 10, 200)\n\n\nlosses = train(params5, 1, Xtr, Ytr)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 273.58it/s]\n\n\n0 3.2925052642822266\n0 3.2925052642822266\n\n\n\nh1 = compute_h(params5, batch_x)\nplot.hist(h1.view(-1).tolist(), 50);\n\n\n\n\n\nh1_pre_act = compute_pre_activation(params5, batch_x)\nplot.hist(h1_pre_act.view(-1).tolist(), 50);\n\n\n\n\n\nplot.figure(figsize=(20, 10))\nplot.imshow(h1.abs() &gt; 0.99, cmap='gray', interpolation='nearest')\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nNo neurons saturated over 0.99 in either direction\n\ndef initilialize_parameters_v4(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g) * 0.2\n    b1 = torch.randn(hidden_neuron, generator=g) * 0.01\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    return [C, W1, b1, W2, b2]\n\n\nparams6 = initilialize_parameters_v4(3, 10, 200)\n\n\nlosses = train(params6, 1, Xtr, Ytr)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 387.46it/s]\n\n\n0 3.315361976623535\n0 3.315361976623535\n\n\n\nh2 = compute_h(params6, batch_x)\nplot.hist(h2.view(-1).tolist(), 50);\n\n\n\n\n\nh2_pre_act = compute_pre_activation(params6, batch_x)\nplot.hist(h2_pre_act.view(-1).tolist(), 50);\n\n\n\n\n\nplot.figure(figsize=(20, 10))\nplot.imshow(h2.abs() &gt; 0.99, cmap='gray', interpolation='nearest')\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nlosses = train(params6, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                 | 114/200000 [00:00&lt;05:52, 566.51it/s]  5%|████████                                                                                                                                                       | 10086/200000 [00:18&lt;05:32, 571.40it/s] 10%|███████████████▉                                                                                                                                               | 20103/200000 [00:37&lt;05:18, 564.13it/s] 15%|███████████████████████▉                                                                                                                                       | 30062/200000 [01:00&lt;07:17, 388.41it/s] 20%|███████████████████████████████▉                                                                                                                               | 40130/200000 [01:20&lt;04:18, 618.81it/s] 25%|███████████████████████████████████████▊                                                                                                                       | 50087/200000 [01:37&lt;04:15, 587.11it/s] 30%|███████████████████████████████████████████████▊                                                                                                               | 60082/200000 [01:54&lt;04:00, 582.39it/s] 35%|███████████████████████████████████████████████████████▋                                                                                                       | 70069/200000 [02:14&lt;05:20, 405.03it/s] 40%|███████████████████████████████████████████████████████████████▋                                                                                               | 80094/200000 [02:38&lt;03:43, 535.66it/s] 45%|███████████████████████████████████████████████████████████████████████▌                                                                                       | 90067/200000 [03:00&lt;03:50, 477.81it/s] 50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 100049/200000 [03:26&lt;03:11, 520.97it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 110095/200000 [03:54&lt;03:16, 457.70it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                               | 120021/200000 [04:21&lt;02:43, 487.95it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                       | 130064/200000 [04:40&lt;02:11, 531.51it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 140109/200000 [05:01&lt;01:48, 549.81it/s] 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 150027/200000 [05:22&lt;01:33, 531.64it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 160100/200000 [05:45&lt;01:09, 573.43it/s] 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 170099/200000 [06:03&lt;00:53, 562.53it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 180121/200000 [06:22&lt;00:30, 652.22it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 190125/200000 [06:38&lt;00:14, 664.52it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [16:38:01&lt;00:00,  3.34it/s]\n\n\n0 3.252462863922119\n10000 2.2485759258270264\n20000 1.924424648284912\n30000 2.3567395210266113\n40000 1.7223490476608276\n50000 1.9401909112930298\n60000 2.2472681999206543\n70000 2.110548973083496\n80000 1.9843206405639648\n90000 2.498479127883911\n100000 2.0100741386413574\n110000 1.9128767251968384\n120000 2.1294615268707275\n130000 1.7961547374725342\n140000 1.6151217222213745\n150000 1.905795693397522\n160000 2.0080981254577637\n170000 2.0118043422698975\n180000 1.73159921169281\n190000 2.196617841720581\n199999 2.2335524559020996\n\n\n\nloss_split(params6, 'train'), loss_split(params6, 'valid')\n\n(tensor(2.0385, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1043, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#calculating-the-init-scale-kaiming-init",
    "href": "lecture_notes/building_makemore_mlp2.html#calculating-the-init-scale-kaiming-init",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Calculating the init scale: “Kaiming init”",
    "text": "Calculating the init scale: “Kaiming init”\n\nx = torch.randn(1000, 10)\nw = torch.randn(10, 200)\ny = x @ w\nprint(x.mean(), x.std())\nprint(y.mean(), y.std())\nplot.figure(figsize=(20, 5))\nplot.subplot(121)\nplot.hist(x.view(-1).tolist(), 50, density=True);\nplot.subplot(122)\nplot.hist(y.view(-1).tolist(), 50, density=True);\n\ntensor(-0.0029) tensor(0.9987)\ntensor(-0.0028) tensor(3.1829)\n\n\n\n\n\nThe y has more standard deviation compared to x\nThe guassian is expanding, we dont want that, we need similar activations during initialization\nIf we multiply w by a large number i.e. 5, then the standard deviation increases\n\nx = torch.randn(1000, 10)\nw = torch.randn(10, 200) * 5\ny = x @ w\nprint(x.mean(), x.std())\nprint(y.mean(), y.std())\nplot.figure(figsize=(20, 5))\nplot.subplot(121)\nplot.hist(x.view(-1).tolist(), 50, density=True);\nplot.subplot(122)\nplot.hist(y.view(-1).tolist(), 50, density=True);\n\ntensor(-0.0073) tensor(0.9939)\ntensor(0.0191) tensor(15.9527)\n\n\n\n\n\nif we multiply w by a small number, then the standard deviation will be smaller\n\nx = torch.randn(1000, 10)\nw = torch.randn(10, 200) * 0.2\ny = x @ w\nprint(x.mean(), x.std())\nprint(y.mean(), y.std())\nplot.figure(figsize=(20, 5))\nplot.subplot(121)\nplot.hist(x.view(-1).tolist(), 50, density=True);\nplot.subplot(122)\nplot.hist(y.view(-1).tolist(), 50, density=True);\n\ntensor(-0.0038) tensor(0.9879)\ntensor(-0.0003) tensor(0.6244)\n\n\n\n\n\nWhat number should we multiply w by to preserve the standard deviation as 1?\n\nWe can multiply by (1/sqrt(fan_in))\n\n\nx = torch.randn(1000, 10)\nw = torch.randn(10, 200) * 1/(math.sqrt(10))\ny = x @ w\nprint(x.mean(), x.std())\nprint(y.mean(), y.std())\nplot.figure(figsize=(20, 5))\nplot.subplot(121)\nplot.hist(x.view(-1).tolist(), 50, density=True);\nplot.subplot(122)\nplot.hist(y.view(-1).tolist(), 50, density=True);\n\ntensor(0.0040) tensor(0.9988)\ntensor(-0.0010) tensor(0.9846)\n\n\n\n\n\n\ntorch.randn(1000)\n\ntensor([-4.4491e-01,  8.1468e-01, -4.9041e-01,  8.5084e-01, -7.5546e-01,\n        -1.2834e+00, -1.4080e-01, -1.5628e-01,  7.0759e-01,  5.4389e-01,\n         1.0249e+00, -3.6809e-01,  1.5767e-01, -1.2903e-01, -9.6614e-01,\n        -5.1882e-01, -1.0040e+00, -9.6146e-01, -4.9114e-02, -1.0125e+00,\n         1.0659e+00,  1.3129e+00,  5.9042e-01, -1.3400e-01, -4.0615e-01,\n         7.1402e-01,  8.5876e-01,  1.1057e+00, -1.7399e-01, -2.5380e-01,\n        -1.9785e-01, -2.2213e+00,  3.8632e-02, -1.4455e+00,  1.4416e+00,\n         4.6785e-02,  1.0486e+00,  1.7613e-02,  5.2755e-01, -1.3378e+00,\n         7.3152e-01,  1.5148e+00, -9.0945e-01,  9.5265e-01,  1.7500e+00,\n        -1.5277e+00, -5.6958e-02, -9.2872e-01, -5.7160e-01, -1.3054e+00,\n         2.9642e-01, -9.0971e-01,  1.4852e-01,  4.8295e-01,  2.0170e-02,\n        -1.5728e-01,  6.1446e-01, -7.2750e-01,  2.7242e-01, -5.0731e-01,\n        -1.6935e+00, -1.1223e+00,  4.0411e-01,  7.8083e-01,  8.9397e-01,\n         1.1677e+00, -1.6698e+00, -1.1389e-01, -1.3376e+00, -3.3989e-01,\n         4.6134e-01,  1.3889e+00, -2.0328e-01, -8.0168e-01, -1.3011e+00,\n         1.7799e-01,  9.1866e-01,  7.4976e-01, -1.7144e+00,  7.3718e-01,\n        -3.3846e-01,  1.7048e-01, -1.6116e-01,  5.1980e-01, -3.2220e-01,\n         9.6030e-01,  3.0398e-01, -7.1770e-01,  5.1479e-01,  8.8952e-02,\n         1.5568e+00, -6.4372e-01, -1.4770e-01, -1.2715e+00,  1.1549e+00,\n        -2.2085e+00, -8.0787e-01, -8.0596e-01,  1.1667e+00,  1.1490e+00,\n         1.9939e-01,  3.5011e-01,  4.3978e-01,  1.0387e+00,  1.1948e+00,\n         6.5371e-01, -1.1983e+00, -7.9712e-02,  9.4302e-01,  7.7875e-01,\n        -3.7207e-01,  6.0207e-01,  1.4607e-01,  3.4527e-02, -8.4879e-01,\n        -7.7520e-01, -2.9863e-01,  2.2895e-01, -1.8310e+00,  4.8203e-01,\n        -1.5591e+00,  1.1811e+00, -6.0453e-01, -3.9585e-01, -1.0402e+00,\n         1.1609e+00, -1.3437e+00,  1.3366e+00,  2.7067e+00,  6.6262e-01,\n         1.1726e-01, -1.4091e+00,  8.6855e-01, -3.2722e-01, -1.0854e+00,\n        -1.7248e-01,  4.2303e-01, -1.0056e-01,  6.7321e-01, -2.1935e-01,\n        -1.3298e-01,  3.1146e-01, -3.2207e-01, -8.5663e-01,  1.0111e+00,\n        -3.8868e-01,  1.1240e+00,  1.3679e-01, -1.2754e+00,  1.6846e+00,\n         1.8569e-01, -1.4316e+00, -5.2817e-01,  4.4829e-01,  1.1192e+00,\n        -1.0870e+00,  1.2514e+00,  2.4123e-01,  4.1799e-01,  1.3938e+00,\n        -6.4690e-01, -3.2768e-01, -1.0039e+00,  1.0455e+00, -1.1427e+00,\n        -3.5521e-01,  3.8842e-01, -1.3270e+00,  1.1490e+00, -1.4166e+00,\n        -1.7509e+00, -2.6920e-02, -2.8950e-01,  7.1716e-01, -1.0339e+00,\n        -1.4395e-01, -7.5489e-01, -3.2210e-02,  1.3414e+00, -4.4632e-01,\n         1.0532e+00, -1.1267e+00,  1.1626e+00, -1.0597e+00,  4.8057e-01,\n         1.2193e+00,  9.7095e-01, -4.6855e-01,  9.6808e-01, -7.2747e-01,\n        -1.0172e+00, -1.5430e-01, -3.7111e-01,  1.1363e+00,  1.1665e+00,\n        -5.7238e-01, -7.2625e-01,  1.1168e+00, -1.9561e+00,  3.6347e-01,\n         1.2211e+00, -1.0392e+00,  9.1227e-01, -5.3476e-01,  7.7828e-01,\n        -1.8234e-01,  1.2498e+00, -7.6176e-02, -1.4105e-01, -2.7955e-01,\n        -7.7375e-01, -2.3475e+00, -7.9845e-01,  8.9417e-01, -4.1188e-01,\n         1.1319e+00,  1.2606e-01, -2.1836e+00,  7.5730e-01, -6.8296e-01,\n        -1.0518e+00,  1.2614e+00,  3.5788e-01,  3.6420e-01, -3.9252e-01,\n        -7.4942e-01, -1.8380e+00,  1.3533e+00, -2.8998e-02, -2.3180e+00,\n        -1.9691e-01, -1.2409e+00, -9.2009e-01, -1.9675e-01, -9.7025e-01,\n        -1.3910e+00, -6.6132e-01, -1.7533e+00, -1.0233e+00, -2.0021e+00,\n        -2.3171e+00, -7.7370e-01, -1.2550e+00,  1.2218e+00,  1.7372e-01,\n         8.3574e-01,  1.6951e-01,  1.0796e-01,  1.2036e+00, -1.1552e+00,\n         1.7398e-01,  3.1005e-01, -7.4864e-01,  9.1199e-01, -8.1297e-01,\n        -1.3774e+00, -4.1376e-02,  1.5385e+00,  1.5433e-01,  7.6850e-01,\n        -4.5575e-01, -3.3947e-01,  1.5767e+00, -1.6138e+00,  1.3509e+00,\n         7.7009e-01, -1.6286e+00,  1.4196e+00,  8.5499e-01, -6.5572e-01,\n         1.0467e+00,  5.3764e-01,  5.4705e-01, -3.0934e-01,  1.4358e+00,\n        -2.3142e+00, -2.5676e-01, -6.9334e-01,  6.7920e-01, -1.5806e-02,\n         6.6129e-01, -1.1277e-01,  3.7076e-01, -2.0539e+00,  9.6729e-01,\n        -1.1464e-01,  9.3331e-02,  7.1655e-01,  2.2155e-01, -2.1334e-01,\n        -7.2953e-01, -1.0252e+00,  1.1660e+00,  4.8370e-01, -4.9408e-01,\n         8.3829e-01, -8.5957e-01, -6.6706e-02, -6.7575e-01,  9.3957e-01,\n         5.0669e-01, -2.3851e-01,  2.9753e-01,  5.4236e-01, -7.0215e-01,\n         1.4101e+00,  1.6822e-01,  3.4431e+00,  1.3912e+00, -1.8377e+00,\n         1.4642e+00,  5.8495e-01, -8.7159e-01,  1.9798e+00,  4.8268e-01,\n         1.1796e+00,  1.8971e+00, -2.2471e-01,  1.4477e+00,  1.4796e+00,\n         2.0498e+00,  6.0896e-01,  1.7562e+00, -1.5760e+00, -6.4049e-01,\n         1.2525e+00, -1.5839e-01, -8.6765e-01, -6.2326e-01,  1.1278e-01,\n        -9.8297e-01, -5.5136e-01,  1.4451e-01,  1.4907e+00, -9.7304e-01,\n         1.1056e+00,  1.0133e+00,  6.1220e-01,  4.0848e-01, -6.6162e-01,\n        -7.4903e-01,  2.9114e+00,  1.3749e+00, -2.3306e+00, -2.3087e-01,\n        -1.1470e+00,  2.0197e+00, -9.6675e-01, -7.4702e-01, -1.0908e-01,\n        -2.6147e-01, -3.2547e-01, -1.7522e-01, -2.4414e-01, -3.6424e-01,\n        -1.3112e+00, -4.8352e-01, -1.5956e+00, -1.0321e-01,  3.1300e-01,\n        -2.2417e-01,  6.4919e-01, -9.9813e-01,  1.9788e+00, -2.3398e+00,\n         3.1999e-01,  1.1417e+00, -7.2538e-02,  7.5595e-01, -1.1833e+00,\n        -1.0342e+00,  1.3779e+00,  4.6179e-01, -4.4127e-01, -1.5523e+00,\n         2.4986e+00, -1.4134e+00,  8.8584e-01,  3.8325e-01,  3.0485e-01,\n         1.8157e+00, -7.2691e-01, -4.9207e-01,  8.3230e-01,  1.0072e+00,\n        -8.1437e-01, -1.3365e-01,  3.8920e-01, -1.0508e-01, -1.1311e+00,\n        -8.0398e-01,  3.3417e-01,  3.9109e-01, -9.8168e-01, -1.1504e+00,\n         9.3065e-01, -2.1849e-01, -2.7455e-03,  1.5553e+00, -1.5637e-01,\n         1.1848e-01, -9.1837e-01, -1.1483e-01, -5.6455e-01, -6.8401e-02,\n         5.4284e-01,  6.9041e-01,  1.5359e+00,  6.5503e-02,  1.2606e+00,\n        -2.3238e-01,  5.0018e-01,  4.0842e-01,  1.2282e-01, -8.3332e-01,\n        -5.2143e-01,  1.0709e-01,  5.5946e-01, -1.7920e+00,  9.8011e-02,\n         2.3607e-01,  9.1122e-01, -1.7815e+00, -2.2378e+00,  6.0846e-01,\n        -1.0682e+00,  6.7406e-01,  1.1799e+00, -2.7380e-02,  1.1086e+00,\n         1.2985e-01, -1.5836e+00,  9.9837e-01, -8.8163e-01,  4.2766e-01,\n        -7.5449e-01, -9.1209e-01,  8.2167e-01, -1.3376e+00, -5.5470e-01,\n        -2.5744e+00,  2.3497e+00,  9.1383e-01, -8.6754e-01,  2.6851e-02,\n        -5.6935e-01,  1.7634e+00, -5.4466e-01,  6.4427e-01, -2.8968e+00,\n         1.0398e+00, -1.7710e+00,  2.6833e-01,  9.7795e-01, -5.1294e-01,\n        -4.4039e-01,  8.8880e-01, -1.5962e+00,  2.2802e-01,  9.9065e-01,\n        -3.9762e-01, -8.1780e-01,  2.2655e+00, -1.6902e+00, -1.0324e-01,\n         1.4844e+00,  5.5991e-01, -1.9720e+00,  2.3696e+00,  1.4115e-01,\n        -1.0652e+00, -9.0866e-01,  1.1514e+00,  1.6936e+00,  4.7878e-01,\n        -2.9971e-01,  5.4005e-01,  7.3565e-01, -4.4122e-01, -6.8278e-01,\n         5.9391e-01, -7.5252e-01, -6.0103e-01, -4.1738e-01, -6.0496e-01,\n        -1.9164e+00,  3.4902e-01,  6.5277e-02, -1.8154e-01,  9.1510e-01,\n        -9.1029e-02, -2.4382e-01,  2.3432e+00,  1.9859e+00, -7.8514e-01,\n         1.2721e-01,  1.4515e+00, -1.2700e-01, -1.6711e-01,  6.6730e-01,\n        -1.6903e-01,  1.0743e-01, -1.1094e+00, -1.0274e+00,  3.7128e-01,\n        -2.3233e-01, -3.0973e-01,  1.8141e+00,  1.9199e-01, -1.8364e+00,\n        -2.1589e-01, -7.8127e-02, -3.4849e-01, -2.1622e+00,  4.0660e-01,\n        -9.5050e-01, -9.0194e-01, -5.4401e-01,  1.9922e+00,  5.5333e-01,\n         2.2488e-01, -4.8751e-01,  7.1682e-01,  3.6225e-01,  8.9288e-01,\n        -8.5990e-01,  9.6229e-01,  9.5417e-01, -5.1965e-01, -2.3035e+00,\n        -4.9344e-01, -1.7938e+00,  9.6043e-01,  3.4079e-01, -6.7608e-01,\n         1.1257e+00, -2.9176e-01, -2.4500e-01,  2.1111e+00, -9.0706e-01,\n         2.4174e+00,  1.8432e+00,  8.5921e-01,  3.7028e-02,  3.3475e-01,\n        -1.2499e+00, -2.7984e-01, -9.5921e-02, -1.2070e+00, -5.6210e-01,\n        -7.6785e-01, -1.0238e-01, -4.1785e-01, -1.0449e+00,  1.4974e+00,\n        -9.2206e-01,  8.6997e-02, -7.2990e-01, -5.8177e-02,  1.2354e+00,\n        -1.8226e-02, -1.2640e+00, -8.9501e-01, -1.2832e+00, -4.8085e-01,\n         1.0304e+00, -2.2113e+00, -4.8045e-01, -5.8689e-01,  9.0754e-01,\n         2.4374e-01, -5.2606e-01, -6.5553e-01, -3.4300e-01,  6.7370e-01,\n         9.0023e-01,  1.2187e+00,  1.0026e+00, -5.2062e-01, -1.2393e+00,\n        -7.0569e-01,  1.3346e+00, -1.0457e+00, -2.1257e-02, -4.9760e-01,\n        -6.0507e-01,  1.4430e+00,  3.0979e-01, -1.2321e+00,  1.8128e-01,\n         4.8367e-01,  5.6369e-01, -9.7980e-02,  1.4244e+00,  9.5563e-02,\n         8.2211e-01,  1.2565e+00,  1.7145e+00,  1.8543e+00,  8.3598e-01,\n        -1.5805e+00,  4.4981e-01,  4.9791e-01,  1.5932e+00, -7.8263e-01,\n         1.1016e+00, -1.4328e+00, -1.3174e-01, -2.3278e-01,  1.2399e+00,\n        -1.1156e-01, -1.0908e+00, -8.6325e-01, -1.2553e-02, -2.0168e-01,\n         9.7023e-02,  6.2413e-01,  4.3617e-01, -7.6339e-01,  1.7359e+00,\n        -8.8891e-02,  1.1993e+00,  1.2335e+00, -1.7588e-01, -1.1068e+00,\n         1.5370e+00,  5.3286e-01, -1.7069e+00, -7.0883e-01,  6.0098e-01,\n        -1.8722e+00,  1.0028e+00, -1.7522e+00,  1.9773e+00,  7.6629e-02,\n         9.7794e-01,  9.1844e-01,  3.6816e-02,  7.0968e-01,  1.4424e+00,\n        -8.8674e-01,  9.6100e-01,  4.4609e-01, -3.7348e-01,  2.1652e+00,\n        -2.0705e-01,  2.8895e+00, -8.2157e-03,  1.0014e-01,  1.3509e+00,\n        -9.2852e-01, -1.4189e+00, -3.4976e-01,  1.1974e-01, -6.0752e-01,\n         1.2418e+00,  1.4813e+00, -2.9009e-01, -6.5577e-02, -3.7928e-01,\n         4.0710e-01, -7.4858e-02,  5.8135e-01,  1.4308e+00,  4.6332e-01,\n         1.5282e+00, -1.1648e+00, -1.3339e+00,  8.2611e-01, -8.9988e-02,\n         1.2866e+00,  9.7417e-04, -1.0353e+00, -7.7178e-01,  1.5070e+00,\n         1.3771e+00, -1.4094e+00,  4.6631e-02,  6.4983e-01,  4.4373e-01,\n         4.3267e-01, -1.5054e-01, -7.0889e-01, -4.7002e-01, -1.2221e+00,\n         1.1314e-01,  2.0500e-01,  5.8394e-01, -1.1366e-02,  1.1042e-02,\n        -5.1476e-01, -1.6741e+00,  1.8323e+00, -3.9113e-01,  3.0786e-01,\n         5.6348e-01, -2.2628e-01, -1.0945e+00, -2.5047e+00, -1.3732e-01,\n        -9.4335e-01,  9.3365e-01, -1.3367e-01,  5.3266e-01, -7.3486e-01,\n        -1.2251e+00,  1.5481e+00,  7.1739e-01,  1.1879e+00,  8.1519e-01,\n         2.0114e-01, -1.6691e+00, -2.9070e-02, -1.6126e+00,  2.6002e-01,\n         1.7315e+00, -3.7181e-01,  1.7891e+00,  2.6855e-02,  1.3394e+00,\n        -1.2826e-01,  6.9187e-01,  5.2593e-01,  5.1028e-01, -2.9707e-01,\n         1.0332e+00,  7.6733e-01,  2.4797e+00, -1.6167e-01,  6.6452e-02,\n         9.0616e-01,  7.1738e-01,  7.1106e-01,  9.8761e-02, -1.0895e+00,\n        -8.7591e-01,  8.7157e-01,  1.9313e+00, -6.2044e-01,  3.4145e-02,\n        -3.4549e-01,  5.2566e-01, -6.1216e-01,  4.0845e-01,  1.2780e+00,\n        -6.7273e-01, -1.6323e+00,  1.3512e+00,  7.9965e-02,  1.2352e-01,\n        -2.9542e-01,  1.5546e+00, -2.2835e-01, -1.1723e+00, -8.9304e-01,\n        -9.0590e-01, -3.3402e-01,  5.9588e-01,  1.6975e-01, -1.3846e+00,\n         8.1981e-01,  2.4207e-02,  1.1152e-01,  2.1979e+00,  4.7347e-01,\n        -3.6197e-02, -7.3026e-03, -6.8270e-01, -8.8449e-01,  3.6973e-01,\n         7.1029e-01,  1.6141e-01,  3.9045e-01, -1.6220e-01, -1.0303e-02,\n         2.9736e-01, -2.5634e-01, -7.6549e-01,  7.0336e-01,  4.5149e-01,\n        -3.2849e-01, -1.6511e+00,  6.9789e-01,  1.1553e+00, -1.5515e+00,\n         1.1479e+00,  7.9370e-01, -1.1824e+00, -8.7946e-01,  1.3841e+00,\n        -1.8442e+00,  9.5913e-01, -1.0785e+00,  2.8138e-01,  1.4519e+00,\n         1.6403e+00, -1.6989e+00, -1.7778e+00, -1.3598e+00, -6.3483e-01,\n         5.2751e-01,  4.9287e-01,  5.0181e-01, -6.0085e-01,  6.2637e-01,\n        -7.0738e-01,  4.8160e-01, -1.1089e-01,  7.4083e-01, -8.9509e-01,\n         9.0353e-01, -4.0467e-01, -8.5919e-02,  2.6746e-01, -1.9548e+00,\n         6.0947e-01,  8.7655e-01, -6.5896e-01, -6.1613e-01,  1.7297e+00,\n         1.9492e-01, -1.8195e+00,  2.2503e-02, -1.9076e+00,  7.5093e-02,\n         1.6529e+00,  3.4259e-01,  1.4164e+00,  1.5928e+00, -4.3144e-01,\n        -9.2303e-01, -4.6064e-01, -4.6902e-01, -1.5084e+00,  8.9347e-01,\n         1.1865e+00, -9.3348e-01,  8.6712e-01, -3.0535e-01, -7.8115e-01,\n         1.8942e+00,  4.7689e-01,  6.3666e-01, -2.2987e-01,  2.2629e+00,\n        -3.9918e-01, -1.7252e+00,  1.5192e+00, -2.2920e+00,  2.6366e-01,\n        -6.8147e-02,  1.3599e-01,  1.3921e+00,  1.6916e-01,  1.2853e+00,\n        -1.9718e+00, -5.3289e-01,  1.2188e+00, -2.4011e-01, -4.4860e-01,\n        -5.3689e-01, -5.2381e-01,  1.0976e-01,  5.2891e-01, -9.0536e-01,\n        -5.3731e-01, -5.0766e-01, -1.2572e+00, -1.3832e+00,  1.0783e+00,\n        -5.6167e-01,  3.8724e-01,  1.8097e-01, -5.9655e-01, -9.2021e-01,\n        -2.1552e+00,  1.3431e-01, -1.2162e+00, -1.5789e+00, -4.1252e-01,\n        -1.0802e+00, -6.0434e-02, -3.3154e-01,  1.1832e+00,  7.1232e-01,\n        -1.1653e+00, -2.1207e+00,  2.2294e-01, -4.0428e-01,  1.6746e+00,\n        -9.8364e-01, -1.8898e+00,  4.7501e-02, -1.9037e-02,  6.3712e-01,\n        -5.2208e-01,  1.1077e+00,  5.4200e-02,  4.3732e-01,  9.7521e-01,\n         4.4448e-01,  8.5956e-01,  5.4088e-01, -7.5151e-01, -8.2385e-02,\n         2.7066e+00, -4.0313e-01, -1.2705e+00, -1.8110e-03, -7.9295e-01,\n        -4.0852e-01, -7.5687e-01,  1.1580e+00,  3.5440e-01,  8.5731e-01,\n        -1.4712e+00,  1.6121e-01, -4.4616e-01,  2.1555e+00,  6.6903e-02,\n        -1.2607e+00, -2.2889e-01, -2.4372e-02,  1.6145e+00,  1.6716e+00,\n         4.4838e-01, -1.8342e-01,  7.2343e-01, -1.0761e+00,  2.1152e+00,\n        -1.0926e+00,  7.0204e-01, -3.3275e-01,  2.6774e-02, -3.8973e-01,\n        -1.2466e+00,  5.2782e-01, -5.4509e-01,  4.6797e-01,  9.5262e-01,\n         3.0096e-02, -1.1982e+00, -6.1488e-01,  5.2910e-01, -4.2660e-01,\n        -1.1221e+00, -3.5857e-01,  2.8266e-01, -6.7295e-01, -6.9349e-01,\n         8.0775e-01,  5.0606e-01,  1.9053e+00,  1.2228e+00,  4.7014e-01,\n         3.8042e-01, -5.3530e-01, -1.6741e-01,  1.2887e+00, -1.3320e+00,\n         1.2936e+00,  1.0690e+00,  1.3661e+00,  6.7960e-01, -4.4733e-01,\n         6.9975e-01,  1.8949e-01,  3.4809e-02, -1.2910e-01,  1.0193e+00,\n        -1.7590e-01, -2.5930e-01,  3.2330e-01, -2.9028e-01, -5.4029e-01,\n        -8.1340e-01, -1.1686e+00,  9.4940e-01, -6.8079e-02, -3.1358e-01,\n        -2.6569e-01, -3.5748e-01,  1.4510e+00, -7.5871e-01,  9.6715e-01,\n         3.7772e-01,  8.1767e-01, -1.7959e+00, -3.8471e-01, -1.3908e+00,\n        -7.9921e-01, -1.6201e+00, -1.7005e-01,  9.1469e-01,  1.7542e+00,\n        -1.3094e+00, -1.0830e+00, -2.7837e+00,  3.6276e-01,  3.3478e-01])\n\n\n\n(torch.randn(1000) * 0.2).std()\n\ntensor(0.2046)\n\n\n\nstd_dev = (5/3) / (30 ** 0.5); std_dev\n\n0.3042903097250923\n\n\n\ndef initilialize_parameters_v5(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g) * ((5/3) / ((embedding_size * block_size) ** 0.5))\n    b1 = torch.randn(hidden_neuron, generator=g) * 0.01\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    return [C, W1, b1, W2, b2]\n\n\nparams7 = initilialize_parameters_v5(3, 10, 200)\n\n\nlosses = train(params7, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                 | 132/200000 [00:00&lt;05:09, 645.01it/s]  5%|████████                                                                                                                                                       | 10125/200000 [00:14&lt;04:19, 733.07it/s] 10%|███████████████▊                                                                                                                                             | 20084/200000 [1:00:00&lt;04:29, 668.33it/s] 15%|███████████████████████▋                                                                                                                                     | 30138/200000 [1:00:14&lt;04:01, 702.35it/s] 19%|██████████████████████████████▌                                                                                                                              | 38883/200000 [4:01:38&lt;04:25, 605.88it/s] 25%|███████████████████████████████████████▎                                                                                                                     | 50091/200000 [4:01:54&lt;03:29, 715.16it/s] 30%|███████████████████████████████████████████████▏                                                                                                             | 60084/200000 [4:02:09&lt;03:31, 661.46it/s] 35%|███████████████████████████████████████████████████████                                                                                                      | 70071/200000 [4:02:24&lt;03:19, 650.30it/s] 40%|██████████████████████████████████████████████████████████████▉                                                                                              | 80121/200000 [4:02:39&lt;02:59, 667.73it/s] 45%|██████████████████████████████████████████████████████████████████████▋                                                                                      | 90110/200000 [4:02:55&lt;02:37, 695.86it/s] 50%|██████████████████████████████████████████████████████████████████████████████                                                                              | 100130/200000 [4:03:10&lt;02:26, 680.25it/s] 55%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                      | 110085/200000 [4:03:25&lt;02:17, 655.44it/s] 60%|█████████████████████████████████████████████████████████████████████████████████████████████▋                                                              | 120089/200000 [4:03:40&lt;02:04, 644.20it/s] 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                      | 130067/200000 [4:03:55&lt;01:46, 655.76it/s] 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                              | 140063/200000 [4:04:10&lt;01:29, 666.43it/s] 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 150093/200000 [4:04:25&lt;01:19, 628.04it/s] 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 160115/200000 [4:04:40&lt;00:53, 743.59it/s] 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 170077/200000 [4:04:54&lt;00:44, 677.63it/s] 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 180117/200000 [4:05:08&lt;00:26, 751.15it/s] 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 190117/200000 [4:05:23&lt;00:14, 702.81it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [4:05:37&lt;00:00, 13.57it/s]\n\n\n0 3.2795920372009277\n10000 2.470160484313965\n20000 2.183413505554199\n30000 2.3951308727264404\n40000 1.8953704833984375\n50000 2.1281423568725586\n60000 2.08463454246521\n70000 1.564221739768982\n80000 2.0972611904144287\n90000 2.21366810798645\n100000 2.302164077758789\n110000 1.839044451713562\n120000 1.8937313556671143\n130000 2.7189743518829346\n140000 2.1313252449035645\n150000 1.9625704288482666\n160000 1.89139723777771\n170000 1.889981985092163\n180000 1.9499194622039795\n190000 1.9968667030334473\n199999 1.9478144645690918\n\n\n\nloss_split(params7, 'train'), loss_split(params7, 'valid')\n\n(tensor(2.0388, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1015, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#batch-normalization",
    "href": "lecture_notes/building_makemore_mlp2.html#batch-normalization",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Batch Normalization",
    "text": "Batch Normalization\n\nNormalization\n\nh_pre_act = compute_pre_activation(params7, batch_x)\nh_pre_act.shape\n\ntorch.Size([32, 200])\n\n\n\nh_pre_act_mean = h_pre_act.mean(0, keepdim = True)\nh_pre_act_mean.shape\n\ntorch.Size([1, 200])\n\n\n\nh_pre_act_std = h_pre_act.std(0, keepdim = True)\nh_pre_act_std.shape\n\ntorch.Size([1, 200])\n\n\n\nh_pre_act_norm = (h_pre_act - h_pre_act_mean)/h_pre_act_std\nh_pre_act_norm.shape\n\ntorch.Size([32, 200])\n\n\n\n\nDefine new training\n\ndef initilialize_parameters_v6(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g) * ((5/3) / ((embedding_size * block_size) ** 0.5))\n    b1 = torch.randn(hidden_neuron, generator=g) * 0.01\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    \n    bngain = torch.ones((1, hidden_neuron)) # for scale in batch normalization\n    bnbias = torch.zeros((1, hidden_neuron)) # for shift in batch normalization\n    \n    return [C, W1, b1, W2, b2, bngain, bnbias]\n\n\ndef compute_logits_v2(parameters, X):\n    C, W1, b1, W2, b2, bngain, bnbias = parameters\n    emb = C[X]\n    embcat = emb.view(-1, W1.shape[0]) \n    h_pre_act = embcat @ W1 + b1\n    h_pre_act_norm = (h_pre_act - h_pre_act.mean(0, keepdim = True)) / h_pre_act.std(0, keepdim = True)\n    h_pre_act_scale_shift = bngain * h_pre_act_norm + bnbias\n    h = torch.tanh(h_pre_act_scale_shift)\n    logits = h @ W2 + b2\n    return logits\n\n\ndef train_v2(parameters,\n          epochs,\n          X, \n          Y, \n          bs=32, \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        raise Exception(\"No initial parameters passed\")\n    \n    for p in parameters: p.requires_grad = True \n    \n    losses = [] \n    \n    for epoch in tqdm(range(epochs)):\n        \n        lr = 0.1 if epoch &lt; 100_000 else 0.01\n        \n        ix = torch.randint(0, X.shape[0], (bs, ))\n        batch_x, batch_y = X[ix], Y[ix]\n        \n        logits = compute_logits_v2(parameters, batch_x)\n        \n        loss = F.cross_entropy(logits, batch_y)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n            \n        loss.backward()\n\n\n        for index, p in enumerate(parameters):\n            if p.grad is None: print(index)\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n            \n        losses.append(loss.item())\n        \n    if enable_print:  print(epoch, loss.item())   \n    return losses\n\n\nparams8 = initilialize_parameters_v6(3, 10, 200)\n\n\nlen(params8)\n\n7\n\n\n\nlosses = train_v2(params8, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                 | 104/200000 [00:00&lt;06:19, 526.54it/s]  5%|████████                                                                                                                                                       | 10108/200000 [00:19&lt;05:51, 539.73it/s] 10%|███████████████▉                                                                                                                                               | 20082/200000 [00:37&lt;05:26, 550.87it/s] 15%|███████████████████████▊                                                                                                                                       | 29997/200000 [00:58&lt;05:47, 489.55it/s] 20%|███████████████████████████████▊                                                                                                                               | 40049/200000 [01:26&lt;09:19, 285.73it/s] 25%|███████████████████████████████████████▊                                                                                                                       | 50024/200000 [02:08&lt;16:14, 153.85it/s] 30%|███████████████████████████████████████████████▊                                                                                                               | 60073/200000 [02:38&lt;04:36, 505.30it/s] 35%|███████████████████████████████████████████████████████▋                                                                                                       | 70022/200000 [03:17&lt;16:32, 130.95it/s] 40%|███████████████████████████████████████████████████████████████▋                                                                                               | 80090/200000 [03:41&lt;04:08, 483.44it/s] 45%|███████████████████████████████████████████████████████████████████████▌                                                                                       | 90063/200000 [04:00&lt;03:17, 557.71it/s] 50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 100101/200000 [04:20&lt;03:10, 523.81it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 110071/200000 [04:39&lt;03:39, 410.30it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                               | 120058/200000 [05:16&lt;02:55, 454.36it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 130062/200000 [05:37&lt;02:17, 506.81it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 140096/200000 [05:56&lt;01:53, 527.01it/s] 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 150095/200000 [06:15&lt;01:35, 525.17it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 160090/200000 [06:35&lt;01:19, 502.85it/s] 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 170102/200000 [06:54&lt;00:59, 501.19it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 180063/200000 [07:13&lt;00:38, 511.77it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 190075/200000 [07:32&lt;00:18, 540.90it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [07:51&lt;00:00, 424.08it/s]\n\n\n0 3.2974765300750732\n10000 2.2444629669189453\n20000 2.0267117023468018\n30000 2.3122177124023438\n40000 1.8772399425506592\n50000 1.8241453170776367\n60000 2.2491812705993652\n70000 2.332838535308838\n80000 2.2603352069854736\n90000 2.521674394607544\n100000 2.1766295433044434\n110000 2.0648574829101562\n120000 1.9632437229156494\n130000 2.6266632080078125\n140000 1.9747267961502075\n150000 2.2220919132232666\n160000 2.2269341945648193\n170000 1.8781782388687134\n180000 2.018829107284546\n190000 1.694084644317627\n199999 1.8435885906219482\n\n\n\ndef evaluate_loss_v2(parameters, X, Y):\n    logits = compute_logits_v2(parameters, X)\n    return F.cross_entropy(logits, Y)\n\n\ndef loss_split_v2(parameters, dataset='train'):\n    dataset_choices = {\n        'train': (Xtr, Ytr),\n        'valid': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }\n    return evaluate_loss_v2(parameters, *dataset_choices[dataset])\n\n\nloss_split_v2(params8, 'train'), loss_split_v2(params8, 'valid')\n\n(tensor(2.0683, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1130, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\nConsider the Training set mean and std deviation\n\ndef compute_batchnorm_mean_std(parameters):\n    C, W1, b1, *rest = parameters\n    emb = C[Xtr]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the training set\n    bnmean = hpreact.mean(0, keepdim = True)\n    bnstd = hpreact.std(0, keepdim = True)\n    return bnmean, bnstd\n\n\ntrain_bnmean, train_bnstd = compute_batchnorm_mean_std(params8)\n\n\ndef compute_logits_v3(parameters, X, bnmean=None, bnstd=None):\n    C, W1, b1, W2, b2, bngain, bnbias = parameters\n    emb = C[X]\n    embcat = emb.view(-1, W1.shape[0]) \n    h_pre_act = embcat @ W1 + b1\n\n    if  bnmean is None  and bnstd is None:\n        bnmean = h_pre_act.mean(0, keepdim = True)\n        bnstd = h_pre_act.std(0, keepdim = True)\n        \n    h_pre_act_norm = (h_pre_act - bnmean) / bnstd\n    h_pre_act_scale_shift = bngain * h_pre_act_norm + bnbias\n    h = torch.tanh(h_pre_act_scale_shift)\n    logits = h @ W2 + b2\n    return logits\n\n\ndef evaluate_loss_v3(parameters, X, Y, bnmean, bnstd):\n    logits = compute_logits_v3(parameters, X, bnmean, bnstd)\n    return F.cross_entropy(logits, Y)\n\n\ndef loss_split_v3(parameters, bnmean, bnstd, dataset='train'):\n    dataset_choices = {\n        'train': (Xtr, Ytr),\n        'valid': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }\n    return evaluate_loss_v3(parameters, *dataset_choices[dataset], bnmean, bnstd)\n\n\nloss_split_v3(params8, train_bnmean, train_bnstd, 'train'), loss_split_v3(params8, train_bnmean, train_bnstd, 'valid')\n\n(tensor(2.0683, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1131, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\nCompute running mean and std\n\ndef initilialize_parameters_v7(block_size, embedding_size, hidden_neuron):\n    C = torch.randn((27, embedding_size), generator=g)\n    W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g) * ((5/3) / ((embedding_size * block_size) ** 0.5))\n    b1 = torch.randn(hidden_neuron, generator=g) * 0.01\n    W2 = torch.randn((hidden_neuron, 27), generator=g) * 0.01\n    b2 = torch.randn(27, generator=g) * 0\n    \n    bngain = torch.ones((1, hidden_neuron)) # for scale in batch normalization\n    bnbias = torch.zeros((1, hidden_neuron)) # for shift in batch normalization\n    \n    bnmean_running = torch.zeros((1, hidden_neuron))\n    bnstd_running = torch.ones((1, hidden_neuron))\n    \n    return [C, W1, b1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running]\n\n\ndef compute_logits_v4(parameters, X, step='training'):\n    C, W1, b1, W2, b2, bngain, bnbias, bnmean_running, bnstd_running = parameters\n    emb = C[X]\n    embcat = emb.view(-1, W1.shape[0]) \n    h_pre_act = embcat @ W1 + b1\n\n    if step == 'training':\n        bnmeani = h_pre_act.mean(0, keepdim = True)\n        bnstdi = h_pre_act.std(0, keepdim = True)\n\n        with torch.no_grad():\n            bnmean_running.data = 0.999 * bnmean_running.data + 0.001 * bnmeani\n            bnstd_running.data = 0.999 * bnstd_running.data + 0.001 * bnstdi\n    else:\n        bnmeani = bnmean_running\n        bnstdi = bnstd_running\n        \n    h_pre_act_norm = (h_pre_act - bnmeani) / bnstdi\n    h_pre_act_scale_shift = bngain * h_pre_act_norm + bnbias\n    h = torch.tanh(h_pre_act_scale_shift)\n    logits = h @ W2 + b2\n    return logits\n\n\ndef train_v3(parameters,\n          epochs,\n          X, \n          Y, \n          bs=32, \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        raise Exception(\"No initial parameters passed\")\n    \n    for p in parameters: p.requires_grad = True \n    \n    losses = [] \n    \n    for epoch in tqdm(range(epochs)):\n        \n        lr = 0.1 if epoch &lt; 100_000 else 0.01\n        \n        ix = torch.randint(0, X.shape[0], (bs, ))\n        batch_x, batch_y = X[ix], Y[ix]\n        \n        logits = compute_logits_v4(parameters, batch_x)\n        \n        loss = F.cross_entropy(logits, batch_y)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n            \n        loss.backward()\n\n\n        for index, p in enumerate(parameters):\n            if p.grad is not None: p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n            \n        losses.append(loss.item())\n        \n    if enable_print:  print(epoch, loss.item())   \n    return losses\n\n\nparams9 = initilialize_parameters_v7(3, 10, 200)\n\n\nlosses = train_v3(params9, 200_000, Xtr, Ytr)\n\n  0%|                                                                                                                                                                  | 53/200000 [00:00&lt;11:28, 290.39it/s]  5%|████████                                                                                                                                                       | 10064/200000 [00:21&lt;06:26, 491.17it/s] 10%|███████████████▉                                                                                                                                               | 20069/200000 [00:44&lt;06:25, 466.38it/s] 15%|███████████████████████▉                                                                                                                                       | 30094/200000 [01:05&lt;05:47, 489.06it/s] 20%|███████████████████████████████▊                                                                                                                               | 40057/200000 [01:25&lt;05:32, 481.69it/s] 25%|███████████████████████████████████████▊                                                                                                                       | 50058/200000 [01:47&lt;05:00, 499.70it/s] 30%|███████████████████████████████████████████████▊                                                                                                               | 60073/200000 [02:08&lt;04:49, 483.97it/s] 35%|███████████████████████████████████████████████████████▋                                                                                                       | 70031/200000 [02:36&lt;09:18, 232.65it/s] 40%|███████████████████████████████████████████████████████████████▋                                                                                               | 80046/200000 [03:29&lt;06:10, 323.66it/s] 45%|████████████████████████████████████████████████████████████████████████                                                                                        | 90023/200000 [04:09&lt;22:28, 81.54it/s] 50%|███████████████████████████████████████████████████████████████████████████████                                                                               | 100065/200000 [04:35&lt;03:46, 440.83it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 110070/200000 [05:05&lt;03:09, 474.93it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                               | 120084/200000 [05:26&lt;02:52, 462.69it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 130038/200000 [05:56&lt;03:10, 367.74it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 140079/200000 [06:35&lt;02:15, 443.66it/s] 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 150058/200000 [06:58&lt;02:12, 375.75it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 160053/200000 [07:30&lt;01:22, 482.75it/s] 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 170064/200000 [08:07&lt;01:10, 422.13it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 180042/200000 [08:39&lt;00:47, 422.86it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 190066/200000 [09:03&lt;00:22, 440.71it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [09:44&lt;00:00, 341.89it/s]\n\n\n0 3.2910704612731934\n10000 2.2731170654296875\n20000 2.676584482192993\n30000 2.425685405731201\n40000 2.1894543170928955\n50000 2.406543731689453\n60000 2.19313383102417\n70000 1.9194616079330444\n80000 2.9036688804626465\n90000 2.281238079071045\n100000 1.890221357345581\n110000 2.034389019012451\n120000 1.7974919080734253\n130000 2.2577686309814453\n140000 2.1341137886047363\n150000 2.03934907913208\n160000 2.2662088871002197\n170000 2.285844564437866\n180000 2.451364755630493\n190000 2.512420654296875\n199999 2.0207467079162598\n\n\n\ntrain_bmean, train_bstd = compute_batchnorm_mean_std(params9)\n\n\ntrain_bmean.shape\n\ntorch.Size([1, 200])\n\n\n\ntrain_bstd.shape\n\ntorch.Size([1, 200])\n\n\n\nparams9[-2]\n\ntensor([[ 0.0375, -0.6331, -0.3791, -0.9008,  0.2962,  1.4619, -0.2891,  0.1883,\n          0.3329,  0.2727, -0.9458, -0.6716, -0.5048, -0.6620,  0.2342,  1.1776,\n          0.5657,  1.9836, -0.2188, -0.1207, -1.8573,  0.3398,  0.3636,  2.3903,\n          0.4538,  0.0833, -1.0701, -0.7340, -0.3513,  1.9030, -0.7348, -0.4769,\n         -2.3932, -0.3250,  0.3072,  0.4761,  1.0564, -0.9924, -0.5760,  1.1480,\n         -1.0848,  1.0707,  1.4146, -1.5361, -1.7056, -1.0768,  0.5271, -0.1483,\n         -0.3467, -1.4341,  0.3082, -0.1178,  1.0577, -1.3717,  0.2063, -0.7474,\n         -0.5008,  0.7797, -0.4381, -0.5850, -0.6317,  0.9563, -1.2107,  0.2256,\n         -0.3417, -0.0634, -1.0470, -0.7367,  0.4666, -0.9433,  0.0427,  0.6610,\n         -0.0472, -0.6191, -0.2211, -0.3358, -0.1517, -0.5254,  1.0690, -1.2109,\n         -0.2712, -1.7430, -1.3447,  0.6727,  0.9114, -0.9647, -0.1962,  0.2689,\n         -0.5687,  0.5511, -0.1002,  0.4604, -0.3753, -0.3717, -0.8587, -0.2480,\n         -0.8048,  0.5067,  0.9874, -0.9155,  1.0679,  0.8766, -0.3299, -0.4363,\n          0.4756, -0.2838, -1.1314,  0.8018,  0.0026, -0.1068, -0.7755, -0.3257,\n          1.3104,  0.4733,  0.2502,  0.9611,  1.2929, -0.7287, -0.7842, -1.1771,\n          0.6131, -0.4987, -0.2233, -0.0505,  0.2554, -1.3391,  0.4501, -0.6442,\n         -0.2634, -0.3602,  2.0635,  0.6144, -0.2697,  1.7540, -2.0597,  0.7119,\n         -0.7171, -0.0732,  0.8917,  0.1726,  0.6225, -0.5065,  1.7100, -0.0922,\n          0.1529,  1.6846,  0.3420, -0.4169, -0.2356, -0.4468, -0.8213,  0.2331,\n         -2.2176, -0.0141,  0.2564,  0.1819,  0.0972,  0.4663,  0.5573,  0.5170,\n         -2.4660,  0.6317, -0.0866, -1.3206, -0.3167, -1.1647, -1.4368,  0.0419,\n         -0.6170, -0.8835,  0.8117, -0.8902, -1.6758,  1.4599,  0.6674, -0.5737,\n         -0.3982, -0.8486,  0.7648, -0.6740, -0.8773,  0.1644,  0.3821, -1.0831,\n          1.0532,  0.7580, -0.5744, -0.6737, -0.6705,  0.5464,  0.3474,  0.5626,\n         -0.8972, -0.6644, -0.4031, -1.2800,  1.5996, -0.8948, -1.6051, -0.8797]],\n       requires_grad=True)\n\n\n\ntrain_bmean\n\ntensor([[ 0.0336, -0.6265, -0.3734, -0.9142,  0.3122,  1.4727, -0.2720,  0.2043,\n          0.3280,  0.2649, -0.9420, -0.6603, -0.5234, -0.6541,  0.2307,  1.1702,\n          0.5762,  1.9697, -0.2134, -0.1228, -1.8548,  0.3674,  0.3822,  2.3695,\n          0.4678,  0.0884, -1.0760, -0.7574, -0.3623,  1.9024, -0.7362, -0.4469,\n         -2.3866, -0.3177,  0.2965,  0.4548,  1.0385, -0.9807, -0.5685,  1.1614,\n         -1.0926,  1.0664,  1.4133, -1.5146, -1.6854, -1.0843,  0.5003, -0.1437,\n         -0.3558, -1.4272,  0.2930, -0.1223,  1.0598, -1.3414,  0.2255, -0.7481,\n         -0.5164,  0.7950, -0.4377, -0.5765, -0.6527,  0.9657, -1.1949,  0.2556,\n         -0.3367, -0.0697, -1.0539, -0.7473,  0.4742, -0.9174,  0.0496,  0.6626,\n         -0.0252, -0.6193, -0.2340, -0.3298, -0.1581, -0.5270,  1.0956, -1.1991,\n         -0.2696, -1.7306, -1.3725,  0.6711,  0.9122, -0.9572, -0.1943,  0.2736,\n         -0.5639,  0.5646, -0.0927,  0.4803, -0.3902, -0.3292, -0.8637, -0.2507,\n         -0.8104,  0.5088,  0.9935, -0.9224,  1.0957,  0.8640, -0.3443, -0.4084,\n          0.4823, -0.2982, -1.1175,  0.8094,  0.0229, -0.1139, -0.7825, -0.3265,\n          1.3089,  0.4729,  0.2671,  0.9844,  1.3121, -0.7067, -0.8011, -1.1575,\n          0.6211, -0.5021, -0.2209, -0.0643,  0.2590, -1.3219,  0.4299, -0.6472,\n         -0.2602, -0.3664,  2.0622,  0.6203, -0.2817,  1.7380, -2.0716,  0.7414,\n         -0.6948, -0.0536,  0.8889,  0.1630,  0.6067, -0.5124,  1.7246, -0.0809,\n          0.1703,  1.6875,  0.3339, -0.4017, -0.2522, -0.4726, -0.8133,  0.2514,\n         -2.2188, -0.0041,  0.2641,  0.1699,  0.0992,  0.4487,  0.5679,  0.5218,\n         -2.4712,  0.6221, -0.0852, -1.3236, -0.3386, -1.1213, -1.4408,  0.0377,\n         -0.5953, -0.8718,  0.8178, -0.9079, -1.6565,  1.4652,  0.6479, -0.5730,\n         -0.4037, -0.8535,  0.7510, -0.6731, -0.8535,  0.1698,  0.3929, -1.0634,\n          1.0645,  0.7542, -0.5708, -0.6714, -0.6442,  0.5538,  0.3533,  0.5629,\n         -0.8813, -0.6482, -0.3858, -1.2797,  1.6030, -0.8916, -1.6031, -0.8928]],\n       grad_fn=&lt;MeanBackward1&gt;)\n\n\n\nparams9[-1]\n\ntensor([[2.4632, 2.3683, 2.4826, 2.0530, 2.1001, 2.5936, 2.4268, 1.8161, 2.2527,\n         2.2914, 2.0653, 1.7652, 2.0370, 2.3531, 2.3336, 2.2467, 2.3725, 2.4981,\n         2.1250, 1.9589, 2.4479, 2.1229, 1.9701, 2.6517, 2.4070, 2.2383, 1.5956,\n         1.9934, 2.2423, 2.3018, 2.1449, 1.9444, 2.1350, 2.3635, 2.1110, 2.2966,\n         1.9588, 2.2902, 2.0242, 2.2285, 2.6163, 2.5771, 1.9682, 2.4096, 2.1897,\n         1.9587, 2.5828, 2.2682, 1.5224, 2.2569, 2.0790, 2.0309, 2.7052, 2.0490,\n         2.6919, 2.7425, 1.6170, 2.2639, 2.2183, 2.4126, 2.5572, 2.1070, 2.3111,\n         2.1343, 2.4835, 1.9523, 2.4436, 2.1352, 2.6667, 2.5792, 2.4142, 2.3900,\n         1.8665, 2.1212, 2.2905, 2.1226, 1.9209, 2.4108, 2.4251, 1.9492, 2.0006,\n         2.7582, 2.5923, 2.1482, 1.9433, 1.8152, 2.2074, 1.9798, 2.1282, 2.5727,\n         2.2498, 2.1983, 2.3262, 2.6791, 2.0241, 2.0521, 2.2381, 2.0871, 2.0417,\n         2.5972, 2.0449, 2.4388, 1.9639, 2.2393, 2.1035, 2.1849, 1.9384, 2.3872,\n         2.5280, 2.6528, 2.2955, 1.9553, 2.3484, 2.3475, 2.7836, 2.1356, 2.3427,\n         2.0554, 2.3580, 1.9564, 2.2688, 2.2788, 2.2936, 2.1819, 2.2038, 2.3220,\n         2.2896, 1.9991, 2.0549, 2.1163, 2.6239, 1.5893, 2.8965, 2.0469, 2.2779,\n         2.1321, 2.1158, 1.8507, 2.3508, 1.9726, 1.9283, 2.2762, 1.9608, 2.4423,\n         2.0968, 2.0759, 2.7557, 3.1357, 2.2457, 2.5234, 2.3572, 2.6196, 2.2824,\n         2.1964, 2.4175, 1.3403, 2.5489, 2.2041, 2.5038, 1.9908, 2.0546, 2.3802,\n         2.4392, 2.2461, 2.1533, 2.1316, 2.4615, 1.8033, 2.3087, 1.9742, 2.3235,\n         1.7176, 2.0494, 2.3848, 2.3092, 2.4218, 2.2263, 2.4015, 2.1627, 2.1673,\n         2.3420, 2.0868, 2.7352, 2.4064, 2.0937, 2.4994, 1.7547, 2.3966, 2.3889,\n         1.9188, 2.1525, 2.0753, 2.1131, 2.1583, 2.1470, 2.2530, 2.6288, 1.9458,\n         2.1839, 2.2465]], requires_grad=True)\n\n\n\ntrain_bstd\n\ntensor([[2.4732, 2.3784, 2.5213, 2.0699, 2.1313, 2.6214, 2.4610, 1.8370, 2.2874,\n         2.3359, 2.0795, 1.7793, 2.0715, 2.3730, 2.3941, 2.2722, 2.4023, 2.5138,\n         2.1447, 1.9971, 2.4711, 2.1447, 1.9872, 2.6469, 2.4311, 2.2664, 1.6039,\n         2.0137, 2.2677, 2.3347, 2.1688, 1.9666, 2.1642, 2.4079, 2.1322, 2.3575,\n         1.9967, 2.3048, 2.0512, 2.2445, 2.6568, 2.6134, 1.9959, 2.4373, 2.2055,\n         1.9620, 2.6040, 2.3006, 1.5517, 2.2881, 2.1018, 2.0604, 2.7347, 2.0694,\n         2.7125, 2.7579, 1.6329, 2.3031, 2.2478, 2.4416, 2.5732, 2.1449, 2.3496,\n         2.1591, 2.5255, 1.9768, 2.4779, 2.1505, 2.6936, 2.5978, 2.4561, 2.3916,\n         1.8887, 2.1492, 2.3122, 2.1676, 1.9488, 2.4345, 2.4486, 1.9695, 2.0231,\n         2.7833, 2.6296, 2.1697, 1.9649, 1.8332, 2.2276, 1.9933, 2.1393, 2.5949,\n         2.2839, 2.2298, 2.3553, 2.7204, 2.0429, 2.0738, 2.2546, 2.1089, 2.0694,\n         2.6374, 2.0650, 2.4688, 1.9873, 2.2620, 2.1333, 2.2223, 1.9737, 2.4104,\n         2.5586, 2.6578, 2.3239, 1.9960, 2.3708, 2.3778, 2.8150, 2.1605, 2.3796,\n         2.0766, 2.3811, 1.9827, 2.2918, 2.3128, 2.3298, 2.2081, 2.2340, 2.3566,\n         2.3248, 2.0105, 2.0888, 2.1318, 2.6333, 1.6082, 2.9117, 2.0752, 2.3033,\n         2.1490, 2.1393, 1.8752, 2.3683, 1.9951, 1.9514, 2.3120, 1.9701, 2.4542,\n         2.1293, 2.0938, 2.7844, 3.1507, 2.2620, 2.5633, 2.3879, 2.6383, 2.3134,\n         2.2227, 2.4502, 1.3551, 2.5690, 2.2434, 2.5294, 2.0216, 2.0723, 2.4025,\n         2.4604, 2.2773, 2.1765, 2.1551, 2.4940, 1.8347, 2.3483, 1.9932, 2.3455,\n         1.7362, 2.0621, 2.4156, 2.3354, 2.4520, 2.2545, 2.4163, 2.1852, 2.1869,\n         2.3587, 2.1191, 2.7597, 2.4383, 2.1112, 2.5392, 1.7744, 2.4260, 2.4106,\n         1.9521, 2.1830, 2.1063, 2.1329, 2.1864, 2.1679, 2.2876, 2.6466, 1.9717,\n         2.1994, 2.2678]], grad_fn=&lt;StdBackward0&gt;)\n\n\nabove values are close\n\ndef evaluate_loss_v4(parameters, X, Y, bnmean, bnstd):\n    logits = compute_logits_v4(parameters, X, 'evaluation')\n    return F.cross_entropy(logits, Y)\n\n\ndef loss_split_v4(parameters, bnmean, bnstd, dataset='train'):\n    dataset_choices = {\n        'train': (Xtr, Ytr),\n        'valid': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }\n    return evaluate_loss_v4(parameters, *dataset_choices[dataset], bnmean, bnstd)\n\n\nloss_split_v4(params9, params9[-2], params9[-1], 'train'), loss_split_v4(params9, train_bnmean, train_bnstd, 'valid')\n\n(tensor(2.0704, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1112, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp2.html#pytorchifying-the-code",
    "href": "lecture_notes/building_makemore_mlp2.html#pytorchifying-the-code",
    "title": "Building makemore - Activations & Gradients, BatchNorm",
    "section": "Pytorchifying the code",
    "text": "Pytorchifying the code\n\ng = torch.Generator().manual_seed(2147483647)\n\n\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in ** 0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running `monentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var  = torch.ones(dim)\n    \n    def __call__(self, x):\n        if self.training:\n            xmean = x.mean(0, keepdim=True) # batch mean\n            xvar = x.var(0, keepdim=True, unbiased=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        x_hat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * x_hat + self.beta\n        \n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean  + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    def parameters(self):\n        return []\n\n\ndef train(max_steps, X, Y, \n            bs = 32, \n            vocab_size = 27, # the number of characters\n            n_embd = 10, # the dimensionality of the character embedding vectors\n            n_hidden = 100, # the number of neurons in the hidden layer of the MLP\n            block_size = 3, \n            weight_scale = 5/3,\n            network_type = 'non-linear',\n            learning_rate = None\n         ):\n\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    \n    if network_type == 'linear':\n        layers = [\n            Linear(n_embd * block_size, n_hidden),\n            Linear(n_hidden, n_hidden),\n            Linear(n_hidden, n_hidden), \n            Linear(n_hidden, n_hidden), \n            Linear(n_hidden, n_hidden), \n            Linear(n_hidden, vocab_size)\n        ]\n    elif network_type == 'non-linear':\n        layers = [\n            Linear(n_embd * block_size, n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), Tanh(),\n            Linear(n_hidden, vocab_size)\n        ]\n    else:\n        layers = [\n            Linear(n_embd * block_size, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n            Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n            Linear(n_hidden, vocab_size), BatchNorm1d(vocab_size)\n        ]\n    \n    with torch.no_grad():\n        # last layer: make less confident\n        if network_type != 'batchnorm':\n            layers[-1].weight *= 0.1\n        else:\n            layers[-1].gamma *= 0.1\n        # all other layers: apply gain\n        for layer in layers[:-1]:\n            if isinstance(layer, Linear):\n                layer.weight *= weight_scale\n                \n    \n    parameters = [C] + [p for layer in layers for p in layer.parameters()]\n    print('Total number of parameters are :', sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True \n        \n    lossi = []\n    ud = []\n    for i in range(max_steps):\n        # minibatch construct\n        ix = torch.randint(0, X.shape[0], (bs, ), generator = g)\n        Xb, Yb = X[ix], Y[ix]\n        \n        # forward pass\n        emb = C[Xb]\n        x = emb.view(emb.shape[0], -1)\n        for layer in layers:\n            x = layer(x)\n        loss = F.cross_entropy(x, Yb)\n        \n        for layer in layers:\n            layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n        \n        for p in parameters:\n            p.grad = None\n        \n        loss.backward()\n        \n        lr = 0.1 if i &lt; 100_000 else 0.01 # step learning rate decay\n        if learning_rate: lr = learning_rate\n        \n        \n        for p in parameters:\n            p.data += -lr * p.grad\n        \n        # track stats\n        if i % 10_000 == 0: # print every once in a while\n            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n        lossi.append(loss.log10().item())\n        \n        with torch.no_grad():\n            ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in parameters])\n    return layers, parameters, lossi, ud\n\n\nl, p, li, ud = train(1, Xtr, Ytr, network_type = 'non-linear')\n\nTotal number of parameters are : 46497\n      0/      1: 3.3099\n\n\n\ndef visualize_histograms(layers, instance_layer = Tanh, output_type='forward'):\n    plot.figure(figsize=(20, 4))\n    legends = []\n    for i, layer in enumerate(layers[:-1]):\n        if isinstance(layer, instance_layer):\n            if output_type == 'forward': t = layer.out\n            else: t = layer.out.grad\n            print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n            hy, hx = torch.histogram(t, density=True)\n            plot.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f'layer {i} ({layer.__class__.__name__})')\n    plot.legend(legends)\n    plot.title('gradient distribution')\n\n\nvisualize_histograms(l)\n\nlayer 1 (      Tanh): mean -0.023409, std 7.497526e-01\nlayer 3 (      Tanh): mean -0.002852, std 6.864228e-01\nlayer 5 (      Tanh): mean +0.001338, std 6.732427e-01\nlayer 7 (      Tanh): mean -0.006005, std 6.569249e-01\nlayer 9 (      Tanh): mean -0.020739, std 6.626034e-01\n\n\n\n\n\n\nvisualize_histograms(l, output_type='backward')\n\nlayer 1 (      Tanh): mean +0.000010, std 4.205588e-04\nlayer 3 (      Tanh): mean -0.000003, std 3.991179e-04\nlayer 5 (      Tanh): mean +0.000003, std 3.743020e-04\nlayer 7 (      Tanh): mean +0.000015, std 3.290473e-04\nlayer 9 (      Tanh): mean -0.000014, std 3.054035e-04\n\n\n\n\n\n\nOnly Non Linearities\n\nl, p, li, ud = train(1, Xtr, Ytr, network_type='linear')\n\nTotal number of parameters are : 46497\n      0/      1: 3.9508\n\n\n\nvisualize_histograms(l, instance_layer=Linear)\n\nlayer 0 (    Linear): mean -0.026808, std 1.717835e+00\nlayer 1 (    Linear): mean +0.066778, std 2.721437e+00\nlayer 2 (    Linear): mean +0.011798, std 4.644542e+00\nlayer 3 (    Linear): mean +0.402860, std 7.625082e+00\nlayer 4 (    Linear): mean +0.125483, std 1.243459e+01\n\n\n\n\n\n\nvisualize_histograms(l, instance_layer=Linear, output_type='backward')\n\nlayer 0 (    Linear): mean -0.000040, std 2.520851e-03\nlayer 1 (    Linear): mean +0.000041, std 1.514516e-03\nlayer 2 (    Linear): mean +0.000030, std 9.373975e-04\nlayer 3 (    Linear): mean +0.000000, std 5.414668e-04\nlayer 4 (    Linear): mean +0.000006, std 3.237360e-04\n\n\n\n\n\n\nl, p, li, ud = train(1, Xtr, Ytr, weight_scale = 0.5, network_type='linear')\n\nTotal number of parameters are : 46497\n      0/      1: 3.2955\n\n\n\nvisualize_histograms(l, instance_layer=Linear)\n\nlayer 0 (    Linear): mean +0.005062, std 5.380081e-01\nlayer 1 (    Linear): mean +0.003729, std 2.749813e-01\nlayer 2 (    Linear): mean +0.004225, std 1.427801e-01\nlayer 3 (    Linear): mean +0.000118, std 7.137968e-02\nlayer 4 (    Linear): mean -0.000081, std 3.339273e-02\n\n\n\n\n\n\nvisualize_histograms(l, instance_layer=Linear, output_type='backward')\n\nlayer 0 (    Linear): mean -0.000001, std 2.208486e-05\nlayer 1 (    Linear): mean +0.000000, std 4.229027e-05\nlayer 2 (    Linear): mean +0.000002, std 8.004090e-05\nlayer 3 (    Linear): mean +0.000003, std 1.509417e-04\nlayer 4 (    Linear): mean +0.000007, std 3.050811e-04\n\n\n\n\n\n\nl, p, li, ud = train(1, Xtr, Ytr, weight_scale = 1, network_type='linear')\n\nTotal number of parameters are : 46497\n      0/      1: 3.2962\n\n\n\nvisualize_histograms(l, instance_layer=Linear)\n\nlayer 0 (    Linear): mean -0.033228, std 1.018080e+00\nlayer 1 (    Linear): mean +0.032009, std 9.739050e-01\nlayer 2 (    Linear): mean -0.021459, std 9.661991e-01\nlayer 3 (    Linear): mean -0.006396, std 9.748541e-01\nlayer 4 (    Linear): mean +0.008816, std 1.019902e+00\n\n\n\n\n\n\nvisualize_histograms(l, instance_layer=Linear, output_type='backward')\n\nlayer 0 (    Linear): mean +0.000011, std 3.135176e-04\nlayer 1 (    Linear): mean +0.000007, std 3.096962e-04\nlayer 2 (    Linear): mean -0.000003, std 3.211265e-04\nlayer 3 (    Linear): mean -0.000011, std 3.192310e-04\nlayer 4 (    Linear): mean -0.000010, std 3.158194e-04\n\n\n\n\n\n\ngrad: data ratio\n\nl, p, li, ud = train(1, Xtr, Ytr, weight_scale=1, network_type='linear')\n\nTotal number of parameters are : 46497\n      0/      1: 3.2988\n\n\n\ndef visualize_gain_data_ratio(parameters):\n    plot.figure(figsize=(20, 4))\n    legends = []\n    for i, p in enumerate(parameters):\n        t = p.grad\n        if p.ndim==2: # excluding bias, gamma, beta\n            print('weight %10s | mean %+f | std %e | grad:data ratio %e' %(tuple(p.shape), t.mean(), t.std(), t.std()/p.std()))\n            hy, hx = torch.histogram(t, density = True)\n            plot.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f'{i}{tuple(p.shape)}')\n    plot.legend(legends)\n    plot.title('Weights gradient distribution')\n\n\nvisualize_gain_data_ratio(p)\n\nweight   (27, 10) | mean -0.000028 | std 9.268780e-04 | grad:data ratio 8.800050e-04\nweight  (30, 100) | mean -0.000030 | std 1.736440e-03 | grad:data ratio 9.734222e-03\nweight (100, 100) | mean -0.000002 | std 1.681667e-03 | grad:data ratio 1.686680e-02\nweight (100, 100) | mean +0.000011 | std 1.695616e-03 | grad:data ratio 1.695003e-02\nweight (100, 100) | mean -0.000042 | std 1.680904e-03 | grad:data ratio 1.670431e-02\nweight (100, 100) | mean -0.000029 | std 1.830903e-03 | grad:data ratio 1.843546e-02\nweight  (100, 27) | mean -0.000000 | std 3.505145e-02 | grad:data ratio 3.360777e+00\n\n\n\n\n\n\nl, p, li, ud = train(1000, Xtr, Ytr, weight_scale=1, network_type='linear')\n\nTotal number of parameters are : 46497\n      0/   1000: 3.2966\n\n\n\nvisualize_gain_data_ratio(p)\n\nweight   (27, 10) | mean +0.001294 | std 1.157769e-02 | grad:data ratio 1.172457e-02\nweight  (30, 100) | mean -0.000177 | std 1.309982e-02 | grad:data ratio 7.482659e-02\nweight (100, 100) | mean +0.000006 | std 8.080219e-03 | grad:data ratio 8.206636e-02\nweight (100, 100) | mean -0.000033 | std 6.700047e-03 | grad:data ratio 7.002961e-02\nweight (100, 100) | mean +0.000095 | std 6.837256e-03 | grad:data ratio 7.082513e-02\nweight (100, 100) | mean -0.000055 | std 6.807048e-03 | grad:data ratio 7.096651e-02\nweight  (100, 27) | mean +0.000000 | std 2.501121e-02 | grad:data ratio 4.598068e-01\n\n\n\n\n\n\n\ndata ratio over time\n\ndef visualize_data_ratio_over_time(parameters, ud):\n    plot.figure(figsize=(20, 4))\n    legends = []\n    for i, p in enumerate(parameters):\n        if p.ndim == 2:\n            plot.plot([ud[j][i] for j in range(len(ud))])\n            legends.append('param %d' % i)\n    plot.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~ 1e-3, indicate on plot\n    plot.legend(legends);\n\n\nvisualize_data_ratio_over_time(p, ud)\n\n\n\n\n\nl, p, li, ud = train(1000, Xtr, Ytr, weight_scale=1, network_type='linear', learning_rate=0.001)\n\nTotal number of parameters are : 46497\n      0/   1000: 3.3380\n\n\n\nvisualize_data_ratio_over_time(p, ud)\n\n\n\n\n\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=g) #/ fan_in ** 0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nl, p, li, ud = train(1000, Xtr, Ytr)\n\nTotal number of parameters are : 46497\n      0/   1000: 3.7327\n\n\n\nvisualize_data_ratio_over_time(p, ud)\n\n\n\n\n\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in ** 0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nl, p, li, ud = train(1000, Xtr, Ytr, network_type = 'batchnorm')\n\nTotal number of parameters are : 47551\n      0/   1000: 3.3050\n\n\n\nvisualize_histograms(l)\n\nlayer 2 (      Tanh): mean -0.002824, std 6.360319e-01\nlayer 5 (      Tanh): mean -0.003577, std 6.418643e-01\nlayer 8 (      Tanh): mean -0.000634, std 6.393718e-01\nlayer 11 (      Tanh): mean +0.004614, std 6.429055e-01\nlayer 14 (      Tanh): mean -0.005592, std 6.441681e-01\n\n\n\n\n\n\nvisualize_histograms(l, output_type='backward')\n\nlayer 2 (      Tanh): mean +0.000000, std 3.329200e-03\nlayer 5 (      Tanh): mean +0.000000, std 3.038263e-03\nlayer 8 (      Tanh): mean -0.000000, std 2.737059e-03\nlayer 11 (      Tanh): mean +0.000000, std 2.601666e-03\nlayer 14 (      Tanh): mean -0.000000, std 2.437597e-03\n\n\n\n\n\n\nvisualize_data_ratio_over_time(p, ud)\n\n\n\n\n\nl, p, li, ud = train(1000, Xtr, Ytr, weight_scale=0.2, network_type = 'batchnorm')\n\nTotal number of parameters are : 47551\n      0/   1000: 3.2990\n\n\n\nvisualize_histograms(l)\n\nlayer 2 (      Tanh): mean -0.001280, std 6.382423e-01\nlayer 5 (      Tanh): mean +0.004930, std 6.569912e-01\nlayer 8 (      Tanh): mean -0.003945, std 6.697033e-01\nlayer 11 (      Tanh): mean -0.000414, std 6.793090e-01\nlayer 14 (      Tanh): mean -0.002082, std 6.810060e-01\n\n\n\n\n\n\nvisualize_histograms(l, output_type='backward')\n\nlayer 2 (      Tanh): mean -0.000000, std 1.021346e-03\nlayer 5 (      Tanh): mean -0.000000, std 8.389445e-04\nlayer 8 (      Tanh): mean +0.000000, std 8.275748e-04\nlayer 11 (      Tanh): mean +0.000000, std 8.728803e-04\nlayer 14 (      Tanh): mean +0.000000, std 1.020851e-03\n\n\n\n\n\n\nvisualize_data_ratio_over_time(p, ud)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html",
    "href": "lecture_notes/building_makemore_mlp.html",
    "title": "Building makemore - MLP",
    "section": "",
    "text": "This lecture note will mostly be following the https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf paper\nimport numpy\nimport torch\nimport torch.nn.functional as F\nfrom rich import print\nfrom rich import pretty\nimport matplotlib.pyplot as plot\nimport random\npretty.install()\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#read-in-all-the-words",
    "href": "lecture_notes/building_makemore_mlp.html#read-in-all-the-words",
    "title": "Building makemore - MLP",
    "section": "Read in all the words",
    "text": "Read in all the words\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\nlen(words)\n\n32033"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#build-the-vocabulary-of-characters-and-mapping-tofrom-integers",
    "href": "lecture_notes/building_makemore_mlp.html#build-the-vocabulary-of-characters-and-mapping-tofrom-integers",
    "title": "Building makemore - MLP",
    "section": "Build the vocabulary of characters and mapping to/from integers",
    "text": "Build the vocabulary of characters and mapping to/from integers\n\nchars = sorted(list(set(''.join(words))))\n\n\nstoi = {s: i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\n\n\nitos = {i:s for s, i in stoi.items()}\n\n\nitos\n\n{\n    1: 'a',\n    2: 'b',\n    3: 'c',\n    4: 'd',\n    5: 'e',\n    6: 'f',\n    7: 'g',\n    8: 'h',\n    9: 'i',\n    10: 'j',\n    11: 'k',\n    12: 'l',\n    13: 'm',\n    14: 'n',\n    15: 'o',\n    16: 'p',\n    17: 'q',\n    18: 'r',\n    19: 's',\n    20: 't',\n    21: 'u',\n    22: 'v',\n    23: 'w',\n    24: 'x',\n    25: 'y',\n    26: 'z',\n    0: '.'\n}"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#build-the-training-dataset",
    "href": "lecture_notes/building_makemore_mlp.html#build-the-training-dataset",
    "title": "Building makemore - MLP",
    "section": "Build the training dataset",
    "text": "Build the training dataset\n\nblock_size = 3\n\nWhat is block_size? &gt; context length: how many characters do we take to predict the next one?\n\ndef generate_training_set(words, block_size, print_disabled=False):\n    X, Y = [], []\n    for w in words:\n        if print_disabled: print(w)\n        \n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            if print_disabled: print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n            context = context[1:] + [ix] # crop and append\n            \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y\n\n\nGenerating training set for block size of 2\n\nX, Y = generate_training_set(words[:2], 2, True)\n\nemma\n\n\n\n.. ---&gt; e\n\n\n\n.e ---&gt; m\n\n\n\nem ---&gt; m\n\n\n\nmm ---&gt; a\n\n\n\nma ---&gt; .\n\n\n\nolivia\n\n\n\n.. ---&gt; o\n\n\n\n.o ---&gt; l\n\n\n\nol ---&gt; i\n\n\n\nli ---&gt; v\n\n\n\niv ---&gt; i\n\n\n\nvi ---&gt; a\n\n\n\nia ---&gt; .\n\n\n\n\n\nGenerating training set for block size of 5\n\nX, Y = generate_training_set(words[:2], 5, True)\n\nemma\n\n\n\n..... ---&gt; e\n\n\n\n....e ---&gt; m\n\n\n\n...em ---&gt; m\n\n\n\n..emm ---&gt; a\n\n\n\n.emma ---&gt; .\n\n\n\nolivia\n\n\n\n..... ---&gt; o\n\n\n\n....o ---&gt; l\n\n\n\n...ol ---&gt; i\n\n\n\n..oli ---&gt; v\n\n\n\n.oliv ---&gt; i\n\n\n\nolivi ---&gt; a\n\n\n\nlivia ---&gt; .\n\n\n\n\n\nGenerate training set for block size of 3\n\nas mentioned in the above paper\n\n\nX, Y = generate_training_set(words[:2], 3)\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([12, 3]), torch.int64, torch.Size([12]), torch.int64)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#embedding-lookup-table",
    "href": "lecture_notes/building_makemore_mlp.html#embedding-lookup-table",
    "title": "Building makemore - MLP",
    "section": "Embedding Lookup Table",
    "text": "Embedding Lookup Table\n\nC = torch.randn((27, 2))\n\n\nC.dtype\n\ntorch.float32\n\n\n\n\nC\n\ntensor([[ 0.0494, -0.0211],\n        [ 0.6477, -0.5360],\n        [-0.9756, -1.5765],\n        [-0.0935, -0.5134],\n        [-0.3660, -1.1029],\n        [ 1.3134, -0.4618],\n        [-0.9528,  0.0358],\n        [ 0.5850,  0.0884],\n        [-0.6457,  0.5524],\n        [-0.5182,  0.1713],\n        [-1.1681, -1.2686],\n        [-0.2436, -0.2827],\n        [ 1.7086,  0.4918],\n        [-1.1856,  1.1905],\n        [ 0.6216, -1.3340],\n        [ 0.3233, -0.4307],\n        [ 0.0207,  0.2296],\n        [-1.4710,  0.0351],\n        [-0.7824,  1.2608],\n        [-0.6630,  0.0953],\n        [-0.3874, -0.9611],\n        [ 0.2068, -0.2570],\n        [-1.0127, -1.0864],\n        [-0.3755, -1.8281],\n        [ 0.6910,  0.0300],\n        [-0.2178, -1.1080],\n        [-1.0800, -1.2573]])\n\n\n\n\nIndexing into the embedding\n\n1. Index with a number\n\nC[5]\n\ntensor([ 1.3134, -0.4618])\n\n\n\n\n\n2. Multiplying with one-hot encoding\n\none_hot_vec = F.one_hot(torch.tensor(5), num_classes=27).float()\n\nConverted the above one into float so that it can be multiplied with C, which is a float\n\none_hot_vec.shape\n\ntorch.Size([27])\n\n\n\n\none_hot_vec @ C\n\ntensor([ 1.3134, -0.4618])\n\n\n\nAs we can see the result is same as indexing with a number\n\ntorch.equal(one_hot_vec @ C, C[5])\n\nTrue\n\n\n\n\nLets explore indexing\n\nC[torch.tensor([5, 6, 7, 7, 7, 7, 7])]\n\ntensor([[ 1.3134, -0.4618],\n        [-0.9528,  0.0358],\n        [ 0.5850,  0.0884],\n        [ 0.5850,  0.0884],\n        [ 0.5850,  0.0884],\n        [ 0.5850,  0.0884],\n        [ 0.5850,  0.0884]])\n\n\n\n\nC[X]\n\ntensor([[[ 0.0494, -0.0211],\n         [ 0.0494, -0.0211],\n         [ 0.0494, -0.0211]],\n\n        [[ 0.0494, -0.0211],\n         [ 0.0494, -0.0211],\n         [ 1.3134, -0.4618]],\n\n        [[ 0.0494, -0.0211],\n         [ 1.3134, -0.4618],\n         [-1.1856,  1.1905]],\n\n        [[ 1.3134, -0.4618],\n         [-1.1856,  1.1905],\n         [-1.1856,  1.1905]],\n\n        [[-1.1856,  1.1905],\n         [-1.1856,  1.1905],\n         [ 0.6477, -0.5360]],\n\n        [[ 0.0494, -0.0211],\n         [ 0.0494, -0.0211],\n         [ 0.0494, -0.0211]],\n\n        [[ 0.0494, -0.0211],\n         [ 0.0494, -0.0211],\n         [ 0.3233, -0.4307]],\n\n        [[ 0.0494, -0.0211],\n         [ 0.3233, -0.4307],\n         [ 1.7086,  0.4918]],\n\n        [[ 0.3233, -0.4307],\n         [ 1.7086,  0.4918],\n         [-0.5182,  0.1713]],\n\n        [[ 1.7086,  0.4918],\n         [-0.5182,  0.1713],\n         [-1.0127, -1.0864]],\n\n        [[-0.5182,  0.1713],\n         [-1.0127, -1.0864],\n         [-0.5182,  0.1713]],\n\n        [[-1.0127, -1.0864],\n         [-0.5182,  0.1713],\n         [ 0.6477, -0.5360]]])\n\n\n\n\nC[X].shape\n\ntorch.Size([12, 3, 2])\n\n\n\n\nX[11, 2]\n\ntensor(1)\n\n\n\n\nC[X][11, 2]\n\ntensor([ 0.6477, -0.5360])\n\n\n\n\nC[1]\n\ntensor([ 0.6477, -0.5360])\n\n\n\n\ntorch.equal(C[X][11, 2], C[1])\n\nTrue"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#hidden-layer",
    "href": "lecture_notes/building_makemore_mlp.html#hidden-layer",
    "title": "Building makemore - MLP",
    "section": "Hidden Layer",
    "text": "Hidden Layer\n\nemb = C[X]\nemb.shape\n\ntorch.Size([12, 3, 2])\n\n\n\n\nW1 = torch.randn((6, 100))\nb1 = torch.randn(100)\n\n\nW1.shape, b1.shape\n\n(torch.Size([6, 100]), torch.Size([100]))\n\n\n\nWe have to flatten the last two dimensions of emb so that it can be multiplied with W1\n\nemb[:, 0, :].shape\n\ntorch.Size([12, 2])\n\n\n\n\ntorch.cat and torch.bind\n\ntorch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape\n\ntorch.Size([12, 6])\n\n\n\n\nlen(torch.unbind(emb, 1))\n\n3\n\n\n\n\ntorch.cat(torch.unbind(emb, 1), 1).shape\n\ntorch.Size([12, 6])\n\n\n\nThe above one is insufficient and creates memory\n\n\ntorch internal: storage, views\n\na = torch.arange(18)\na\n\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n\n\n\n\na.shape\n\ntorch.Size([18])\n\n\n\n\na.view(2, 9)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])\n\n\n\n\na.view(9, 2)\n\ntensor([[ 0,  1],\n        [ 2,  3],\n        [ 4,  5],\n        [ 6,  7],\n        [ 8,  9],\n        [10, 11],\n        [12, 13],\n        [14, 15],\n        [16, 17]])\n\n\n\n\na.view(3, 3, 2)\n\ntensor([[[ 0,  1],\n         [ 2,  3],\n         [ 4,  5]],\n\n        [[ 6,  7],\n         [ 8,  9],\n         [10, 11]],\n\n        [[12, 13],\n         [14, 15],\n         [16, 17]]])\n\n\n\n\na.storage()\n\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]\n\n\n\n\nemb.shape\n\ntorch.Size([12, 3, 2])\n\n\n\n\nemb.view(12, 6)\n\ntensor([[ 0.0494, -0.0211,  0.0494, -0.0211,  0.0494, -0.0211],\n        [ 0.0494, -0.0211,  0.0494, -0.0211,  1.3134, -0.4618],\n        [ 0.0494, -0.0211,  1.3134, -0.4618, -1.1856,  1.1905],\n        [ 1.3134, -0.4618, -1.1856,  1.1905, -1.1856,  1.1905],\n        [-1.1856,  1.1905, -1.1856,  1.1905,  0.6477, -0.5360],\n        [ 0.0494, -0.0211,  0.0494, -0.0211,  0.0494, -0.0211],\n        [ 0.0494, -0.0211,  0.0494, -0.0211,  0.3233, -0.4307],\n        [ 0.0494, -0.0211,  0.3233, -0.4307,  1.7086,  0.4918],\n        [ 0.3233, -0.4307,  1.7086,  0.4918, -0.5182,  0.1713],\n        [ 1.7086,  0.4918, -0.5182,  0.1713, -1.0127, -1.0864],\n        [-0.5182,  0.1713, -1.0127, -1.0864, -0.5182,  0.1713],\n        [-1.0127, -1.0864, -0.5182,  0.1713,  0.6477, -0.5360]])\n\n\n\n\ntorch.equal(emb.view(12, 6), torch.cat(torch.unbind(emb, 1), 1))\n\nTrue\n\n\n\n\nNow lets multiply emb and W1\n\nh = emb.view(12, 6) @ W1 + b1\nh.shape\n\ntorch.Size([12, 100])\n\n\n\n\nh = emb.view(emb.shape[0], 6) @ W1  + b1\nh.shape\n\ntorch.Size([12, 100])\n\n\n\n\nh = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n\n\nh\n\ntensor([[ 0.4915,  0.7098, -0.0430,  ...,  0.2994,  0.9402,  0.7398],\n        [-0.6305,  0.6804, -0.4876,  ...,  0.9315,  0.9978,  0.9405],\n        [ 1.0000,  0.9850, -0.0248,  ..., -0.6091, -0.5962,  0.8272],\n        ...,\n        [-0.9984,  0.0219,  0.4668,  ..., -0.9974,  0.9967,  0.8833],\n        [ 0.2683, -0.9647, -0.9713,  ..., -0.4932, -0.9902,  0.7563],\n        [-0.0409,  0.5042,  0.7568,  ...,  0.9979,  0.8698,  0.9306]])\n\n\n\n\nh.shape\n\ntorch.Size([12, 100])\n\n\n\n\n\nDeconstruct the addition of emb.view(-1, 6) @ W1 and b1 : Broadcasting\n\n(emb.view(-1, 6) @ W1).shape\n\ntorch.Size([12, 100])\n\n\n\n\nb1.shape\n\ntorch.Size([100])\n\n\n\nBroadcasting\n\n12 by 100\n1 by 100\n\nRowwise addition of b1 to (emb.view(-1, 6) @ W1)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#output-layer",
    "href": "lecture_notes/building_makemore_mlp.html#output-layer",
    "title": "Building makemore - MLP",
    "section": "Output Layer",
    "text": "Output Layer\n\nW2 = torch.randn((100, 27))\nb2 = torch.randn(27)\n\n\nlogits = h @ W2 + b2\n\n\nlogits.shape\n\ntorch.Size([12, 27])\n\n\n\n\ncounts = logits.exp()\n\n\nprob = counts/counts.sum(1, keepdims=True)\n\n\nprob.shape\n\ntorch.Size([12, 27])\n\n\n\n\nprob[0].sum()\n\ntensor(1.)\n\n\n\n\ntorch.arange(12)\n\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n\n\n\nY\n\ntensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0])\n\n\n\n\nprob[torch.arange(12), Y]\n\ntensor([8.2187e-08, 1.1724e-01, 1.6640e-18, 1.7638e-02, 8.5638e-13, 3.6065e-03,\n        1.6173e-08, 8.1841e-01, 3.2394e-06, 6.6904e-06, 1.5042e-07, 9.6335e-15])\n\n\n\n\nloss = -prob[torch.arange(12), Y].log().mean()\nloss\n\ntensor(15.6269)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#summary-of-the-full-network",
    "href": "lecture_notes/building_makemore_mlp.html#summary-of-the-full-network",
    "title": "Building makemore - MLP",
    "section": "Summary of the Full Network",
    "text": "Summary of the Full Network\n\nX.shape, Y.shape\n\n(torch.Size([12, 3]), torch.Size([12]))\n\n\n\n\ndef initialize_params(hc):\n    C = torch.randn((27, 2), generator=g)\n    W1 = torch.randn((6, hc), generator=g)\n    b1 = torch.randn(hc, generator=g)\n    W2 = torch.randn((hc, 27), generator=g)\n    b2 = torch.randn(27, generator=g)\n\n    return [C, W1, b1, W2, b2]\n\n\nparameters = initialize_params(100)\n\n\nsum(p.nelement() for p in parameters)\n\n3481\n\n\n\n\ndef forward_pass(params, X):\n    C, W1, b1, W2, b2 = params\n    emb = C[X] # (32, 3, 2)\n    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n    logits = h @ W2 + b2\n    return logits\n\n\ndef calcualte_loss(logits, Y):\n    counts = logits.exp()\n    prob = counts/counts.sum(1, keepdims=True)\n    return -prob[torch.arange(12), Y].log().mean()\n\n\nlogits = forward_pass(parameters, X)\n\n\ncalcualte_loss(logits, Y)\n\ntensor(19.1366)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#introducting-f.cross_entropy-and-why",
    "href": "lecture_notes/building_makemore_mlp.html#introducting-f.cross_entropy-and-why",
    "title": "Building makemore - MLP",
    "section": "Introducting F.cross_entropy and why?",
    "text": "Introducting F.cross_entropy and why?\n\nF.cross_entropy(logits, Y)\n\ntensor(19.1366)\n\n\n\n\nwhy?\n\nPytorch does not create intermediate tensors for the following code  counts = logits.exp() prob = counts/counts.sum(1, keepdims=True) loss = -prob[torch.arange(12), Y].log().mean()  Instead it clusters up all the above operation and it efficiently computes\nBackward prop is efficient. Simple to implement\nIt can numerically well behaved\n\n\nlogits = torch.tensor([-2, -3, 0, 5])\ncounts = logits.exp()\nprobs = counts / counts.sum()\nprobs\n\ntensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])\n\n\n\n\nlogits = torch.tensor([-100, -3, 0, 100])\ncounts = logits.exp()\nprobs = counts / counts.sum()\nprobs\n\ntensor([0., 0., 0., nan])\n\n\n\n\ncounts\n\ntensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])\n\n\n\nAs we can see the negative numbers are ok, but positive numbers can overflow the exp\nSo pytorch internally offset the logits with maximum number\n\nlogits = torch.tensor([-100, -3, 0, 100]) - 100\ncounts = logits.exp()\nprobs = counts / counts.sum()\nprobs\n\ntensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])\n\n\n\nso better to call F.cross_entropy(logits, Y)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#implementing-the-training-loop-overfitting-one-batch",
    "href": "lecture_notes/building_makemore_mlp.html#implementing-the-training-loop-overfitting-one-batch",
    "title": "Building makemore - MLP",
    "section": "Implementing the Training Loop, overfitting one batch",
    "text": "Implementing the Training Loop, overfitting one batch\n\nfor p in parameters: p.requires_grad = True\n\n\ndef backward_pass(params, loss, lr=0.1):\n    for p in params:\n        p.grad= None\n    loss.backward()\n    \n    for p in params:\n        p.data += - lr * p.grad\n    return params, loss\n\n\ndef an_epoch(parameters, X, Y, **args):\n    logits = forward_pass(parameters, X)\n    loss = F.cross_entropy(logits, Y)\n    backward_pass(parameters, loss, **args)\n    return loss\n\n\nfor _ in range(10):\n    loss = an_epoch(parameters, X, Y)\n    print(loss.item())\n\n19.136564254760742\n\n\n\n13.381081581115723\n\n\n\n9.38259220123291\n\n\n\n6.92555570602417\n\n\n\n4.83230447769165\n\n\n\n3.4374144077301025\n\n\n\n2.392893075942993\n\n\n\n1.488807201385498\n\n\n\n0.9096290469169617\n\n\n\n0.6129968762397766\n\n\n\n\nlogits = forward_pass(parameters, X); logits.max(1)\n\ntorch.return_types.max(\nvalues=tensor([ 9.8012, 13.4625, 12.6716, 12.9291, 14.5114,  9.8012, 10.4842, 14.7211,\n         5.7120, 15.7225,  5.5768, 18.3183], grad_fn=&lt;MaxBackward0&gt;),\nindices=tensor([ 5, 13, 13,  1,  0,  5, 12,  9, 22,  9,  1,  0]))\n\n\n\n\nY\n\ntensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0])"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#training-on-a-full-dataset",
    "href": "lecture_notes/building_makemore_mlp.html#training-on-a-full-dataset",
    "title": "Building makemore - MLP",
    "section": "Training on a full dataset",
    "text": "Training on a full dataset\n\nX, Y = generate_training_set(words, 3)\n\n\nX.shape, Y.shape, X.dtype, Y.dtype\n\n(torch.Size([228146, 3]), torch.Size([228146]), torch.int64, torch.int64)\n\n\n\n\ndef train(epochs, X, Y, **args):\n    parameters = initialize_params(100)\n    \n    for p in parameters: p.requires_grad = True \n        \n    for _ in range(epochs):\n        loss = an_epoch(parameters, X, Y, **args)\n        print(loss.item())\n    return parameters\n\n\nC[X].shape\n\ntorch.Size([228146, 3, 2])\n\n\n\n\nparams = train(10, X, Y)\n\n18.036645889282227\n\n\n\n16.586727142333984\n\n\n\n15.626733779907227\n\n\n\n14.729887962341309\n\n\n\n13.938693046569824\n\n\n\n13.19073486328125\n\n\n\n12.597437858581543\n\n\n\n12.086341857910156\n\n\n\n11.660746574401855\n\n\n\n11.272781372070312\n\n\n\n\nGenerate minibatches\n\ntorch.randint(0, 5, (32, ))\n\ntensor([1, 2, 2, 0, 3, 2, 1, 1, 3, 4, 2, 3, 1, 3, 1, 0, 1, 2, 4, 2, 3, 2, 4, 3,\n        2, 4, 4, 0, 3, 1, 1, 1])\n\n\n\n\ntorch.randint(0, X.shape[0], (32,))\n\ntensor([228007, 156095, 153320,  70517,  60450,  38038,  75888,  55665, 227431,\n        154371,  85786,  37967, 184558, 213374, 153605, 224477,  51769, 208919,\n        202749, 106279, 175975, 114188, 172753,  24093,   3400, 121271,  14529,\n         84926, 184325,  45408, 126493, 159063])\n\n\n\n\nix = torch.randint(0, X.shape[0], (32,))\n\n\nix\n\ntensor([ 28715, 214418,  88178,  86154, 101624,  58413, 129213,  85203,  32300,\n         94100,  13057,  94251,  52467, 225125,  40821,  52848,  90216, 168981,\n        131762,  69714, 210151,  38251,  73070, 142627, 113949,  84569, 106754,\n        175039, 148649,  14182, 120906, 172509])\n\n\n\n\n\nTrain on minibatches\n\ndef an_epoch_minibatch(parameters, X, Y, bs=32, **args):\n    ix = torch.randint(0, X.shape[0], (bs, )) # taking minibatches of 32 size\n    loss = an_epoch(parameters, X[ix], Y[ix], **args)\n    return loss\n\n\ndef train(epochs, X, Y, parameters=[], enable_print=True, **args):\n    if not parameters: parameters = initialize_params(100)\n    \n    for p in parameters: p.requires_grad = True \n        \n    for _ in range(epochs):\n        loss = an_epoch_minibatch(parameters, X, Y, **args)\n        if enable_print: print(loss.item())\n        \n    return parameters, loss.item()\n\n\n_, _ = train(10, X, Y)\n\n15.801168441772461\n\n\n\n14.867757797241211\n\n\n\n11.627082824707031\n\n\n\n13.213473320007324\n\n\n\n10.204056739807129\n\n\n\n12.32088851928711\n\n\n\n12.276724815368652\n\n\n\n10.75289249420166\n\n\n\n10.235709190368652\n\n\n\n8.84250545501709\n\n\n\n\nparams, loss = train(1000, X, Y, enable_print=False)\nloss\n\n2.5062108039855957\n\n\n\nWe have to define a function for evaluating loss on the whole dataset\n\ndef evaluate_loss(parameters, X, Y):\n    logits = forward_pass(parameters, X)\n    loss = F.cross_entropy(logits, Y)\n    return loss\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.6242, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, loss = train(2000, X, Y, enable_print=False)\nloss\n\n2.4619643688201904\n\n\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.6243, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#finding-a-good-learning-rate",
    "href": "lecture_notes/building_makemore_mlp.html#finding-a-good-learning-rate",
    "title": "Building makemore - MLP",
    "section": "Finding a good learning rate",
    "text": "Finding a good learning rate\n\nparams, loss = train(10, X, Y) # default one is 0.1\n\n16.19735336303711\n\n\n\n17.501420974731445\n\n\n\n15.06136703491211\n\n\n\n14.115403175354004\n\n\n\n12.98757266998291\n\n\n\n12.952009201049805\n\n\n\n13.650276184082031\n\n\n\n11.860039710998535\n\n\n\n12.437535285949707\n\n\n\n11.10865592956543\n\n\n\n\nparams, loss = train(100, X, Y, lr=0.001)\n\n17.875307083129883\n\n\n\n16.96537208557129\n\n\n\n16.46739387512207\n\n\n\n16.826091766357422\n\n\n\n19.715730667114258\n\n\n\n17.28173065185547\n\n\n\n17.353927612304688\n\n\n\n15.2487154006958\n\n\n\n16.242963790893555\n\n\n\n20.624465942382812\n\n\n\n19.717395782470703\n\n\n\n22.210813522338867\n\n\n\n17.298879623413086\n\n\n\n16.800865173339844\n\n\n\n19.643619537353516\n\n\n\n19.11254119873047\n\n\n\n16.98759651184082\n\n\n\n17.522817611694336\n\n\n\n16.67815399169922\n\n\n\n16.857685089111328\n\n\n\n20.040325164794922\n\n\n\n19.82996368408203\n\n\n\n17.39429473876953\n\n\n\n17.95696258544922\n\n\n\n18.889270782470703\n\n\n\n17.577701568603516\n\n\n\n16.490873336791992\n\n\n\n17.70130157470703\n\n\n\n18.09929656982422\n\n\n\n19.035062789916992\n\n\n\n19.805776596069336\n\n\n\n15.114175796508789\n\n\n\n21.028043746948242\n\n\n\n19.15180778503418\n\n\n\n18.2927188873291\n\n\n\n17.4348201751709\n\n\n\n18.359844207763672\n\n\n\n17.481584548950195\n\n\n\n17.3405704498291\n\n\n\n18.606294631958008\n\n\n\n16.104042053222656\n\n\n\n21.377683639526367\n\n\n\n17.499004364013672\n\n\n\n19.097225189208984\n\n\n\n19.2764949798584\n\n\n\n15.840270042419434\n\n\n\n20.504976272583008\n\n\n\n14.716649055480957\n\n\n\n19.399646759033203\n\n\n\n19.350618362426758\n\n\n\n17.978057861328125\n\n\n\n15.874401092529297\n\n\n\n19.028240203857422\n\n\n\n21.055648803710938\n\n\n\n17.59156036376953\n\n\n\n18.06299591064453\n\n\n\n19.219995498657227\n\n\n\n19.179119110107422\n\n\n\n18.878189086914062\n\n\n\n19.174936294555664\n\n\n\n15.592581748962402\n\n\n\n16.595109939575195\n\n\n\n18.39055633544922\n\n\n\n11.832684516906738\n\n\n\n17.001625061035156\n\n\n\n19.766319274902344\n\n\n\n16.30816078186035\n\n\n\n19.118053436279297\n\n\n\n20.007619857788086\n\n\n\n16.1800479888916\n\n\n\n21.173416137695312\n\n\n\n19.897363662719727\n\n\n\n19.161693572998047\n\n\n\n18.2498779296875\n\n\n\n13.18708324432373\n\n\n\n15.345601081848145\n\n\n\n16.639053344726562\n\n\n\n18.496694564819336\n\n\n\n14.642602920532227\n\n\n\n21.97202491760254\n\n\n\n17.084003448486328\n\n\n\n18.820404052734375\n\n\n\n16.859872817993164\n\n\n\n16.28878402709961\n\n\n\n16.989421844482422\n\n\n\n19.51396369934082\n\n\n\n14.59709358215332\n\n\n\n18.681137084960938\n\n\n\n14.994373321533203\n\n\n\n15.734018325805664\n\n\n\n15.58342456817627\n\n\n\n17.513246536254883\n\n\n\n19.031356811523438\n\n\n\n18.412912368774414\n\n\n\n15.050882339477539\n\n\n\n18.225595474243164\n\n\n\n17.290569305419922\n\n\n\n18.107513427734375\n\n\n\n17.607986450195312\n\n\n\n19.539506912231445\n\n\n\nDecreasing quite slow\n\nparams, loss = train(100, X, Y, lr=0.01)\n\n17.217191696166992\n\n\n\n18.98297119140625\n\n\n\n16.730661392211914\n\n\n\n18.72515869140625\n\n\n\n15.642149925231934\n\n\n\n18.69635581970215\n\n\n\n19.240758895874023\n\n\n\n15.680923461914062\n\n\n\n17.75980567932129\n\n\n\n15.805423736572266\n\n\n\n17.46143913269043\n\n\n\n14.92746639251709\n\n\n\n15.784602165222168\n\n\n\n17.56768226623535\n\n\n\n17.278583526611328\n\n\n\n16.729448318481445\n\n\n\n17.613697052001953\n\n\n\n17.079381942749023\n\n\n\n16.603227615356445\n\n\n\n15.034947395324707\n\n\n\n10.689759254455566\n\n\n\n14.267993927001953\n\n\n\n14.27953815460205\n\n\n\n17.156047821044922\n\n\n\n16.04837417602539\n\n\n\n14.437764167785645\n\n\n\n15.547869682312012\n\n\n\n14.045707702636719\n\n\n\n17.114885330200195\n\n\n\n14.959957122802734\n\n\n\n12.732937812805176\n\n\n\n12.506109237670898\n\n\n\n13.542440414428711\n\n\n\n11.618020057678223\n\n\n\n15.980241775512695\n\n\n\n13.710258483886719\n\n\n\n13.079842567443848\n\n\n\n17.457290649414062\n\n\n\n12.832975387573242\n\n\n\n15.989693641662598\n\n\n\n16.086259841918945\n\n\n\n14.375199317932129\n\n\n\n13.443093299865723\n\n\n\n14.050329208374023\n\n\n\n10.909284591674805\n\n\n\n10.330706596374512\n\n\n\n12.20095443725586\n\n\n\n13.081060409545898\n\n\n\n13.757899284362793\n\n\n\n11.393020629882812\n\n\n\n14.146169662475586\n\n\n\n12.656024932861328\n\n\n\n11.959654808044434\n\n\n\n12.629054069519043\n\n\n\n12.20356273651123\n\n\n\n12.901283264160156\n\n\n\n11.310954093933105\n\n\n\n11.71440601348877\n\n\n\n12.643488883972168\n\n\n\n12.945002555847168\n\n\n\n10.115494728088379\n\n\n\n10.972354888916016\n\n\n\n11.523700714111328\n\n\n\n8.636861801147461\n\n\n\n12.185890197753906\n\n\n\n10.055598258972168\n\n\n\n10.197139739990234\n\n\n\n11.891265869140625\n\n\n\n13.066970825195312\n\n\n\n11.821182250976562\n\n\n\n11.078763008117676\n\n\n\n11.997090339660645\n\n\n\n11.150663375854492\n\n\n\n11.122605323791504\n\n\n\n10.081033706665039\n\n\n\n9.641432762145996\n\n\n\n12.542899131774902\n\n\n\n11.15687370300293\n\n\n\n10.160797119140625\n\n\n\n10.767355918884277\n\n\n\n12.057256698608398\n\n\n\n11.341407775878906\n\n\n\n10.812758445739746\n\n\n\n10.457738876342773\n\n\n\n9.38547134399414\n\n\n\n10.351698875427246\n\n\n\n11.28741455078125\n\n\n\n10.267965316772461\n\n\n\n11.363784790039062\n\n\n\n11.739876747131348\n\n\n\n10.682605743408203\n\n\n\n9.142884254455566\n\n\n\n10.5222806930542\n\n\n\n12.483572006225586\n\n\n\n11.053152084350586\n\n\n\n8.63429069519043\n\n\n\n10.406988143920898\n\n\n\n8.784645080566406\n\n\n\n8.676480293273926\n\n\n\n10.117318153381348\n\n\n\nLets find a learning rate where the loss explodes\n\nparams, loss = train(100, X, Y, lr=1)\n\n20.110599517822266\n\n\n\n17.089881896972656\n\n\n\n14.24488353729248\n\n\n\n11.815177917480469\n\n\n\n12.130462646484375\n\n\n\n12.217141151428223\n\n\n\n11.420377731323242\n\n\n\n12.206888198852539\n\n\n\n8.879064559936523\n\n\n\n12.650971412658691\n\n\n\n9.793768882751465\n\n\n\n10.987152099609375\n\n\n\n9.248224258422852\n\n\n\n8.691174507141113\n\n\n\n9.365839958190918\n\n\n\n11.592811584472656\n\n\n\n10.417448043823242\n\n\n\n9.230694770812988\n\n\n\n11.055594444274902\n\n\n\n8.526256561279297\n\n\n\n8.585600852966309\n\n\n\n8.599808692932129\n\n\n\n7.238353252410889\n\n\n\n10.277055740356445\n\n\n\n11.222267150878906\n\n\n\n8.401968002319336\n\n\n\n12.263262748718262\n\n\n\n9.596739768981934\n\n\n\n8.914466857910156\n\n\n\n7.447011947631836\n\n\n\n8.390132904052734\n\n\n\n9.006266593933105\n\n\n\n10.928899765014648\n\n\n\n8.826053619384766\n\n\n\n10.818211555480957\n\n\n\n8.960731506347656\n\n\n\n10.464555740356445\n\n\n\n9.895048141479492\n\n\n\n9.146349906921387\n\n\n\n9.828166007995605\n\n\n\n9.657388687133789\n\n\n\n11.057379722595215\n\n\n\n11.779601097106934\n\n\n\n9.626520156860352\n\n\n\n12.079124450683594\n\n\n\n10.157666206359863\n\n\n\n10.510750770568848\n\n\n\n7.288343906402588\n\n\n\n9.157801628112793\n\n\n\n10.79643440246582\n\n\n\n9.7307767868042\n\n\n\n7.377813339233398\n\n\n\n8.90126895904541\n\n\n\n6.9252166748046875\n\n\n\n8.585623741149902\n\n\n\n5.940133094787598\n\n\n\n11.175277709960938\n\n\n\n8.790698051452637\n\n\n\n8.79421329498291\n\n\n\n6.400926113128662\n\n\n\n9.390056610107422\n\n\n\n8.232134819030762\n\n\n\n7.604208469390869\n\n\n\n11.635899543762207\n\n\n\n8.385150909423828\n\n\n\n7.61635684967041\n\n\n\n11.110309600830078\n\n\n\n8.195934295654297\n\n\n\n8.883593559265137\n\n\n\n9.026359558105469\n\n\n\n9.291215896606445\n\n\n\n9.069995880126953\n\n\n\n7.059180736541748\n\n\n\n7.9250664710998535\n\n\n\n7.074198246002197\n\n\n\n5.54173469543457\n\n\n\n8.047550201416016\n\n\n\n9.206929206848145\n\n\n\n5.927646160125732\n\n\n\n8.086305618286133\n\n\n\n10.902512550354004\n\n\n\n8.214250564575195\n\n\n\n5.934093475341797\n\n\n\n7.36422872543335\n\n\n\n7.87753438949585\n\n\n\n9.3895263671875\n\n\n\n7.487529754638672\n\n\n\n7.65749454498291\n\n\n\n7.1809916496276855\n\n\n\n7.551138877868652\n\n\n\n7.282594203948975\n\n\n\n7.792741298675537\n\n\n\n8.060037612915039\n\n\n\n7.456161975860596\n\n\n\n7.980192184448242\n\n\n\n4.748911380767822\n\n\n\n8.45018196105957\n\n\n\n7.393383026123047\n\n\n\n7.372125625610352\n\n\n\n7.169820785522461\n\n\n\n\nlre = torch.linspace(-3, 0, 1000)\nlrs = 10**lre\nlrs\n\ntensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n        1.0000])\n\n\n\n\ndef track_lr_vs_loss(epochs, X, Y, lre, enable_print=True):\n    parameters = initialize_params(100)\n    \n    lrs = 10**lre\n    \n    for p in parameters: p.requires_grad = True \n\n    lri = []\n    lres = []\n    lossi = []\n    \n    for i in range(epochs):\n        lr = lrs[i]\n        loss = an_epoch_minibatch(parameters, X, Y, lr=lr)\n        if enable_print: print(loss.item())\n        \n        # track stats\n        lri.append(lr)\n        lres.append(lre[i])\n        lossi.append(loss.item())\n        \n    return lri, lres, lossi\n\n\nlri, lres, lossi = track_lr_vs_loss(1000, X, Y, lre, enable_print=False)\n\n\nplot.plot(lri, lossi)\n\n[&lt;matplotlib.lines.Line2D object at 0x7f997ed80110&gt;]\n\n\n\n\n\n\nLets plot the exponent of the learning rate\n\nplot.plot(lres, lossi)\n\n[&lt;matplotlib.lines.Line2D object at 0x7f995a478390&gt;]\n\n\n\n\n\n\n\nparams, loss = train(10_000, X, Y, enable_print=False, lr=0.1)\n\n\nloss\n\n2.8730247020721436\n\n\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.5346, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nLearning rate decay\n\nparams, loss = train(10_000, X, Y, enable_print=False, lr=0.1)\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.4748, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, loss = train(10_000, X, Y, parameters = params, enable_print=False, lr=0.01)\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.3843, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, loss = train(10_000, X, Y, parameters = params, enable_print=False, lr=0.001)\n\n\nevaluate_loss(params, X, Y)\n\ntensor(2.3754, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#train-devvalid-test-set",
    "href": "lecture_notes/building_makemore_mlp.html#train-devvalid-test-set",
    "title": "Building makemore - MLP",
    "section": "Train, Dev/Valid & Test Set",
    "text": "Train, Dev/Valid & Test Set\nTraining Split: 80% &gt; Optimize the parameters of the model\nValidation Split: 10% &gt; Development over the hyperparameters of the model\nTest Split: 10% &gt; Evaluate the performance of the model at the end\n\ndef generate_train_valid_test_split(words):\n    random.seed(42)\n    random.shuffle(words)\n    n1 = int(0.8*len(words))\n    n2 = int(0.9*len(words))\n\n    Xtr, Ytr = generate_training_set(words[:n1], 3)\n    Xdev, Ydev = generate_training_set(words[n1:n2], 3)\n    Xte, Yte = generate_training_set(words[n2:], 3)\n    \n    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n\n\nXtr, Ytr, Xdev, Ydev, Xte, Yte = generate_train_valid_test_split(words)\n\n\nXtr.shape, Ytr.shape\n\n(torch.Size([182625, 3]), torch.Size([182625]))\n\n\n\n\nXdev.shape, Ydev.shape\n\n(torch.Size([22655, 3]), torch.Size([22655]))\n\n\n\n\nXte.shape, Yte.shape\n\n(torch.Size([22866, 3]), torch.Size([22866]))\n\n\n\n\nparams, loss = train(30_000, Xtr, Ytr, enable_print=False)\n\n\nloss\n\n2.5267984867095947\n\n\n\n\nparams, loss = train(10_000, Xtr, Ytr, lr=0.01, parameters=params, enable_print=False)\n\n\nloss\n\n2.570688247680664\n\n\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.3291, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.3266, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#experiment-larger-hidden-layer",
    "href": "lecture_notes/building_makemore_mlp.html#experiment-larger-hidden-layer",
    "title": "Building makemore - MLP",
    "section": "Experiment: Larger Hidden Layer",
    "text": "Experiment: Larger Hidden Layer\n\ndef track_hidden_layer_vs_loss(epochs, X, Y, parameters = [], lr=0.1, enable_print=True):\n    if not parameters: parameters = initialize_params(300)\n    \n    for p in parameters: p.requires_grad = True \n    \n    stepi = []\n    lossi = []\n    \n    for i in range(epochs):\n        loss = an_epoch_minibatch(parameters, X, Y, lr=lr)\n        if enable_print: print(loss.item())\n        \n        stepi.append(i)\n        lossi.append(loss.item())\n        \n    return parameters, stepi, lossi\n\n\nparams, steps, losses = track_hidden_layer_vs_loss(30_000, Xtr, Ytr, lr=0.5, enable_print=False)\n\n\nplot.plot(steps, losses)\n\n[&lt;matplotlib.lines.Line2D object at 0x7f997f749710&gt;]\n\n\n\n\n\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.4145, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.4132, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, steps, losses = track_hidden_layer_vs_loss(30_000, X, Y, params, lr=0.1, enable_print=False)\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.2973, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.2926, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, steps, losses = track_hidden_layer_vs_loss(30_000, X, Y, params, lr=0.05, enable_print=False)\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.2798, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.2770, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xte, Yte)\n\ntensor(2.2773, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#visualize-character-embeddings",
    "href": "lecture_notes/building_makemore_mlp.html#visualize-character-embeddings",
    "title": "Building makemore - MLP",
    "section": "Visualize Character Embeddings",
    "text": "Visualize Character Embeddings\n\nplot.figure(figsize=(8,8))\nplot.scatter(C[:, 0], C[:, 1].data, s = 200)\nfor i in range(C.shape[0]):\n    plot.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\nplot.grid('minor')"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#larger-embedding-size",
    "href": "lecture_notes/building_makemore_mlp.html#larger-embedding-size",
    "title": "Building makemore - MLP",
    "section": "Larger Embedding Size",
    "text": "Larger Embedding Size\n\ndef initialize_params(hc):\n    C = torch.randn((27, 10), generator=g)\n    W1 = torch.randn((30, hc), generator=g)\n    b1 = torch.randn(hc, generator=g)\n    W2 = torch.randn((hc, 27), generator=g)\n    b2 = torch.randn(27, generator=g)\n\n    return [C, W1, b1, W2, b2]\n\n\ndef forward_pass(params, X):\n    C, W1, b1, W2, b2 = params\n    emb = C[X] # (32, 3, 2)\n    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n    logits = h @ W2 + b2\n    return logits\n\n\ndef track_hidden_layer_vs_loss(epochs, X, Y, parameters = [], lr=0.1, enable_print=True):\n    if not parameters: parameters = initialize_params(200)\n    \n    for p in parameters: p.requires_grad = True \n    \n    stepi = []\n    lossi = []\n    \n    for i in range(epochs):\n        loss = an_epoch_minibatch(parameters, X, Y, lr=lr)\n        if enable_print: print(loss.item())\n        \n        stepi.append(i)\n        lossi.append(loss.log10().item())\n        \n    return parameters, stepi, lossi\n\n\nparams, steps, losses = track_hidden_layer_vs_loss(50_000, Xtr, Ytr, enable_print=False)\n\n\nplot.plot(steps, losses)\n\n[&lt;matplotlib.lines.Line2D object at 0x7f993b493cd0&gt;]\n\n\n\n\n\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.3460, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.3639, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xte, Yte)\n\ntensor(2.3736, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nparams, steps, losses = track_hidden_layer_vs_loss(50_000, Xtr, Ytr, params, lr=0.001, enable_print=False)\n\n\nplot.plot(steps, losses)\n\n[&lt;matplotlib.lines.Line2D object at 0x7f984ac705d0&gt;]\n\n\n\n\n\n\n\nevaluate_loss(params, Xtr, Ytr)\n\ntensor(2.1660, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xdev, Ydev)\n\ntensor(2.1936, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\nevaluate_loss(params, Xte, Yte)\n\ntensor(2.1990, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_makemore_mlp.html#sampling-from-the-model",
    "href": "lecture_notes/building_makemore_mlp.html#sampling-from-the-model",
    "title": "Building makemore - MLP",
    "section": "Sampling from the model",
    "text": "Sampling from the model\n\ndef generate_words(count):\n    for _ in range(count):\n        out = []\n        context = [0] * block_size # initialize with all ...\n        while True:\n            logits = forward_pass(params, torch.tensor([context]))\n            probs = F.softmax(logits, dim=1)\n            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n            context = context[1:] + [ix]\n            out.append(ix)\n            if ix == 0: break\n\n        print(''.join(itos[i] for i in out))\n\n\ngenerate_words(20)\n\nnamileina.\n\n\n\nbrigan.\n\n\n\nmarxea.\n\n\n\nluel.\n\n\n\nparkonzayah.\n\n\n\nnwayleah.\n\n\n\njusi.\n\n\n\ntyrfen.\n\n\n\nwin.\n\n\n\ndylynn.\n\n\n\nxolle.\n\n\n\ndilin.\n\n\n\nmicheshevpaveett.\n\n\n\nparandee.\n\n\n\nyannie.\n\n\n\naddioktes.\n\n\n\nkajaurvne.\n\n\n\nkah.\n\n\n\nhari.\n\n\n\ntor."
  },
  {
    "objectID": "lecture_notes/building_makemore.html",
    "href": "lecture_notes/building_makemore.html",
    "title": "Building makemore",
    "section": "",
    "text": "from collections import Counter\nimport numpy as np\nimport torch\nfrom rich import print\nfrom rich import pretty\nfrom matplotlib import pyplot as plt\ng = torch.Generator().manual_seed(2147483647)\npretty.install()"
  },
  {
    "objectID": "lecture_notes/building_makemore.html#counting",
    "href": "lecture_notes/building_makemore.html#counting",
    "title": "Building makemore",
    "section": "Counting",
    "text": "Counting\n\nRead in the data\n\ndef get_words(filename):\n    with open('../data/names.txt') as f: \n        return list(map(lambda x: x.strip(), f.readlines()))\n\n\nwords = get_words('../data/names.txt')\n\n\nwords[:10]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n\n\n\n\nlen(words)\n\n32033\n\n\n\n\n\nMinimum Length\n\nmin(len(w) for w in words)\n\n2\n\n\n\n\n\nMaximum Length\n\nmax(len(w) for w in words)\n\n15\n\n\n\n\n\nCreate paring of nth and n + 1th position characters\n\nfor w in words[:1]:\n    for ch1, ch2 in zip(w, w[1:]):\n        print(ch1, ch2)\n\ne m\n\n\n\nm m\n\n\n\nm a\n\n\n\n\nAdd start (&lt;S&gt;) and end(&lt;E&gt;) tokens to the word\n\nThe model will know the starting and ending of the word\n\n\ndef generate_pairings(words, start_token='&lt;S&gt;', end_token='&lt;E&gt;'):\n    for w in words:\n        chs = [start_token] + list(w) + [end_token]\n        for ch1, ch2 in zip(chs, chs[1:]):\n            yield ch1, ch2\n\n\nfor ch1, ch2 in generate_pairings(words[:1]):\n    print(ch1, ch2)\n\n&lt;S&gt; e\n\n\n\ne m\n\n\n\nm m\n\n\n\nm a\n\n\n\na &lt;E&gt;\n\n\n\n\nsum(1 for ch1, ch2 in generate_pairings(words))\n\n228146\n\n\n\n\n\nlets see for 3 words\n\nfor ch1, ch2 in generate_pairings(words[:3]):\n    print(ch1, ch2)\n\n&lt;S&gt; e\n\n\n\ne m\n\n\n\nm m\n\n\n\nm a\n\n\n\na &lt;E&gt;\n\n\n\n&lt;S&gt; o\n\n\n\no l\n\n\n\nl i\n\n\n\ni v\n\n\n\nv i\n\n\n\ni a\n\n\n\na &lt;E&gt;\n\n\n\n&lt;S&gt; a\n\n\n\na v\n\n\n\nv a\n\n\n\na &lt;E&gt;\n\n\n\n\n\n\nCount of bigrams\n\nBigram for 3 words\n\ndef create_bigram_counter(words):\n    b = Counter()\n    for ch1, ch2 in generate_pairings(words):\n        bigram = (ch1, ch2)\n        b[bigram] += 1\n    return b\n\n\ncreate_bigram_counter(words[:3])\n\nCounter({\n    ('&lt;S&gt;', 'e'): 1,\n    ('e', 'm'): 1,\n    ('m', 'm'): 1,\n    ('m', 'a'): 1,\n    ('a', '&lt;E&gt;'): 3,\n    ('&lt;S&gt;', 'o'): 1,\n    ('o', 'l'): 1,\n    ('l', 'i'): 1,\n    ('i', 'v'): 1,\n    ('v', 'i'): 1,\n    ('i', 'a'): 1,\n    ('&lt;S&gt;', 'a'): 1,\n    ('a', 'v'): 1,\n    ('v', 'a'): 1\n})\n\n\n\n\n\nBigram for all words\n\nb = create_bigram_counter(words)\n\n\nb.most_common(10)\n\n[\n    (('n', '&lt;E&gt;'), 6763),\n    (('a', '&lt;E&gt;'), 6640),\n    (('a', 'n'), 5438),\n    (('&lt;S&gt;', 'a'), 4410),\n    (('e', '&lt;E&gt;'), 3983),\n    (('a', 'r'), 3264),\n    (('e', 'l'), 3248),\n    (('r', 'i'), 3033),\n    (('n', 'a'), 2977),\n    (('&lt;S&gt;', 'k'), 2963)\n]\n\n\n\n\n\n\nCreate 2D array of the bigram\n\nLittle warmup with tensors\n\na = torch.zeros((3, 5), dtype=torch.int32)\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n\n\na.dtype\n\ntorch.int32\n\n\n\n\na[1,3] = 1\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n\n\na[1, 3] += 1\na\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 2, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\n\n\n\n\n\n2D matrix of alpahabets\n\ndef get_stoi(words, start_token, end_token, tokens_at_start=True):\n    chars = []\n    if tokens_at_start:\n        chars.append(start_token)\n        if start_token != end_token: chars.append(end_token)\n        \n    chars.extend(sorted(list(set(''.join(words)))))\n\n    if not tokens_at_start:\n        chars.append(start_token)\n        if start_token != end_token: chars.append(end_token)\n\n    stoi = {s:i for i,s in enumerate(chars)}\n    \n    return stoi\n\n\nstoi = get_stoi(words, '&lt;S&gt;', '&lt;E&gt;', tokens_at_start=False)\nstoi\n\n{\n    'a': 0,\n    'b': 1,\n    'c': 2,\n    'd': 3,\n    'e': 4,\n    'f': 5,\n    'g': 6,\n    'h': 7,\n    'i': 8,\n    'j': 9,\n    'k': 10,\n    'l': 11,\n    'm': 12,\n    'n': 13,\n    'o': 14,\n    'p': 15,\n    'q': 16,\n    'r': 17,\n    's': 18,\n    't': 19,\n    'u': 20,\n    'v': 21,\n    'w': 22,\n    'x': 23,\n    'y': 24,\n    'z': 25,\n    '&lt;S&gt;': 26,\n    '&lt;E&gt;': 27\n}\n\n\n\n\ndef create_bigram_matrix(words, start_token, end_token, tokens_at_start=True):\n    stoi = get_stoi(words, start_token, end_token, tokens_at_start)\n    alphabet_size = len(stoi)\n    N = torch.zeros((alphabet_size, alphabet_size), dtype=torch.int32)\n    for ch1, ch2 in generate_pairings(words, start_token, end_token):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1, ix2] += 1\n    return N\n\n\nN = create_bigram_matrix(words, '&lt;S&gt;', '&lt;E&gt;', False)\n\n\nN[:10, :10]\n\ntensor([[ 556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175],\n        [ 321,   38,    1,   65,  655,    0,    0,   41,  217,    1],\n        [ 815,    0,   42,    1,  551,    0,    2,  664,  271,    3],\n        [1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9],\n        [ 679,  121,  153,  384, 1271,   82,  125,  152,  818,   55],\n        [ 242,    0,    0,    0,  123,   44,    1,    1,  160,    0],\n        [ 330,    3,    0,   19,  334,    1,   25,  360,  190,    3],\n        [2244,    8,    2,   24,  674,    2,    2,    1,  729,    9],\n        [2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76],\n        [1473,    1,    4,    4,  440,    0,    0,   45,  119,    2]],\n       dtype=torch.int32)\n\n\n\nThe type of a cell in the above N is tensor\n\ntype(N[1, 1])\n\n&lt;class 'torch.Tensor'&gt;\n\n\n\nTherefore we have to call it with .item() to get the value\n\ntype(N[1, 1].item())\n\n&lt;class 'int'&gt;\n\n\n\n\nplt.imshow(N)\n\n\n\n\n\nitos = dict(map(reversed, stoi.items()))\nitos\n\n{\n    0: 'a',\n    1: 'b',\n    2: 'c',\n    3: 'd',\n    4: 'e',\n    5: 'f',\n    6: 'g',\n    7: 'h',\n    8: 'i',\n    9: 'j',\n    10: 'k',\n    11: 'l',\n    12: 'm',\n    13: 'n',\n    14: 'o',\n    15: 'p',\n    16: 'q',\n    17: 'r',\n    18: 's',\n    19: 't',\n    20: 'u',\n    21: 'v',\n    22: 'w',\n    23: 'x',\n    24: 'y',\n    25: 'z',\n    26: '&lt;S&gt;',\n    27: '&lt;E&gt;'\n}\n\n\n\n\ndef plot_matrix(N, itos):\n    plt.figure(figsize=(16, 16))\n    plt.imshow(N, cmap='Blues')\n    for i in range(N.shape[0]):\n        for j in range(N.shape[1]):\n            chstr = itos[i] + itos[j]\n            plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n            plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n    plt.axis(\"off\")\n\n\nplot_matrix(N, itos)\n\n\n\n\n\n\nRemove &lt;E&gt; and &lt;S&gt; in favor of a single . token\n\nWill deduct the columns and row having 0 values\n\n\nstoi = get_stoi(words, '.', '.')\nstoi\n\n{\n    '.': 0,\n    'a': 1,\n    'b': 2,\n    'c': 3,\n    'd': 4,\n    'e': 5,\n    'f': 6,\n    'g': 7,\n    'h': 8,\n    'i': 9,\n    'j': 10,\n    'k': 11,\n    'l': 12,\n    'm': 13,\n    'n': 14,\n    'o': 15,\n    'p': 16,\n    'q': 17,\n    'r': 18,\n    's': 19,\n    't': 20,\n    'u': 21,\n    'v': 22,\n    'w': 23,\n    'x': 24,\n    'y': 25,\n    'z': 26\n}\n\n\n\n\nitos = dict(map(reversed, stoi.items()))\n\n\nN = create_bigram_matrix(words, '.', '.')\n\n\nN[0, 0]\n\ntensor(0, dtype=torch.int32)\n\n\n\n\nplot_matrix(N, itos)\n\n\n\n\n\nN[0]\n\ntensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)\n\n\n\n\n\n\nSampling\n\nWarm up with probability tensor\n\np = torch.rand(3, generator=g)\np\n\ntensor([0.7081, 0.3542, 0.1054])\n\n\n\n\np.sum()\n\ntensor(1.1678)\n\n\n\n\np = p/p.sum()\np\n\ntensor([0.6064, 0.3033, 0.0903])\n\n\n\n\nDrawing 20 samples\n\np_dist = torch.multinomial(p, num_samples=20, replacement=True, generator=g)\np_dist\n\ntensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1])\n\n\n\n\nlen(p_dist[p_dist == 0])/len(p_dist)\n\n0.45\n\n\n\n\nlen(p_dist[p_dist == 1])/len(p_dist)\n\n0.45\n\n\n\n\nlen(p_dist[p_dist == 2])/len(p_dist)\n\n0.1\n\n\n\n\n\nDrawing 50 samples\n\np_dist = torch.multinomial(p, num_samples=50, replacement=True, generator=g)\np_dist\n\ntensor([0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n        1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n        0, 0])\n\n\n\n\nlen(p_dist[p_dist == 0])/len(p_dist)\n\n0.64\n\n\n\n\nlen(p_dist[p_dist == 1])/len(p_dist)\n\n0.32\n\n\n\n\nlen(p_dist[p_dist == 2])/len(p_dist)\n\n0.04\n\n\n\n\n\n\nDrawing a character wrt to probability of occurance\n\np = N[0].float()\np = p / p.sum() \np\n\ntensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])\n\n\n\n\nix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nix\n\n19\n\n\n\n\nitos[ix]\n\n's'\n\n\n\n\ndef generate_names(count, pdist_func, g):\n    for i in range(count):\n        out = []\n        ix = 0\n        while True:\n            p = pdist_func(ix)\n            ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n            out.append(itos[ix])\n            if ix == 0:\n                break\n        yield ''.join(out)\n\n\np_occurance = lambda ix: N[ix].float()/N[ix].sum()\nfor name in generate_names(10, p_occurance, g): print(name)\n\nblon.\n\n\n\nke.\n\n\n\na.\n\n\n\nry.\n\n\n\nl.\n\n\n\nbalycaweriginnn.\n\n\n\ndata.\n\n\n\nbh.\n\n\n\nmatt.\n\n\n\njeeve.\n\n\n\n\n\nDrawing a character wrt to uniform probability\n\np_uniform = lambda ix: torch.ones(len(N[ix]))/len(N[ix])\n\n\nfor name in generate_names(10, p_uniform, g): print(name)\n\nwwjieqrrlvhtwogbqtwrmcjpnvrkifgnsgfvp.\n\n\n\nkynsszpvqzmmwpogyzdhpfapyhlqdxcvczntn.\n\n\n\n.\n\n\n\n.\n\n\n\nrxnsmepegjipknhbzrrz.\n\n\n\nkgkznqqzsdaacfanvedfjga.\n\n\n\nycgfsirvvmcrvssnqjbjuqfzanulmxxkseuktjmbhn.\n\n\n\nx.\n\n\n\nwsuzuxkneqmel.\n\n\n\nqrbcskqqopeqbkuidxrnmyyfvysdxvfwix.\n\n\n\n\n\n\nVectorized normalization of rows and columns\n\nWarm up with normalization\n\nP = N.float()\n\n\nP.shape\n\ntorch.Size([27, 27])\n\n\n\n\nP.sum(0, keepdim=True).shape\n\ntorch.Size([1, 27])\n\n\n\n\nP.sum(1, keepdim=True).shape\n\ntorch.Size([27, 1])\n\n\n\n\nP.sum(0, keepdim=False).shape\n\ntorch.Size([27])\n\n\n\n\nP.sum(1, keepdim=False).shape\n\ntorch.Size([27])\n\n\n\n\nBroadcasting\nTwo tensors are “broadcastable” if the following rules hold:\n- Each tensor has at least one dimension.\n- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\nP.shape\n\ntorch.Size([27, 27])\n\n\n\n\nP_sum_col = P.sum(1, keepdim=True)\nP_sum_col.shape\n\ntorch.Size([27, 1])\n\n\n\nAs you can see above the shapes of the two variables P and P_sum_col are\n\n27 by 27\n27 by 1\n\nBroadcasting will repeat the unit dimension of the second variable 27 times along the y axis and it does element wise division\nSo the P_norm will be\n\nP_norm = P/P_sum_col\n\n\nP_norm.shape\n\ntorch.Size([27, 27])\n\n\n\n\nnormalized_P = lambda ix: P_norm[ix]\n\n\nfor name in generate_names(10, normalized_P, g): print(name)\n\nele.\n\n\n\nzelensskan.\n\n\n\na.\n\n\n\nilelena.\n\n\n\narah.\n\n\n\nlizanolbraris.\n\n\n\nsil.\n\n\n\nkyliketo.\n\n\n\nasonnngaeyja.\n\n\n\nan.\n\n\n\n\n\nP_sum_col without keepdims\n\nP_sum_col_wo_keepdims = P.sum(1)\nP_sum_col_wo_keepdims.shape\n\ntorch.Size([27])\n\n\n\nAnd what if we use the variable P_sum_col_wo_keepdims to divide the P, how will the broadcasting rule be applied?\nSo the shapes of the two variables P and P_sum_col_wo_keepdims are\n\n27 by 27\n27\n\nWe will arrange the trailing dimension of the P_sum_col_wo_keepdims shape along with the P shape, so it will be\n\n27 by 27\n1 by 27\n\nNow broadcasting will copy the unit dimension of the P_sum_col_wo_keepdims along the x-axis 27 times\nThe result will be\n\nP_norm_wo_keepdims = P/P_sum_col_wo_keepdims\n\n\ntorch.equal(P_norm_wo_keepdims, P_norm)\n\nFalse\n\n\n\nSo here we are normalizing the columns instead of the rows when broadcasting without keepdims\n\nwrongly_normalized_P = lambda ix: P_norm_wo_keepdims[ix]\n\n\nfor name in generate_names(10, wrongly_normalized_P, g): print(name)\n\ncishwambitzuruvefaum.\n\n\n\najorun.\n\n\n\nxilinnophajorovebrglmivoublicckyle.\n\n\n\njoyquwasooxxentomprtyuquviequzaq.\n\n\n\njuxtrcoxluckyjayspttycelllwyddstotyphaxxxwecquxzikoququzynikoposylixxuffruedrkowh.\n\n\n\nju.\n\n\n\nixxxisrielyavrhmidexytzrohauxiexxxxxxzurefffaigtzuzzantallyojoxxxt.\n\n\n\noprghah.\n\n\n\nstzldouwinolyselppp.\n\n\n\nj.\n\n\n\n\n\n\n\nLoss function\n\nProbability of each pairing\n\nfor ch1, ch2 in generate_pairings(words[:3], '.', '.'): print(f'{ch1}{ch2}')\n\n.e\n\n\n\nem\n\n\n\nmm\n\n\n\nma\n\n\n\na.\n\n\n\n.o\n\n\n\nol\n\n\n\nli\n\n\n\niv\n\n\n\nvi\n\n\n\nia\n\n\n\na.\n\n\n\n.a\n\n\n\nav\n\n\n\nva\n\n\n\na.\n\n\n\n\ndef generate_pairing_probs(words):\n    for ch1, ch2 in generate_pairings(words,'.', '.'):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        prob = P_norm[ix1, ix2]\n        yield ch1, ch2, prob\n\n\nfor ch1, ch2, prob in generate_pairing_probs(words[:3]): print(f'{ch1}{ch2}: {prob: .4f}')\n\n.e:  0.0478\n\n\n\nem:  0.0377\n\n\n\nmm:  0.0253\n\n\n\nma:  0.3899\n\n\n\na.:  0.1960\n\n\n\n.o:  0.0123\n\n\n\nol:  0.0780\n\n\n\nli:  0.1777\n\n\n\niv:  0.0152\n\n\n\nvi:  0.3541\n\n\n\nia:  0.1381\n\n\n\na.:  0.1960\n\n\n\n.a:  0.1377\n\n\n\nav:  0.0246\n\n\n\nva:  0.2495\n\n\n\na.:  0.1960\n\n\n\nThe individual character probability is\n\n1/27\n\n0.037037037037037035\n\n\n\nwhich is ~4%.\nif the above probability assigned by the bigram model was 1 then the model is sure about what will come will next\n\n\n\nNegative Log Likelihood\nThe product of the above probabilities will determine how the model is performing. As the product of probabilities will be very small, we are taking the log likelihood\nMaximum Likelihood \\[ ML = a \\times b \\times c \\]\nLog Likelihood \\[ \\log {(a \\times b \\times c)} = \\log {a} + \\log {b} + \\log {c} \\]\n\ndef print_prob_logprob(words):\n    for ch1, ch2, prob in generate_pairing_probs(words): \n        logprob = torch.log(prob)\n        print(f'{ch1}{ch2}: {prob: .4f} {logprob: .4f}')\nprint_prob_logprob(words[:3])\n\n.e:  0.0478 -3.0408\n\n\n\nem:  0.0377 -3.2793\n\n\n\nmm:  0.0253 -3.6772\n\n\n\nma:  0.3899 -0.9418\n\n\n\na.:  0.1960 -1.6299\n\n\n\n.o:  0.0123 -4.3982\n\n\n\nol:  0.0780 -2.5508\n\n\n\nli:  0.1777 -1.7278\n\n\n\niv:  0.0152 -4.1867\n\n\n\nvi:  0.3541 -1.0383\n\n\n\nia:  0.1381 -1.9796\n\n\n\na.:  0.1960 -1.6299\n\n\n\n.a:  0.1377 -1.9829\n\n\n\nav:  0.0246 -3.7045\n\n\n\nva:  0.2495 -1.3882\n\n\n\na.:  0.1960 -1.6299\n\n\n\nLets sum up all the log probabilities\n\ndef log_likelihood(words):\n    log_likelihood = 0\n    for ch1, ch2, prob in generate_pairing_probs(words): \n        log_likelihood += torch.log(prob)\n    return log_likelihood\n\n\nlog_likelihood(words[:3])\n\ntensor(-38.7856)\n\n\n\nThe log likelihood will be 0 if all the probabilities will be 1 and will be negative if one of more of the probability will be less than 0. The maximum number the log likelihood will be 1. We want something which can be defined as loss such that higher the amount of inaccurate predictions higher the loss.\nSo if we take the negative of log likelihood, we will get an increasing number with higher innacuracy.\n\ndef negative_log_likelihood(words):\n    return -log_likelihood(words)\n\n\nnegative_log_likelihood(words[:3])\n\ntensor(38.7856)\n\n\n\nSometimes we want to normalize the log_likelihood by the count of pairs. Lets do that\n\ndef log_likelihood_normalized(words):\n    count = 0\n    log_likelihood = 0\n    for ch1, ch2, prob in generate_pairing_probs(words):\n        log_likelihood += torch.log(prob)\n        count += 1\n    return log_likelihood/count\n\n\nlog_likelihood_normalized(words)\n\ntensor(-2.4541)\n\n\n\n\ndef negative_log_likelihood_normalized(words):\n    return -log_likelihood_normalized(words)\n\n\nnegative_log_likelihood_normalized(words)\n\ntensor(2.4541)\n\n\n\nSo the training loss is 38.7856\n\nTest it on a test data\n\nnegative_log_likelihood_normalized([\"anubhav\"])\n\ntensor(3.1186)\n\n\n\n\nnegative_log_likelihood_normalized([\"anubhavm\"])\n\ntensor(inf)\n\n\n\nIt is infinite loss, means that the model will not predict anubhavm\nLets see which pairing is giving infinite prob\n\nprint_prob_logprob([\"anubhavm\"])\n\n.a:  0.1377 -1.9829\n\n\n\nan:  0.1605 -1.8296\n\n\n\nnu:  0.0052 -5.2518\n\n\n\nub:  0.0329 -3.4157\n\n\n\nbh:  0.0155 -4.1669\n\n\n\nha:  0.2946 -1.2220\n\n\n\nav:  0.0246 -3.7045\n\n\n\nvm:  0.0000 -inf\n\n\n\nm.:  0.0777 -2.5551\n\n\n\nWe see that the pairing vm has 0 probability of occurance which leads to infinite loss.\nIn the following table also m is following v 0 times\n\nplot_matrix(N, itos)\n\n\n\n\n\n\n\nModel Smooting\nTo add a very small number (fake counts) to the count of pairing so that the likelihood is not 0 and therefore the negative log likelihood is not negative infinity\n\nP = (N + 1).float()\n\nThe more fake count you add to N, the more uniform model (uniform probabilities) you will have. The less you add the more peak model (model probabilities) you will have\n\nP_sum_col = P.sum(1, keepdim=True)\n\n\nP_norm = P/P_sum_col\n\n\nprint_prob_logprob([\"anubhavm\"])\n\n.a:  0.1376 -1.9835\n\n\n\nan:  0.1604 -1.8302\n\n\n\nnu:  0.0053 -5.2429\n\n\n\nub:  0.0329 -3.4146\n\n\n\nbh:  0.0157 -4.1529\n\n\n\nha:  0.2937 -1.2251\n\n\n\nav:  0.0246 -3.7041\n\n\n\nvm:  0.0004 -7.8633\n\n\n\nm.:  0.0775 -2.5572\n\n\n\n\nnegative_log_likelihood_normalized([\"anubhavm\"])\n\ntensor(3.5526)"
  },
  {
    "objectID": "lecture_notes/building_makemore.html#neural-network",
    "href": "lecture_notes/building_makemore.html#neural-network",
    "title": "Building makemore",
    "section": "Neural Network",
    "text": "Neural Network\n\nCreate the train set of the bigrams\n\ndef generate_training_set(words, start_token='.', end_token='.'):\n    xs, ys = [], []\n    for ch1, ch2 in generate_pairings(words, start_token, end_token):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        xs.append(ix1)\n        ys.append(ix2)\n    return xs, ys\n\n\nxs, ys = generate_training_set(words[:1])\n\n\nxs = torch.tensor(xs); xs\n\ntensor([ 0,  5, 13, 13,  1])\n\n\n\n\nys = torch.tensor(ys); ys\n\ntensor([ 5, 13, 13,  1,  0])\n\n\n\n\nfor ch1, ch2 in generate_pairings(words[:1], '.', '.'):\n    print(ch1, ch2)\n\n. e\n\n\n\ne m\n\n\n\nm m\n\n\n\nm a\n\n\n\na .\n\n\n\n\nDifference between torch.tensor and torch.Tensor\n\ntorch.tensor infers the dtype automatically, while torch.Tensor returns a torch.FloatTensor. I would recommend to stick to torch.tensor, which also has arguments like dtype, if you would like to change the type.\n\nhttps://stackoverflow.com/a/63116398\n\nxs.dtype, ys.dtype\n\n(torch.int64, torch.int64)\n\n\n\n\nxs, ys = generate_training_set(words)\n\n\nxs = torch.Tensor(xs)\nys = torch.Tensor(ys)\nxs.dtype, ys.dtype\n\n(torch.float32, torch.float32)\n\n\n\n\n\n\nOne Hot Encoding of the training dataset\n\nimport torch.nn.functional as F\n\n\nxs, ys = generate_training_set(words[:1])\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n\nxenc = F.one_hot(xs, num_classes=27)\nxenc\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0]])\n\n\n\n\nxenc.shape\n\ntorch.Size([5, 27])\n\n\n\n\nplt.imshow(xenc)\n\n\n\n\n\nxenc.dtype\n\ntorch.int64\n\n\n\nWhen we are sending numbers to NN we dont want the numbers to be int but to be float as it can take various values\n\nxenc = F.one_hot(xs, num_classes=27).float()\n\n\nxenc.dtype\n\ntorch.float32\n\n\n\n\n\nInitialize the weight\n\nW = torch.randn((27, 1))\nW\n\ntensor([[-1.0414],\n        [-0.4622],\n        [ 0.4704],\n        [ 0.2034],\n        [ 0.4376],\n        [ 0.8326],\n        [-1.1531],\n        [-0.5384],\n        [-1.5000],\n        [-0.3734],\n        [-0.9722],\n        [ 0.7093],\n        [ 1.6148],\n        [ 0.6154],\n        [ 0.6585],\n        [-1.2100],\n        [-0.4480],\n        [ 2.4709],\n        [ 1.5362],\n        [-0.8239],\n        [-1.8200],\n        [-2.4810],\n        [-1.1249],\n        [ 1.2613],\n        [-0.7899],\n        [-0.3423],\n        [-0.8073]])\n\n\n\n\nW.shape\n\ntorch.Size([27, 1])\n\n\n\n\nxenc @ W\n\ntensor([[-1.0414],\n        [ 0.8326],\n        [ 0.6154],\n        [ 0.6154],\n        [-0.4622]])\n\n\n\nInitialize random weight of 27 by 27\n\nW = torch.randn((27, 27))\nxenc @ W\n\ntensor([[-1.3844e+00,  1.5959e-02,  3.7060e-01,  1.1356e+00,  5.2515e-01,\n          7.3794e-01, -1.0737e+00, -9.0978e-01,  1.2984e+00,  1.0683e+00,\n          1.2605e+00, -1.7498e+00,  4.6805e-01, -3.4442e-01,  1.0569e+00,\n          1.8138e-01,  8.4084e-01,  1.3287e+00, -7.5910e-01,  7.8683e-01,\n          9.5301e-01, -1.0442e+00, -2.4167e-02,  6.2387e-01, -6.6787e-02,\n         -7.1907e-01,  1.2762e+00],\n        [-9.1542e-01, -8.4699e-02,  8.1029e-01,  5.2382e-01, -1.4164e+00,\n          9.8146e-01,  2.2023e+00,  5.3777e-01,  2.7927e-01, -5.9158e-03,\n          1.1951e-01, -1.0505e+00,  2.1483e-01,  4.4787e-01,  1.7172e+00,\n          1.6195e+00, -1.2666e+00, -4.3973e-01,  7.8754e-02,  2.4022e-01,\n          5.2765e-01,  3.4238e-01, -1.5155e+00, -3.3794e-02,  1.3747e+00,\n          1.8808e+00,  3.2315e-01],\n        [ 1.0474e+00, -1.1022e+00,  1.1412e+00, -1.0475e+00,  1.2827e+00,\n         -1.1662e-01, -1.0313e+00, -5.0630e-01, -5.8584e-01,  3.7119e-01,\n         -6.2447e-01, -6.1076e-01,  7.0085e-01,  2.1230e-01,  1.8492e+00,\n         -1.5117e-01,  2.2283e+00, -1.1119e+00, -9.5698e-01, -2.8551e-02,\n          1.0193e+00, -8.8697e-01, -7.4386e-02,  1.3281e+00,  2.0499e-01,\n          8.1934e-01,  2.3981e-01],\n        [ 1.0474e+00, -1.1022e+00,  1.1412e+00, -1.0475e+00,  1.2827e+00,\n         -1.1662e-01, -1.0313e+00, -5.0630e-01, -5.8584e-01,  3.7119e-01,\n         -6.2447e-01, -6.1076e-01,  7.0085e-01,  2.1230e-01,  1.8492e+00,\n         -1.5117e-01,  2.2283e+00, -1.1119e+00, -9.5698e-01, -2.8551e-02,\n          1.0193e+00, -8.8697e-01, -7.4386e-02,  1.3281e+00,  2.0499e-01,\n          8.1934e-01,  2.3981e-01],\n        [ 1.0060e+00, -1.6259e-02, -1.9179e+00,  1.6954e-02,  1.0129e+00,\n         -8.4792e-01,  1.4553e+00, -8.6143e-01,  3.8685e-01,  7.8658e-01,\n          1.7895e+00, -3.5575e-01,  4.3668e-01,  4.7369e-01, -1.1651e+00,\n          5.3522e-02, -2.1702e+00,  1.2975e+00,  1.1129e+00,  8.5445e-01,\n          2.0814e-01,  2.7412e-01, -2.4321e-04,  1.3574e+00, -4.5190e-01,\n          1.5984e-01, -1.2650e-01]])\n\n\n\n\n(xenc @ W).shape\n\ntorch.Size([5, 27])\n\n\n\n\n(xenc @ W)[3, 13], (xenc[3] * W[:, 13]).sum()\n\n(tensor(0.2123), tensor(0.2123))\n\n\n\n\n\nExponential\n\nlogits = (xenc @ W) # log counts\ncounts = logits.exp() # counts\ncounts\n\ntensor([[0.2505, 1.0161, 1.4486, 3.1130, 1.6907, 2.0916, 0.3418, 0.4026, 3.6636,\n         2.9104, 3.5272, 0.1738, 1.5969, 0.7086, 2.8773, 1.1989, 2.3183, 3.7761,\n         0.4681, 2.1964, 2.5935, 0.3520, 0.9761, 1.8661, 0.9354, 0.4872, 3.5830],\n        [0.4003, 0.9188, 2.2486, 1.6885, 0.2426, 2.6683, 9.0457, 1.7122, 1.3222,\n         0.9941, 1.1269, 0.3498, 1.2396, 1.5650, 5.5687, 5.0507, 0.2818, 0.6442,\n         1.0819, 1.2715, 1.6949, 1.4083, 0.2197, 0.9668, 3.9539, 6.5587, 1.3815],\n        [2.8502, 0.3321, 3.1304, 0.3508, 3.6062, 0.8899, 0.3565, 0.6027, 0.5566,\n         1.4495, 0.5355, 0.5429, 2.0155, 1.2365, 6.3550, 0.8597, 9.2838, 0.3289,\n         0.3841, 0.9719, 2.7713, 0.4119, 0.9283, 3.7739, 1.2275, 2.2690, 1.2710],\n        [2.8502, 0.3321, 3.1304, 0.3508, 3.6062, 0.8899, 0.3565, 0.6027, 0.5566,\n         1.4495, 0.5355, 0.5429, 2.0155, 1.2365, 6.3550, 0.8597, 9.2838, 0.3289,\n         0.3841, 0.9719, 2.7713, 0.4119, 0.9283, 3.7739, 1.2275, 2.2690, 1.2710],\n        [2.7347, 0.9839, 0.1469, 1.0171, 2.7535, 0.4283, 4.2858, 0.4226, 1.4723,\n         2.1959, 5.9862, 0.7006, 1.5476, 1.6059, 0.3119, 1.0550, 0.1142, 3.6601,\n         3.0433, 2.3501, 1.2314, 1.3154, 0.9998, 3.8861, 0.6364, 1.1733, 0.8812]])\n\n\n\n\n(xenc @ W)[3, 13]\n\ntensor(0.2123)\n\n\n\n\nxenc[3]\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\nW[:, 13]\n\ntensor([-0.3444,  0.4737,  0.0557, -0.1620, -0.6734,  0.4479, -0.7111,  1.3282,\n         0.2026,  0.0208,  0.2722,  0.3473, -0.6560,  0.2123,  1.7973,  1.2086,\n        -1.2879, -0.0824, -1.3538, -0.3161, -0.9458, -1.2972,  0.5641, -0.4949,\n         1.0295,  0.0753, -0.1173])\n\n\n\n\n(xenc[3] * W[:, 13]).sum() # is equal to (xenc @ W)[3, 13]\n\ntensor(0.2123)\n\n\n\n\nlogits = xenc @ W # log-counts\ncounts = logits.exp()\n\n\nprobs = counts / counts.sum(1, keepdims=True)\n\n\nprobs.shape\n\ntorch.Size([5, 27])\n\n\n\n\nprobs[0].sum()\n\ntensor(1.)\n\n\n\n\n\nSummary\n\nxs\n\ntensor([ 0,  5, 13, 13,  1])\n\n\n\n\nys\n\ntensor([ 5, 13, 13,  1,  0])\n\n\n\n\nW = torch.randn((27, 27), generator=g)\n\n\nxenc = F.one_hot(xs, num_classes=27).float()\nlogits = xenc @ W\ncounts = logits.exp()\nprobs = counts/counts.sum(1, keepdims=True)\n\n\nprobs.shape\n\ntorch.Size([5, 27])\n\n\n\n\nnlls = torch.zeros(5)\nfor i in range(5):\n    x = xs[i].item()\n    y = ys[i].item()\n    \n    print('-------------------')\n    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})')\n    print('input to the neural network: ', x)\n    print('output probabilities from the neural net:', probs[i])\n    print('label (actual next character):', y)\n    \n    p = probs[i, y]\n    print('probability assigned by the net to the correct character:', p.item())\n    \n    logp = torch.log(p)\n    print('log likelihood:', logp.item())\n    \n    nll = -logp\n    print('negative log likelihood:', nll.item())\n    \n    nlls[i] = nll\n    \nprint('========')\nprint('average negtaive log likelihood, i.e. loss = ', nlls.mean().item())\n\n-------------------\n\n\n\nbigram example 1: .e (indexes 0, 5)\n\n\n\ninput to the neural network:  0\n\n\n\noutput probabilities from the neural net: tensor([0.0204, 0.0134, 0.0078, 0.0670, 0.0130, 0.0115, 0.0175, 0.0121, \n0.0186,\n        0.0311, 0.0275, 0.1659, 0.0087, 0.0143, 0.0518, 0.0317, 0.0831, 0.0230,\n        0.0396, 0.0086, 0.0483, 0.0447, 0.0556, 0.0112, 0.0724, 0.0844, 0.0168])\n\n\n\nlabel (actual next character): 5\n\n\n\nprobability assigned by the net to the correct character: 0.011521384119987488\n\n\n\nlog likelihood: -4.463550567626953\n\n\n\nnegative log likelihood: 4.463550567626953\n\n\n\n-------------------\n\n\n\nbigram example 2: em (indexes 5, 13)\n\n\n\ninput to the neural network:  5\n\n\n\noutput probabilities from the neural net: tensor([0.0081, 0.0690, 0.0499, 0.1331, 0.0985, 0.0740, 0.0093, 0.0052, \n0.0234,\n        0.0321, 0.0267, 0.0309, 0.0093, 0.0228, 0.0269, 0.0085, 0.0049, 0.0363,\n        0.0139, 0.0326, 0.0531, 0.0262, 0.1151, 0.0097, 0.0136, 0.0420, 0.0248])\n\n\n\nlabel (actual next character): 13\n\n\n\nprobability assigned by the net to the correct character: 0.0227525494992733\n\n\n\nlog likelihood: -3.7830779552459717\n\n\n\nnegative log likelihood: 3.7830779552459717\n\n\n\n-------------------\n\n\n\nbigram example 3: mm (indexes 13, 13)\n\n\n\ninput to the neural network:  13\n\n\n\noutput probabilities from the neural net: tensor([0.0230, 0.0133, 0.0162, 0.0483, 0.0080, 0.0372, 0.0084, 0.0216, \n0.0159,\n        0.0524, 0.0227, 0.0227, 0.0092, 0.0415, 0.1000, 0.0354, 0.0172, 0.0423,\n        0.0553, 0.0036, 0.0085, 0.0553, 0.0140, 0.0077, 0.0252, 0.2709, 0.0243])\n\n\n\nlabel (actual next character): 13\n\n\n\nprobability assigned by the net to the correct character: 0.04153481870889664\n\n\n\nlog likelihood: -3.181223154067993\n\n\n\nnegative log likelihood: 3.181223154067993\n\n\n\n-------------------\n\n\n\nbigram example 4: ma (indexes 13, 1)\n\n\n\ninput to the neural network:  13\n\n\n\noutput probabilities from the neural net: tensor([0.0230, 0.0133, 0.0162, 0.0483, 0.0080, 0.0372, 0.0084, 0.0216, \n0.0159,\n        0.0524, 0.0227, 0.0227, 0.0092, 0.0415, 0.1000, 0.0354, 0.0172, 0.0423,\n        0.0553, 0.0036, 0.0085, 0.0553, 0.0140, 0.0077, 0.0252, 0.2709, 0.0243])\n\n\n\nlabel (actual next character): 1\n\n\n\nprobability assigned by the net to the correct character: 0.013294448144733906\n\n\n\nlog likelihood: -4.320408821105957\n\n\n\nnegative log likelihood: 4.320408821105957\n\n\n\n-------------------\n\n\n\nbigram example 5: a. (indexes 1, 0)\n\n\n\ninput to the neural network:  1\n\n\n\noutput probabilities from the neural net: tensor([0.0538, 0.0021, 0.3426, 0.0492, 0.0995, 0.0047, 0.0090, 0.0162, \n0.0012,\n        0.0138, 0.0374, 0.0028, 0.0075, 0.0097, 0.0124, 0.0284, 0.0163, 0.0218,\n        0.0011, 0.0579, 0.0165, 0.0460, 0.0432, 0.0132, 0.0680, 0.0072, 0.0184])\n\n\n\nlabel (actual next character): 0\n\n\n\nprobability assigned by the net to the correct character: 0.05381616950035095\n\n\n\nlog likelihood: -2.9221813678741455\n\n\n\nnegative log likelihood: 2.9221813678741455\n\n\n\n========\n\n\n\naverage negtaive log likelihood, i.e. loss =  3.734088182449341\n\n\n\nLets have the above one into function and try with different sampling\n\ndef train():\n    xenc = F.one_hot(xs, num_classes=27).float()\n    logits = xenc @ W\n    counts = logits.exp()\n    probs = counts/counts.sum(1, keepdims=True)\n    nlls = torch.zeros(5)\n    for i in range(5):\n        x = xs[i].item()\n        y = ys[i].item()\n\n        p = probs[i, y]\n\n        logp = torch.log(p)\n\n        nll = -logp\n\n        nlls[i] = nll\n\n\n    return nlls.mean().item()\n\n\nW = torch.randn((27, 27))\ntrain()\n\n3.5860557556152344\n\n\n\n\nW = torch.randn((27, 27))\ntrain()\n\n3.2332470417022705\n\n\n\n\n\nForward Pass\n\nxs, ys\n\n(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))\n\n\n\n\nprobs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]\n\n(tensor(0.0115), tensor(0.0228), tensor(0.0415), tensor(0.0133), tensor(0.0538))\n\n\n\n\ntorch.arange(5)\n\ntensor([0, 1, 2, 3, 4])\n\n\n\n\nprobs[torch.arange(5), ys]\n\ntensor([0.0115, 0.0228, 0.0415, 0.0133, 0.0538])\n\n\n\n\nprobs[torch.arange(5), ys].log()\n\ntensor([-4.4636, -3.7831, -3.1812, -4.3204, -2.9222])\n\n\n\n\nprobs[torch.arange(5), ys].log().mean()\n\ntensor(-3.7341)\n\n\n\n\nloss = - probs[torch.arange(5), ys].log().mean()\nloss\n\ntensor(3.7341)\n\n\n\n\ndef train():\n    xenc = F.one_hot(xs, num_classes=27).float()\n    logits = xenc @ W\n    counts = logits.exp()\n    probs = counts/counts.sum(1, keepdims=True)\n    loss = - probs[torch.arange(5), ys].log().mean()\n    return loss\n\n\nW = torch.randn((27, 27))\ntrain()\n\ntensor(3.2426)\n\n\n\n\n\nBackward Pass\n\n1st pass\n\nW = torch.randn((27, 27), requires_grad=True)\n\n\nW.grad = None # way to set to zero the gradient\nloss = train()\nloss.backward()\n\n\nloss\n\ntensor(4.3984, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\nW.shape, W.grad.shape\n\n(torch.Size([27, 27]), torch.Size([27, 27]))\n\n\n\n\nW.grad[:1]\n\ntensor([[ 0.0044,  0.0015,  0.0060,  0.0069,  0.0096, -0.1978,  0.0005,  0.0116,\n          0.0018,  0.0012,  0.0054,  0.0056,  0.0202,  0.0023,  0.0066,  0.0012,\n          0.0004,  0.0484,  0.0040,  0.0016,  0.0035,  0.0061,  0.0292,  0.0040,\n          0.0042,  0.0047,  0.0065]])\n\n\n\n\n\n2nd pass\n\nW.data += -0.1 * W.grad\n\n\nW.grad = None\nloss = train()\nloss.backward()\n\n\nloss\n\ntensor(4.3766, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\n3rd pass\n\nW.data += -0.1 * W.grad\n\n\nW.grad = None\nloss = train()\nloss.backward()\n\n\nloss\n\ntensor(4.3549, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\n\nTraining loop\n\nxs, ys = generate_training_set(words)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(\"Number of examples \", num)\nxenc = F.one_hot(xs, num_classes=27).float()\n\nNumber of examples  228146\n\n\n\n\ndef train(xenc, ys, epochs, lr = 0.1):\n    W = torch.randn((27, 27), requires_grad=True)\n    for epoch in range(epochs):\n        # forward pass\n        logits = xenc @ W\n        counts = logits.exp()\n        probs = counts/counts.sum(1, keepdims=True)\n        loss = - probs[torch.arange(ys.shape[0]), ys].log().mean()\n        print('Epoch: ', epoch, 'Loss: ', loss)\n        \n        # backward pass\n        W.grad = None\n        loss.backward()\n        W.data += - lr* W.grad\n    return W\n\n\nmodel = train(xenc, ys, 10, 1)\n\nEpoch:  0 Loss:  tensor(3.7543, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  1 Loss:  tensor(3.7461, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  2 Loss:  tensor(3.7380, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  3 Loss:  tensor(3.7300, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  4 Loss:  tensor(3.7221, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  5 Loss:  tensor(3.7143, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  6 Loss:  tensor(3.7066, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  7 Loss:  tensor(3.6990, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  8 Loss:  tensor(3.6914, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  9 Loss:  tensor(3.6840, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\nmodel = train(xenc, ys, 10, 10)\n\nEpoch:  0 Loss:  tensor(3.7679, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  1 Loss:  tensor(3.6911, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  2 Loss:  tensor(3.6209, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  3 Loss:  tensor(3.5565, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  4 Loss:  tensor(3.4974, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  5 Loss:  tensor(3.4433, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  6 Loss:  tensor(3.3937, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  7 Loss:  tensor(3.3482, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  8 Loss:  tensor(3.3064, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  9 Loss:  tensor(3.2681, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\nmodel = train(xenc, ys, 10, 100)\n\nEpoch:  0 Loss:  tensor(3.8536, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  1 Loss:  tensor(3.1448, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  2 Loss:  tensor(2.9057, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  3 Loss:  tensor(2.7856, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  4 Loss:  tensor(2.7163, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  5 Loss:  tensor(2.6870, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  6 Loss:  tensor(2.6442, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  7 Loss:  tensor(2.6310, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  8 Loss:  tensor(2.6032, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  9 Loss:  tensor(2.6044, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\nmodel = train(xenc, ys, 100, 10)\n\nEpoch:  0 Loss:  tensor(3.9659, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  1 Loss:  tensor(3.8651, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  2 Loss:  tensor(3.7738, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  3 Loss:  tensor(3.6906, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  4 Loss:  tensor(3.6145, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  5 Loss:  tensor(3.5448, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  6 Loss:  tensor(3.4810, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  7 Loss:  tensor(3.4227, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  8 Loss:  tensor(3.3695, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  9 Loss:  tensor(3.3209, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  10 Loss:  tensor(3.2766, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  11 Loss:  tensor(3.2362, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  12 Loss:  tensor(3.1992, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  13 Loss:  tensor(3.1654, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  14 Loss:  tensor(3.1343, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  15 Loss:  tensor(3.1055, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  16 Loss:  tensor(3.0788, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  17 Loss:  tensor(3.0540, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  18 Loss:  tensor(3.0307, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  19 Loss:  tensor(3.0089, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  20 Loss:  tensor(2.9884, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  21 Loss:  tensor(2.9690, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  22 Loss:  tensor(2.9507, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  23 Loss:  tensor(2.9334, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  24 Loss:  tensor(2.9170, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  25 Loss:  tensor(2.9015, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  26 Loss:  tensor(2.8867, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  27 Loss:  tensor(2.8727, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  28 Loss:  tensor(2.8594, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  29 Loss:  tensor(2.8467, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  30 Loss:  tensor(2.8347, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  31 Loss:  tensor(2.8232, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  32 Loss:  tensor(2.8123, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  33 Loss:  tensor(2.8019, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  34 Loss:  tensor(2.7920, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  35 Loss:  tensor(2.7825, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  36 Loss:  tensor(2.7735, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  37 Loss:  tensor(2.7649, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  38 Loss:  tensor(2.7567, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  39 Loss:  tensor(2.7489, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  40 Loss:  tensor(2.7414, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  41 Loss:  tensor(2.7343, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  42 Loss:  tensor(2.7274, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  43 Loss:  tensor(2.7209, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  44 Loss:  tensor(2.7147, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  45 Loss:  tensor(2.7087, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  46 Loss:  tensor(2.7030, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  47 Loss:  tensor(2.6975, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  48 Loss:  tensor(2.6923, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  49 Loss:  tensor(2.6873, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  50 Loss:  tensor(2.6824, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  51 Loss:  tensor(2.6778, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  52 Loss:  tensor(2.6734, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  53 Loss:  tensor(2.6691, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  54 Loss:  tensor(2.6650, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  55 Loss:  tensor(2.6611, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  56 Loss:  tensor(2.6573, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  57 Loss:  tensor(2.6536, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  58 Loss:  tensor(2.6501, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  59 Loss:  tensor(2.6467, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  60 Loss:  tensor(2.6434, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  61 Loss:  tensor(2.6403, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  62 Loss:  tensor(2.6372, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  63 Loss:  tensor(2.6343, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  64 Loss:  tensor(2.6314, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  65 Loss:  tensor(2.6287, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  66 Loss:  tensor(2.6260, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  67 Loss:  tensor(2.6235, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  68 Loss:  tensor(2.6210, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  69 Loss:  tensor(2.6185, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  70 Loss:  tensor(2.6162, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  71 Loss:  tensor(2.6139, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  72 Loss:  tensor(2.6117, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  73 Loss:  tensor(2.6096, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  74 Loss:  tensor(2.6075, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  75 Loss:  tensor(2.6055, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  76 Loss:  tensor(2.6035, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  77 Loss:  tensor(2.6016, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  78 Loss:  tensor(2.5998, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  79 Loss:  tensor(2.5980, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  80 Loss:  tensor(2.5962, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  81 Loss:  tensor(2.5945, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  82 Loss:  tensor(2.5928, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  83 Loss:  tensor(2.5912, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  84 Loss:  tensor(2.5896, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  85 Loss:  tensor(2.5881, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  86 Loss:  tensor(2.5866, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  87 Loss:  tensor(2.5851, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  88 Loss:  tensor(2.5837, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  89 Loss:  tensor(2.5823, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  90 Loss:  tensor(2.5809, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  91 Loss:  tensor(2.5796, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  92 Loss:  tensor(2.5783, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  93 Loss:  tensor(2.5770, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  94 Loss:  tensor(2.5757, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  95 Loss:  tensor(2.5745, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  96 Loss:  tensor(2.5733, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  97 Loss:  tensor(2.5721, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  98 Loss:  tensor(2.5710, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  99 Loss:  tensor(2.5698, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\nPrediction\n\ndef generate_names(count):\n    for i in range(count):\n        out = []\n        ix = 0\n        while True:\n            xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n            logits = xenc @ model # predict log-counts\n            counts = logits.exp()\n            p = counts/counts.sum(1, keepdims=True)\n\n            ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n            out.append(itos[ix])\n            if ix == 0:\n                break\n        print(''.join(out))\n\n\ngenerate_names(5)\n\nzriwreisona.\n\n\n\nady.\n\n\n\nmyonaxrolin.\n\n\n\narravispgoikeen.\n\n\n\narolouliymairekorqgbwyuere.\n\n\n\n\n\nEvaluate on Valid and Test set\n\nfrom torch.utils.data import random_split\n\n\nx_num = xenc.shape[0]\n\n\nxenc.shape\n\ntorch.Size([228146, 27])\n\n\n\n\ntest_range, valid_range, train_range = random_split(range(x_num), \n            [0.1, 0.1, 0.8], \n            generator=g)\n\n\ntest_idx = torch.tensor(test_range)\nvalid_idx = torch.tensor(valid_range)\ntrain_idx = torch.tensor(train_range)\n\n\nlen(train_idx), len(valid_idx), len(test_idx)\n\n(182516, 22815, 22815)\n\n\n\n\nx_train, y_train = xenc[train_idx], ys[train_idx]\nx_valid, y_valid = xenc[valid_idx], ys[valid_idx]\nx_test, y_test = xenc[test_idx], ys[test_idx]\n\n\nx_train.shape, x_valid.shape, x_test.shape\n\n(torch.Size([182516, 27]), torch.Size([22815, 27]), torch.Size([22815, 27]))\n\n\n\n\ny_train.shape, y_valid.shape, y_test.shape\n\n(torch.Size([182516]), torch.Size([22815]), torch.Size([22815]))\n\n\n\n\nmodel = train(x_train, y_train, 100, 10)\n\nEpoch:  0 Loss:  tensor(3.7710, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  1 Loss:  tensor(3.6776, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  2 Loss:  tensor(3.5960, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  3 Loss:  tensor(3.5230, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  4 Loss:  tensor(3.4572, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  5 Loss:  tensor(3.3980, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  6 Loss:  tensor(3.3445, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  7 Loss:  tensor(3.2964, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  8 Loss:  tensor(3.2528, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  9 Loss:  tensor(3.2134, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  10 Loss:  tensor(3.1774, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  11 Loss:  tensor(3.1445, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  12 Loss:  tensor(3.1142, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  13 Loss:  tensor(3.0862, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  14 Loss:  tensor(3.0601, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  15 Loss:  tensor(3.0357, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  16 Loss:  tensor(3.0128, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  17 Loss:  tensor(2.9913, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  18 Loss:  tensor(2.9711, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  19 Loss:  tensor(2.9520, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  20 Loss:  tensor(2.9340, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  21 Loss:  tensor(2.9170, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  22 Loss:  tensor(2.9009, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  23 Loss:  tensor(2.8856, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  24 Loss:  tensor(2.8712, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  25 Loss:  tensor(2.8575, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  26 Loss:  tensor(2.8446, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  27 Loss:  tensor(2.8323, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  28 Loss:  tensor(2.8206, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  29 Loss:  tensor(2.8096, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  30 Loss:  tensor(2.7991, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  31 Loss:  tensor(2.7892, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  32 Loss:  tensor(2.7798, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  33 Loss:  tensor(2.7708, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  34 Loss:  tensor(2.7623, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  35 Loss:  tensor(2.7542, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  36 Loss:  tensor(2.7466, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  37 Loss:  tensor(2.7392, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  38 Loss:  tensor(2.7323, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  39 Loss:  tensor(2.7256, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  40 Loss:  tensor(2.7193, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  41 Loss:  tensor(2.7132, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  42 Loss:  tensor(2.7074, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  43 Loss:  tensor(2.7019, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  44 Loss:  tensor(2.6966, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  45 Loss:  tensor(2.6915, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  46 Loss:  tensor(2.6866, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  47 Loss:  tensor(2.6819, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  48 Loss:  tensor(2.6774, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  49 Loss:  tensor(2.6731, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  50 Loss:  tensor(2.6689, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  51 Loss:  tensor(2.6649, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  52 Loss:  tensor(2.6610, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  53 Loss:  tensor(2.6572, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  54 Loss:  tensor(2.6536, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  55 Loss:  tensor(2.6501, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  56 Loss:  tensor(2.6467, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  57 Loss:  tensor(2.6434, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  58 Loss:  tensor(2.6402, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  59 Loss:  tensor(2.6372, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  60 Loss:  tensor(2.6342, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  61 Loss:  tensor(2.6313, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  62 Loss:  tensor(2.6285, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  63 Loss:  tensor(2.6258, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  64 Loss:  tensor(2.6231, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  65 Loss:  tensor(2.6206, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  66 Loss:  tensor(2.6181, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  67 Loss:  tensor(2.6156, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  68 Loss:  tensor(2.6133, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  69 Loss:  tensor(2.6110, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  70 Loss:  tensor(2.6087, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  71 Loss:  tensor(2.6066, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  72 Loss:  tensor(2.6044, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  73 Loss:  tensor(2.6024, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  74 Loss:  tensor(2.6004, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  75 Loss:  tensor(2.5984, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  76 Loss:  tensor(2.5965, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  77 Loss:  tensor(2.5946, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  78 Loss:  tensor(2.5928, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  79 Loss:  tensor(2.5910, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  80 Loss:  tensor(2.5893, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  81 Loss:  tensor(2.5876, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  82 Loss:  tensor(2.5860, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  83 Loss:  tensor(2.5844, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  84 Loss:  tensor(2.5828, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  85 Loss:  tensor(2.5812, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  86 Loss:  tensor(2.5797, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  87 Loss:  tensor(2.5783, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  88 Loss:  tensor(2.5768, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  89 Loss:  tensor(2.5754, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  90 Loss:  tensor(2.5741, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  91 Loss:  tensor(2.5727, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  92 Loss:  tensor(2.5714, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  93 Loss:  tensor(2.5701, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  94 Loss:  tensor(2.5689, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  95 Loss:  tensor(2.5676, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  96 Loss:  tensor(2.5664, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  97 Loss:  tensor(2.5652, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  98 Loss:  tensor(2.5641, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nEpoch:  99 Loss:  tensor(2.5629, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\nEvaluate on Valid set\n\nlogits_valid = x_valid @ model\ncounts_valid = logits_valid.exp()\npred_valid = counts_valid/counts_valid.sum(1, keepdims=True)\n- pred_valid[torch.arange(x_valid.shape[0]), y_valid].log().mean()\n\ntensor(2.5745, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\nEvaluate on Test set\n\nlogits_test = x_test @ model\ncounts_test = logits_test.exp()\npred_test = counts_test/counts_test.sum(1, keepdims=True)\n- pred_test[torch.arange(x_test.shape[0]), y_test].log().mean()\n\ntensor(2.5639, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n\n\n\nRegularization (Label Smoothing)\n\nAugment the loss function to have a small component (reguliarization loss) to have a smoother distribution of W. To make all W elements 0\n\n\nTo have a uniform probability distribution\n\n\n(W ** 2).mean()\n\ntensor(0.9617, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n\ndef train(xenc, ys, epochs, lr = 0.1, regularization_parameter = 0.01, print_every_epoch=False):\n    W = torch.randn((27, 27), requires_grad=True)\n    for epoch in range(epochs):\n        # forward pass\n        logits = xenc @ W\n        counts = logits.exp()\n        probs = counts/counts.sum(1, keepdims=True)\n        loss = - probs[torch.arange(ys.shape[0]), ys].log().mean()\n        regularization_loss = regularization_parameter * (W ** 2).mean()\n        loss += regularization_loss\n        if print_every_epoch:\n            print('Epoch: ', epoch, 'Loss: ', loss)\n        \n        # backward pass\n        W.grad = None\n        loss.backward()\n        W.data += - lr* W.grad\n        \n    print('Loss: ', loss)\n    return W\n\n\nmodel = train(x_train, y_train, 100, 10, 0.1)\n\nLoss:  tensor(2.6531, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nmodel = train(x_train, y_train, 100, 10, 1)\n\nLoss:  tensor(2.8925, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nmodel = train(x_train, y_train, 100, 10, 0.001)\n\nLoss:  tensor(2.5767, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nmodel = train(x_train, y_train, 100, 10, 0.0001)\n\nLoss:  tensor(2.5635, grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html",
    "href": "lecture_notes/building_a_wavenet.html",
    "title": "Building a WaveNet",
    "section": "",
    "text": "import numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n\n\n\n\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\n\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i + 1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\n\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\n\n\n\n\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n    \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1: n2])\nXte, Yte = build_dataset(words[n2:])\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n... --&gt; y\n..y --&gt; u\n.yu --&gt; h\nyuh --&gt; e\nuhe --&gt; n\nhen --&gt; g\neng --&gt; .\n... --&gt; d\n..d --&gt; i\n.di --&gt; o\ndio --&gt; n\nion --&gt; d\nond --&gt; r\nndr --&gt; e\ndre --&gt; .\n... --&gt; x\n..x --&gt; a\n.xa --&gt; v\nxav --&gt; i\navi --&gt; e\n\n\n\n\n\n\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5 # note: kaiming init\n        self.bias = torch.zeros(fan_out) if bias else None\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n    \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)\n            xvar = x.var(0, keepdim=True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    \n    def parameters(self):\n        return []\n\n\ntorch.manual_seed(42);\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nC = torch.randn((vocab_size, n_embd))\nlayers = [\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size)\n]\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight * 0.1 # last layer make less confident\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    \n    loss = F.cross_entropy(x, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    \n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0:\n        print(f'{i:7d}/{max_steps:7d}: {loss.item(): .4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000:  3.4915\n  10000/ 200000:  2.2179\n  20000/ 200000:  2.3681\n  30000/ 200000:  2.1342\n  40000/ 200000:  2.4067\n  50000/ 200000:  2.2406\n  60000/ 200000:  1.9608\n  70000/ 200000:  1.9236\n  80000/ 200000:  2.6588\n  90000/ 200000:  2.0502\n 100000/ 200000:  2.2596\n 110000/ 200000:  1.6270\n 120000/ 200000:  2.1705\n 130000/ 200000:  2.2806\n 140000/ 200000:  2.1980\n 150000/ 200000:  1.8434\n 160000/ 200000:  1.8251\n 170000/ 200000:  2.3077\n 180000/ 200000:  2.0817\n 190000/ 200000:  2.1585\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\n\nfor layer in layers:\n    layer.training = False\n\n\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n    x, y = {  \n        'train': (Xtr, Ytr),\n        'val': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }[split]\n    \n    emb = C[x]\n    x = emb.view(emb.shape[0], -1)\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\nNameError: name 'C' is not defined\n\n\n\n\n\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # forward pass the neural net\n        emb = C[torch.tensor([context])] # (1, block_size, n_embd)\n        x = emb.view(emb.shape[0], -1)\n        for layer in layers:\n            x = layer(x)\n        logits = x\n        probs = F.softmax(logits, dim = 1)\n        # sample from the distribution\n        ix = torch.multinomial(probs, num_samples = 1).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        # if we sample the special '.' token, break\n        if ix == 0: break\n        \n    print(''.join(itos[i] for i in out))\n\nivon.\nfanili.\nthoommestenell.\nmattevyn.\nalana.\njoleshaun.\nsiah.\nprus.\ncarleen.\njah.\njorrena.\njoriah.\njas.\nvishylaharia.\njuna.\nvio.\norven.\nmina.\nlaylee.\nesteffead."
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#starter-code",
    "href": "lecture_notes/building_a_wavenet.html#starter-code",
    "title": "Building a WaveNet",
    "section": "",
    "text": "import numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n\n\n\n\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n\n\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i + 1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n\n\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n\n\n\n\n\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n    \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1: n2])\nXte, Yte = build_dataset(words[n2:])\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n... --&gt; y\n..y --&gt; u\n.yu --&gt; h\nyuh --&gt; e\nuhe --&gt; n\nhen --&gt; g\neng --&gt; .\n... --&gt; d\n..d --&gt; i\n.di --&gt; o\ndio --&gt; n\nion --&gt; d\nond --&gt; r\nndr --&gt; e\ndre --&gt; .\n... --&gt; x\n..x --&gt; a\n.xa --&gt; v\nxav --&gt; i\navi --&gt; e\n\n\n\n\n\n\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5 # note: kaiming init\n        self.bias = torch.zeros(fan_out) if bias else None\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n    \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)\n            xvar = x.var(0, keepdim=True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    \n    def parameters(self):\n        return []\n\n\ntorch.manual_seed(42);\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nC = torch.randn((vocab_size, n_embd))\nlayers = [\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size)\n]\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight * 0.1 # last layer make less confident\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    \n    loss = F.cross_entropy(x, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    \n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0:\n        print(f'{i:7d}/{max_steps:7d}: {loss.item(): .4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000:  3.4915\n  10000/ 200000:  2.2179\n  20000/ 200000:  2.3681\n  30000/ 200000:  2.1342\n  40000/ 200000:  2.4067\n  50000/ 200000:  2.2406\n  60000/ 200000:  1.9608\n  70000/ 200000:  1.9236\n  80000/ 200000:  2.6588\n  90000/ 200000:  2.0502\n 100000/ 200000:  2.2596\n 110000/ 200000:  1.6270\n 120000/ 200000:  2.1705\n 130000/ 200000:  2.2806\n 140000/ 200000:  2.1980\n 150000/ 200000:  1.8434\n 160000/ 200000:  1.8251\n 170000/ 200000:  2.3077\n 180000/ 200000:  2.0817\n 190000/ 200000:  2.1585\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\n\nfor layer in layers:\n    layer.training = False\n\n\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n    x, y = {  \n        'train': (Xtr, Ytr),\n        'val': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }[split]\n    \n    emb = C[x]\n    x = emb.view(emb.shape[0], -1)\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\nNameError: name 'C' is not defined\n\n\n\n\n\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # forward pass the neural net\n        emb = C[torch.tensor([context])] # (1, block_size, n_embd)\n        x = emb.view(emb.shape[0], -1)\n        for layer in layers:\n            x = layer(x)\n        logits = x\n        probs = F.softmax(logits, dim = 1)\n        # sample from the distribution\n        ix = torch.multinomial(probs, num_samples = 1).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        # if we sample the special '.' token, break\n        if ix == 0: break\n        \n    print(''.join(itos[i] for i in out))\n\nivon.\nfanili.\nthoommestenell.\nmattevyn.\nalana.\njoleshaun.\nsiah.\nprus.\ncarleen.\njah.\njorrena.\njoriah.\njas.\nvishylaharia.\njuna.\nvio.\norven.\nmina.\nlaylee.\nesteffead."
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#fix-the-lr-plot",
    "href": "lecture_notes/building_a_wavenet.html#fix-the-lr-plot",
    "title": "Building a WaveNet",
    "section": "Fix the lr plot",
    "text": "Fix the lr plot\n\nlossi[:10]\n\n[0.5430157780647278,\n 0.5576249957084656,\n 0.523175835609436,\n 0.5327444672584534,\n 0.5206513404846191,\n 0.5284044742584229,\n 0.5306796431541443,\n 0.5056970119476318,\n 0.5213009119033813,\n 0.5147265195846558]\n\n\n\ntorch.arange(10).view(2, 5)\n\ntensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])\n\n\n\ntorch.tensor(lossi).view(-1, 1000).shape\n\ntorch.Size([200, 1000])\n\n\n\ntorch.tensor(lossi).view(-1, 1000).mean(1).shape\n\ntorch.Size([200])\n\n\n\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#pytorchifying-the-code",
    "href": "lecture_notes/building_a_wavenet.html#pytorchifying-the-code",
    "title": "Building a WaveNet",
    "section": "Pytorchifying the code",
    "text": "Pytorchifying the code\n\nlayers, containers, torch.nn, fun bugs\n\n\nLayers\n\nIntroducing new layers: embedding, flatten\n\n\nclass Embedding: \n    def __init__(self, num_embeddings, embedding_dim):\n        self.weight = torch.randn((num_embeddings, embedding_dim))\n        \n    def __call__(self, IX):\n        self.out = self.weight[IX]\n        return self.out\n    \n    def parameters(self):\n        return [self.weight]\n\n\nclass Flatten:\n    \n    def __call__(self, x):\n        self.out = x.view(x.shape[0], -1)\n        return self.out\n    \n    def parameters(self):\n        return []\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nlayers = [\n    Embedding(vocab_size, n_embd),\n    Flatten(),\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size)\n]\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    #forward pass\n    x = Xb\n    for layer in layers:\n        x = layer(x)\n    \n    loss = F.cross_entropy(x, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    \n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0:\n        print(f'{i:7d}/{max_steps:7d}: {loss.item(): .4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000:  3.6104\n  10000/ 200000:  2.1986\n  20000/ 200000:  2.3563\n  30000/ 200000:  2.2846\n  40000/ 200000:  2.3796\n  50000/ 200000:  2.2467\n  60000/ 200000:  2.2208\n  70000/ 200000:  2.3856\n  80000/ 200000:  1.7651\n  90000/ 200000:  2.1291\n 100000/ 200000:  2.0727\n 110000/ 200000:  2.3707\n 120000/ 200000:  1.8593\n 130000/ 200000:  2.2348\n 140000/ 200000:  1.7190\n 150000/ 200000:  2.3271\n 160000/ 200000:  1.9129\n 170000/ 200000:  2.0262\n 180000/ 200000:  1.7435\n 190000/ 200000:  2.2444\n\n\n\n\nContainers\n\nclass Sequential:\n    def __init__(self, layers):\n        self.layers = layers\n        \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n    \n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    Flatten(),\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size)\n])\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight * 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    #forward pass\n    logits = model(Xb)\n    \n    loss = F.cross_entropy(logits, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    \n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0:\n        print(f'{i:7d}/{max_steps:7d}: {loss.item(): .4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000:  3.6506\n  10000/ 200000:  2.3977\n  20000/ 200000:  1.8598\n  30000/ 200000:  2.1111\n  40000/ 200000:  2.3361\n  50000/ 200000:  2.1747\n  60000/ 200000:  2.1322\n  70000/ 200000:  2.3737\n  80000/ 200000:  2.1701\n  90000/ 200000:  2.1263\n 100000/ 200000:  1.7871\n 110000/ 200000:  2.3637\n 120000/ 200000:  2.1640\n 130000/ 200000:  2.1788\n 140000/ 200000:  2.4404\n 150000/ 200000:  2.1481\n 160000/ 200000:  2.0980\n 170000/ 200000:  2.0363\n 180000/ 200000:  2.1364\n 190000/ 200000:  1.8888\n\n\n\n\neval mode\n\n# put layers into eval mode (needed for batchnorm especially)\n\nfor layer in model.layers:\n    layer.training = False\n\n\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n    x, y = {  \n        'train': (Xtr, Ytr),\n        'val': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }[split]\n    \n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.765791416168213\nval 1.9921294450759888\n\n\n\n\nSample from model\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # forward pass the neural net\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim = 1)\n        # sample from the distribution\n        ix = torch.multinomial(probs, num_samples = 1).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        # if we sample the special '.' token, break\n        if ix == 0: break\n        \n    print(''.join(itos[i] for i in out))\n\nnayah.\nkent.\nrohyn.\naaissalyn.\naxminaiyah.\ntayyah.\ndum.\nbrena.\nhoselia.\nkehanikki.\normonadit.\nluchelyn.\nlin.\njannal.\nnoes.\nsude.\nraylen.\nanvika.\nalea.\nwylin."
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#increase-the-context-size",
    "href": "lecture_notes/building_a_wavenet.html#increase-the-context-size",
    "title": "Building a WaveNet",
    "section": "Increase the context size",
    "text": "Increase the context size\n\nblock_size = 8\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1: n2])\nXte, Yte = build_dataset(words[n2:])\n\ntorch.Size([182625, 8]) torch.Size([182625])\ntorch.Size([22655, 8]) torch.Size([22655])\ntorch.Size([22866, 8]) torch.Size([22866])\n\n\n\nfor x, y in zip(Xtr[:20], Ytr[:20]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n........ --&gt; y\n.......y --&gt; u\n......yu --&gt; h\n.....yuh --&gt; e\n....yuhe --&gt; n\n...yuhen --&gt; g\n..yuheng --&gt; .\n........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n........ --&gt; x\n.......x --&gt; a\n......xa --&gt; v\n.....xav --&gt; i\n....xavi --&gt; e\n\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    Flatten(),\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size)\n])\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight * 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters:\n    p.requires_grad = True\n\n22097\n\n\n\ntraining mode\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    #forward pass\n    logits = model(Xb)\n    \n    loss = F.cross_entropy(logits, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    \n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0:\n        print(f'{i:7d}/{max_steps:7d}: {loss.item(): .4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000:  3.7162\n  10000/ 200000:  1.8141\n  20000/ 200000:  2.0207\n  30000/ 200000:  2.1630\n  40000/ 200000:  2.4232\n  50000/ 200000:  2.3131\n  60000/ 200000:  1.9144\n  70000/ 200000:  1.9182\n  80000/ 200000:  2.4822\n  90000/ 200000:  2.1356\n 100000/ 200000:  2.0794\n 110000/ 200000:  2.1322\n 120000/ 200000:  1.8838\n 130000/ 200000:  2.3476\n 140000/ 200000:  2.0343\n 150000/ 200000:  2.0312\n 160000/ 200000:  2.3358\n 170000/ 200000:  2.0393\n 180000/ 200000:  2.1933\n 190000/ 200000:  1.9221\n\n\n\n\neval mode\n\n# put layers into eval mode (needed for batchnorm especially)\n\nfor layer in model.layers:\n    layer.training = False\n\n\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n    x, y = {  \n        'train': (Xtr, Ytr),\n        'val': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }[split]\n    \n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.9201935529708862\nval 2.0279388427734375\n\n\n\n\nPerformance log\n\noriginal (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\ncontext: 3 -&gt; 8 (22K params): train 1.918, val 2.027\n\n\n# sample from model\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size\n    \n    while True:\n        # forward pass the neural net\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        # sample from the distribution\n        ix = torch.multinomial(probs, num_samples=1).item()\n        #shift the context window and track the samples\n        context = context[1:] + [ix]\n        out.append(ix)\n        # if we sample the special '.' token, break\n        if ix == 0: break\n    \n    print(''.join(itos[i] for i in out))\n\nhenyx.\nterna.\nnyaad.\nmaganta.\njalori.\ndelci.\ndysenni.\nkalven.\nallington.\nteb.\nkingstonaa.\nleonnsta.\npaisle.\nanny.\nshanidi.\nazrislaca.\nbrandan.\njiriana.\nnathawa.\nella."
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#implementing-wavenet",
    "href": "lecture_notes/building_a_wavenet.html#implementing-wavenet",
    "title": "Building a WaveNet",
    "section": "Implementing Wavenet",
    "text": "Implementing Wavenet\n\nix = torch.randint(0, Xtr.shape[0], (4,)) # lets look at batch of just 4 examples\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  1, 19,  8, 22]])\n\n\n\nix\n\ntensor([ 71354,  75603,   1768, 142786])\n\n\n\nmodel.layers[0].out.shape\n\ntorch.Size([4, 8, 10])\n\n\n\nmodel.layers[1].out.shape\n\ntorch.Size([4, 80])\n\n\n\nmodel.layers[2].out.shape\n\ntorch.Size([4, 200])\n\n\n\n(torch.randn(4, 5, 6, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n\ntorch.Size([4, 5, 6, 200])\n\n\n\n(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape\n\ntorch.Size([4, 4, 200])\n\n\n\ne = torch.randn(4, 8, 10) # goal: want this to be (4, 4, 20) where consecutive 10d vectors are concatenated\nexplicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2)\n\n\n(e.view(4, 4, 20) == explicit).all()\n\ntensor(True)\n\n\n\nclass FlattenConsecutive:\n    def __init__(self, n):\n        self.n = n\n    \n    def __call__(self, x):\n        B, T, C = x.shape\n        x = x.view(B, T//self.n, C*self.n)\n        if x.shape[1] == 1: \n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n    \n    def parameters(self):\n        return []\n\n\nblock_size\n\n8\n\n\n\nn_embd = 10 # the dimentionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel  = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(block_size),\n    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters: p.requires_grad = True\n\n22097\n\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0, 12,  1]])\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 80)\nLinear : (4, 200)\nBatchNorm1d : (4, 200)\nTanh : (4, 200)\nLinear : (4, 27)\n\n\n\nblock_size = 2\n\n\nn_embd = 10 # the dimentionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel  = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(block_size), Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(block_size), Linear(n_hidden * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(block_size), Linear(n_hidden * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1 # last layer make less confident\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters: p.requires_grad = True\n\n170897\n\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  1,  2,  8,  9],\n        [ 0,  0,  0,  0,  0, 18,  1,  9],\n        [ 0,  0,  0,  0,  0,  0,  7,  1]])\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 200)\nBatchNorm1d : (4, 4, 200)\nTanh : (4, 4, 200)\nFlattenConsecutive : (4, 2, 400)\nLinear : (4, 2, 200)\nBatchNorm1d : (4, 2, 200)\nTanh : (4, 2, 200)\nFlattenConsecutive : (4, 400)\nLinear : (4, 200)\nBatchNorm1d : (4, 200)\nTanh : (4, 200)\nLinear : (4, 27)"
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#training-wavenet-first-pass",
    "href": "lecture_notes/building_a_wavenet.html#training-wavenet-first-pass",
    "title": "Building a WaveNet",
    "section": "Training Wavenet: First Pass",
    "text": "Training Wavenet: First Pass\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 68 # the number of neurons in the hidden layer of the MLP \n# --&gt; want to have the same parameters when the block size was 8\n\nblock_size = 2\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters: p.requires_grad = True\n\n22397\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters: \n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000: 3.6852\n  10000/ 200000: 2.3648\n  20000/ 200000: 2.1416\n  30000/ 200000: 1.9978\n  40000/ 200000: 2.2832\n  50000/ 200000: 1.9900\n  60000/ 200000: 2.0611\n  70000/ 200000: 1.8055\n  80000/ 200000: 2.6343\n  90000/ 200000: 2.1440\n 100000/ 200000: 2.0204\n 110000/ 200000: 1.8402\n 120000/ 200000: 1.5136\n 130000/ 200000: 1.8751\n 140000/ 200000: 2.1031\n 150000/ 200000: 2.0538\n 160000/ 200000: 2.0164\n 170000/ 200000: 2.2596\n 180000/ 200000: 1.9372\n 190000/ 200000: 1.6855\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n\nEmbedding : (32, 8, 10)\nFlattenConsecutive : (32, 4, 20)\nLinear : (32, 4, 68)\nBatchNorm1d : (32, 4, 68)\nTanh : (32, 4, 68)\nFlattenConsecutive : (32, 2, 136)\nLinear : (32, 2, 68)\nBatchNorm1d : (32, 2, 68)\nTanh : (32, 2, 68)\nFlattenConsecutive : (32, 136)\nLinear : (32, 68)\nBatchNorm1d : (32, 68)\nTanh : (32, 68)\nLinear : (32, 27)\n\n\n\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers: layer.training = False\n\n\n# evaluate the loss\n@torch.no_grad() # this decorator disables gradient tracking inside pytorch\ndef split_loss(split):\n    x, y = {\n        'train': (Xtr, Ytr),\n        'val': (Xdev, Ydev),\n        'test': (Xte, Yte)\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n    \nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.9430139064788818\nval 2.027573585510254"
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#fixing-batchnorm1d-bug",
    "href": "lecture_notes/building_a_wavenet.html#fixing-batchnorm1d-bug",
    "title": "Building a WaveNet",
    "section": "Fixing BatchNorm1D bug",
    "text": "Fixing BatchNorm1D bug\n\nclass BatchNorm1d:\n    \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n    \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            if x.ndim == 2: dim = 0\n            elif x.ndim == 3: dim = (0, 1)\n            xmean = x.mean(dim, keepdim=True)\n            xvar = x.var(dim, keepdim=True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 68 # the number of neurons in the hidden layer of the MLP \n# --&gt; want to have the same parameters when the block size was 8\n\n# block_size = 2\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\n# parameter init\n# with torch.no_grad():\n#     layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters: p.requires_grad = True\n\n22397\n\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0,  8,  5],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0, 11,  1, 13,  2, 18],\n        [ 0, 25,  1, 26, 13,  9, 14,  5]])\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__, ':', tuple(layer.out.shape))\n\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 68)\nBatchNorm1d : (4, 4, 68)\nTanh : (4, 4, 68)\nFlattenConsecutive : (4, 2, 136)\nLinear : (4, 2, 68)\nBatchNorm1d : (4, 2, 68)\nTanh : (4, 2, 68)\nFlattenConsecutive : (4, 136)\nLinear : (4, 68)\nBatchNorm1d : (4, 68)\nTanh : (4, 68)\nLinear : (4, 27)\n\n\n\nmodel.layers[3].running_mean.shape\n\ntorch.Size([1, 1, 68])\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters: \n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000: 3.4804\n  10000/ 200000: 1.9962\n  20000/ 200000: 2.0834\n  30000/ 200000: 2.0940\n  40000/ 200000: 2.6074\n  50000/ 200000: 1.9911\n  60000/ 200000: 1.6505\n  70000/ 200000: 2.0625\n  80000/ 200000: 2.0239\n  90000/ 200000: 1.9839\n 100000/ 200000: 1.8198\n 110000/ 200000: 1.9920\n 120000/ 200000: 1.8590\n 130000/ 200000: 2.0162\n 140000/ 200000: 1.9531\n 150000/ 200000: 1.9374\n 160000/ 200000: 1.7478\n 170000/ 200000: 1.7850\n 180000/ 200000: 1.9224\n 190000/ 200000: 1.7220\n\n\n\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n\n\n\n\n\nfor layer in model.layers: layer.training = False\n\n\nsplit_loss('train')\n\ntrain 1.9116132259368896\n\n\n\nsplit_loss('val')\n\nval 2.023597240447998\n\n\n\nsplit_loss('test')\n\ntest 2.013340711593628"
  },
  {
    "objectID": "lecture_notes/building_a_wavenet.html#scaling-up-wavenet",
    "href": "lecture_notes/building_a_wavenet.html#scaling-up-wavenet",
    "title": "Building a WaveNet",
    "section": "Scaling up Wavenet",
    "text": "Scaling up Wavenet\n\nn_embd = 24 # the dimensionality of the character embedding vectors\nn_hidden = 128 # the number of neurons in the hidden layer of the MLP \n# --&gt; want to have the same parameters when the block size was 8\n\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters))\nfor p in parameters: p.requires_grad = True\n\n76579\n\n\n\n# same optimization as last time\nmax_steps = 200_000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n    \n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # update: simple SGD\n    lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n    for p in parameters: \n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10_000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000: 3.2997\n  10000/ 200000: 1.8203\n  20000/ 200000: 1.8036\n  30000/ 200000: 2.1780\n  40000/ 200000: 2.1453\n  50000/ 200000: 1.8008\n  60000/ 200000: 1.6666\n  70000/ 200000: 2.3526\n  80000/ 200000: 1.4201\n  90000/ 200000: 1.7949\n 100000/ 200000: 1.5605\n 110000/ 200000: 1.7919\n 120000/ 200000: 1.6035\n 130000/ 200000: 1.9319\n 140000/ 200000: 1.8383\n 150000/ 200000: 1.6744\n 160000/ 200000: 1.5064\n 170000/ 200000: 1.3609\n 180000/ 200000: 1.8823\n 190000/ 200000: 2.1009\n\n\n\nfor layer in model.layers: layer.training = False\n\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.765791416168213\nval 1.9921294450759888\n\n\n\nlen(Xdev)\n\n22655"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html",
    "href": "lecture_notes/building_micrograd.html",
    "title": "Mircrograd from scratch",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rich import print\nfrom rich import pretty\npretty.install()"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#single-variable-x",
    "href": "lecture_notes/building_micrograd.html#single-variable-x",
    "title": "Mircrograd from scratch",
    "section": "Single Variable: x",
    "text": "Single Variable: x\nCreate a function\n\ndef f(x): return 3*x**2 - 4*x + 5\n\n\nf(3.0)\n\n20.0\n\n\n\n\nxs = np.arange(-5, 5, 0.25)\nxs\n\narray([-5.  , -4.75, -4.5 , -4.25, -4.  , -3.75, -3.5 , -3.25, -3.  ,\n       -2.75, -2.5 , -2.25, -2.  , -1.75, -1.5 , -1.25, -1.  , -0.75,\n       -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ,  1.25,  1.5 ,\n        1.75,  2.  ,  2.25,  2.5 ,  2.75,  3.  ,  3.25,  3.5 ,  3.75,\n        4.  ,  4.25,  4.5 ,  4.75])\n\n\n\n\nys = f(xs)\nys\n\narray([100.    ,  91.6875,  83.75  ,  76.1875,  69.    ,  62.1875,\n        55.75  ,  49.6875,  44.    ,  38.6875,  33.75  ,  29.1875,\n        25.    ,  21.1875,  17.75  ,  14.6875,  12.    ,   9.6875,\n         7.75  ,   6.1875,   5.    ,   4.1875,   3.75  ,   3.6875,\n         4.    ,   4.6875,   5.75  ,   7.1875,   9.    ,  11.1875,\n        13.75  ,  16.6875,  20.    ,  23.6875,  27.75  ,  32.1875,\n        37.    ,  42.1875,  47.75  ,  53.6875])\n\n\n\nPlot the function\n\nplt.plot(xs, ys)\n\n[&lt;matplotlib.lines.Line2D object at 0x7fb25cd152d0&gt;]\n\n\n\n\n\n\n\nDerivative on increasing side of the curve\n\nh = 0.0000000001\nx = 3.0\n(f(x + h) - f(x))/h\n\n14.000001158365194\n\n\n\n\n\nDerivative on the decreasing side of the curve\n\nh = 0.0000000001\nx = -3.0\n(f(x + h) - f(x))/h\n\n-21.999966293151374\n\n\n\n\n\nDerivative on the bottom of the curve\n\nh = 0.0000000001\nx = 2/3\n(f(x + h) - f(x))/h\n\n0.0"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#multivariable-a-b-c",
    "href": "lecture_notes/building_micrograd.html#multivariable-a-b-c",
    "title": "Mircrograd from scratch",
    "section": "Multivariable: a, b, c",
    "text": "Multivariable: a, b, c\n\na = 2.0\nb = -3.0\nc = 10.0\ndef d(a, b, c): return a*b + c\nprint(d(a, b, c))\n\n4.0\n\n\n\n\nDerivative with respect to a\n\nh = 0.0001\n\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = d(a, b, c)\n\na += h\nd2 = d(a, b, c)\n\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2 - d1)/h)\n\nd1 4.0\n\n\n\nd2 3.999699999999999\n\n\n\nslope -3.000000000010772\n\n\n\n\n\nDerivative with respect to b\n\nh = 0.0001\n\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = d(a, b, c)\n\nb += h\nd2 = d(a, b, c)\n\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2 - d1)/h)\n\nd1 4.0\n\n\n\nd2 4.0002\n\n\n\nslope 2.0000000000042206\n\n\n\n\n\nDerivative with respect to c\n\nh = 0.0001\n\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = d(a, b, c)\n\nc += h\nd2 = d(a, b, c)\n\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2 - d1)/h)\n\nd1 4.0\n\n\n\nd2 4.0001\n\n\n\nslope 0.9999999999976694"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#create-value-object",
    "href": "lecture_notes/building_micrograd.html#create-value-object",
    "title": "Mircrograd from scratch",
    "section": "Create Value Object",
    "text": "Create Value Object\n(mentioned in the README of micrograd )\n\nDefine intial template of Value Class\n\nclass Value:\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\na = Value(2.0)\nb = Value(-3.0)\na, b\n\n(Value(data=2.0), Value(data=-3.0))\n\n\n\n\n\nAdd the add function\n\nclass Value:\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other): # ⭠ for adding among the value objects\n        return Value(self.data + other.data)\n\n\na = Value(2.0)\nb = Value(-3.0)\na, b\n\n(Value(data=2.0), Value(data=-3.0))\n\n\n\n\na + b # a.__add__(b)\n\nValue(data=-1.0)\n\n\n\n\n\nAdd the mul function\n\nclass Value:\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data)\n\n    def __mul__(self, other): # ⭠ for multiplying among the value objects\n        return Value(self.data * other.data)\n\n\na = Value(2.0)\nb = Value(-3.0)\na, b\n\n(Value(data=2.0), Value(data=-3.0))\n\n\n\n\na * b # a.__mul__(b)\n\nValue(data=-6.0)\n\n\n\n\nc = Value(10.0)\n\n\nd = a * b + c; d\n\nValue(data=4.0)\n\n\n\n\n\nAdd the functionality to know what values created a value with _children\n\nclass Value:\n    def __init__(self, data, _children=()): # ⭠ Add _children\n        self.data = data\n        self._prev = set(_children) # ⭠ Add _children\n\n    def __repr__(self):\n        return f\"Value(data ={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data, (self, other))\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, (self, other))\n\n\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a*b + c\nd\n\nValue(data =4.0)\n\n\n\n\nd._prev # childrens are -6.0 (a *b) and 10.0 (c)\n\n{Value(data =-6.0), Value(data =10.0)}\n\n\n\n\n\nAdd the functionality to know what operations created a value with _op\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''): # ⭠ Add _op\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op # ⭠ Add _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data, (self, other), '+')\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, (self, other), '*')\n\n\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a*b + c\nd\n\nValue(data=4.0)\n\n\n\n\nd._prev\n\n{Value(data=10.0), Value(data=-6.0)}\n\n\n\n\nd._op\n\n'+'\n\n\n\n\n\nVisualize the expression graph with operators and operands\n\nfrom graphviz import Digraph\n\ndef trace(root):\n    # build a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\n\ndef draw_dot(root, label):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # for any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label=label(n), shape='record') # ⭠ label function getting called\n        if n._op:\n            # if this value is a result of some operation, create an op node for it\n            dot.node(name = uid + n._op, label = n._op)\n            dot.edge(uid + n._op, uid)\n  \n    for n1, n2 in edges:\n        # connect n1 to the op node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n  \n    return dot\n\n\ndef label(node): return \"{data %.4f}\" % (node.data)\ndraw_dot(d, label)\n\n\n\n\n\n\nAdd label to each node\n\nso that we know what are the corresponding variables for each value\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''): # ⭠ Add label\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label # ⭠ Add label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data, (self, other), '+')\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, (self, other), '*')\n\n\na = Value(2.0, label = 'a')\nb = Value(-3.0, label='b')\nc = Value(10, label = 'c')\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\nf = Value(-2.0, label='f')\nL = d * f; L.label = 'L'\nL\n\nValue(label=L data=-8.0)\n\n\n\nChange the label function to render the label\n\ndef label(node): return \"{%s | {data %.4f}}\" % (node.label, node.data)\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nAdd grad to Value class\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data, (self, other), '+')\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, (self, other), '*')\n\n\na = Value(2.0, label = 'a')\nb = Value(-3.0, label='b')\nc = Value(10, label = 'c')\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\nf = Value(-2.0, label='f')\nL = d * f; L.label = 'L'\nL.grad\n\n0.0\n\n\n\n\ndef label(node): return \"{%s | {data %.4f} | grad %.4f}\" % (node.label, node.data, node.grad)\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nCreate a function lol\n\nDerive with respect to a\n\ndef lol():\n    h = 0.0001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0 + h, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n6.000000000021544\n\n\n\n\n\nDerive with respect to L\n\ndef lol():\n    h = 0.0001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data + h\n\n    print((L2 - L1) / h)\n\nlol()\n\n0.9999999999976694\n\n\n\n\nL.grad = 1\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to f\n\\[ L = f \\cdot d \\]\n\\[ \\frac{\\partial L}{\\partial f} = \\frac{\\partial (f \\cdot d)}{\\partial f} = d = 4.0 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0 + h, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n3.9999999999995595\n\n\n\n\nf.grad = 4\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to d\n\\[ \\frac{\\partial L}{\\partial d} = \\frac{\\partial (f \\cdot d)}{\\partial d} = f = -2.0 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    d.data += h\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n-2.000000000000668\n\n\n\n\nd.grad = -2\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to c\n\\[ \\frac{\\partial d}{\\partial c} = \\frac{\\partial (c + e)}{\\partial c} = 1.0 \\]\n\\[ \\frac{\\partial L}{\\partial c} = \\frac{\\partial L}{\\partial d}\\cdot\\frac{\\partial d}{\\partial c} = f = -2.0 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10 + h, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n-1.9999999999988916\n\n\n\n\nc.grad = -2\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to e\n\\[ \\frac{\\partial d}{\\partial e} = \\frac{\\partial (c + e)}{\\partial e} = 1.0 \\]\n\\[ \\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e} = f = -2.0 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    e.data += h\n    \n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n-2.000000000000668\n\n\n\n\ne.grad = -2\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to a\n\\[ \\frac{\\partial e}{\\partial a} = \\frac{\\partial ({a}\\cdot{b})}{\\partial a} = b \\]\n\\[ \\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial a} = -2b = 6 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0 + h, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    \n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n6.000000000000227\n\n\n\n\na.grad = 6\n\n\ndraw_dot(L, label)\n\n\n\n\n\n\nDerivative of L with respect to b\n\\[ \\frac{\\partial e}{\\partial b} = \\frac{\\partial ({a}\\cdot{b})}{\\partial b} = a \\]\n\\[ \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial b}= -2a = -4 \\]\n\ndef lol():\n    h = 0.001\n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L1 = L.data \n\n    a = Value(2.0, label = 'a')\n    b = Value(-3.0 + h, label='b')\n    c = Value(10, label = 'c')\n    e = a*b; e.label = 'e'\n    \n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label='f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2 - L1) / h)\n\nlol()\n\n-3.9999999999995595\n\n\n\n\nb.grad = -4\n\n\ndraw_dot(L, label)\n\n\n\n\n\na.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nf.data += 0.01 * f.grad\n\n\ne = a * b\nd = e + c\nL = d * f\nprint(L.data)\n\n-7.286496"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#neural-network",
    "href": "lecture_notes/building_micrograd.html#neural-network",
    "title": "Mircrograd from scratch",
    "section": "Neural Network",
    "text": "Neural Network\n\nTanh\n\nplt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid();\n\n\n\n\n\n\nAdd tanh\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        return Value(self.data + other.data, (self, other), '+')\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, (self, other), '*')\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        return out\n\n\n\nInputs: x1, x2\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n\n\nWeights: w1, w2\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n\n\nBias\n\nb = Value(6.8813735870195432, label='b')\n\n\n\nx1w1 + x2w2 + b\n\nx1w1 = x1*w1; x1w1.label='x1*w1'\nx2w2 = x2*w2; x2w2.label='x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label='n'\n\n\no = n.tanh(); o.label = 'o'\n\n\ndraw_dot(o, label)\n\n\n\n\n\n\nComputing gradient of each node manually\n\\[ \\frac{\\partial o}{\\partial o} = 1 \\]\n\no.grad = 1.0\n\n\ndraw_dot(o, label)\n\n\n\n\n\\[ o = \\tanh(n) \\] \\[ \\frac{\\partial o}{\\partial n} = \\frac{\\partial{\\tanh(n)}}{\\partial n} = 1 - \\tanh(n)^2 = 1 - o^2 \\]\n\n1 - (o.data ** 2)\n\n0.4999999999999999\n\n\n\n\nn.grad = 0.5\n\n\ndraw_dot(o, label)\n\n\n\n\n\nWith pluses as we saw the gradient will be same as previous gradient\n\n\nx1w1x2w2.grad = 0.5\nb.grad = 0.5\n\n\ndraw_dot(o, label)\n\n\n\n\n\nx1w1.grad = 0.5\nx2w2.grad = 0.5\n\n\ndraw_dot(o, label)\n\n\n\n\n\nx2.grad = w2.data * x2w2.grad\nw2.grad = x2.data * x2w2.grad\n\n\ndraw_dot(o, label)\n\n\n\n\n\nx1.grad = w1.data * x1w1.grad\nw1.grad = x1.data * x1w1.grad\n\n\ndraw_dot(o, label)\n\n\n\n\n\n\nComputing gradient of each node with _backward()\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad = 1.0 * out.grad\n            other.grad = 1.0 * out.grad\n        out._backward = _backward\n        return out\n        \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad = other.data * out.grad\n            other.grad = self.data * out.grad\n        out._backward = _backward\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        \n        def _backward():\n            self.grad = (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n\nLets take the NN code from the above\n\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label='x1*w1'\nx2w2 = x2*w2; x2w2.label='x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label='n'\no = n.tanh(); o.label = 'o'\n\n\ndraw_dot(o, label)\n\n\n\n\n\nBackward on o\n\no.grad = 1.0 # setting this to 1 because Value's grad variable is 0\no._backward();  n.grad\n\n0.4999999999999999\n\n\n\n\n\nBackward on n\n\nn._backward(); \nb.grad, x1w1x2w2.grad\n\n(0.4999999999999999, 0.4999999999999999)\n\n\n\n\n\nBackward on b\n\nb._backward();\n\n\n\nBackward on x1w1x2w2\n\nx1w1x2w2._backward(); \nx1w1.grad, x2w2.grad\n\n(0.4999999999999999, 0.4999999999999999)\n\n\n\n\n\nBackward on x2w2\n\nx2w2._backward()\nx2.grad, w2.grad\n\n(0.4999999999999999, 0.0)\n\n\n\n\n\nBackward on x1w1\n\nx1w1._backward()\nx1.grad, w1.grad\n\n(-1.4999999999999996, 0.9999999999999998)\n\n\n\n\nDraw the computation graph\n\n\ndraw_dot(o, label)\n\n\n\n\n\n\n\nBackward propogation with one call\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label='x1*w1'\nx2w2 = x2*w2; x2w2.label='x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label='n'\no = n.tanh(); o.label = 'o'\n\n\ndraw_dot(o, label)\n\n\n\n\n\nTopological sort\n\ntopo = []\nvisited = set()\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v._prev:\n            build_topo(child)\n        topo.append(v)\nbuild_topo(o)\ntopo\n\n[\n    Value(label=x1 data=2.0),\n    Value(label=w1 data=-3.0),\n    Value(label=x1*w1 data=-6.0),\n    Value(label=w2 data=1.0),\n    Value(label=x2 data=0.0),\n    Value(label=x2*w2 data=0.0),\n    Value(label=x1*w1 + x2*w2 data=-6.0),\n    Value(label=b data=6.881373587019543),\n    Value(label=n data=0.8813735870195432),\n    Value(label=o data=0.7071067811865476)\n]\n\n\n\n\n\nApply backward in reverse order of topological order of the computation graph\n\nfor node in reversed(topo):\n    node._backward()\n\n\ndraw_dot(o, label)\n\n\n\n\n\n\nAdd backward to Value\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad = 1.0 * out.grad\n            other.grad = 1.0 * out.grad\n        out._backward = _backward\n        return out\n        \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad = other.data * out.grad\n            other.grad = self.data * out.grad\n        out._backward = _backward\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        \n        def _backward():\n            self.grad = (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label='x1*w1'\nx2w2 = x2*w2; x2w2.label='x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label='n'\no = n.tanh(); o.label = 'o'\n\n\no.backward()\n\n\ndraw_dot(o, label)\n\n\n\n\n\n\n\nFixing a backprop bug\n\na = Value(3.0, label='b')\nb = a + a; b.label = 'b'\nb.backward()\ndraw_dot(b, label)\n\n\n\n\n\na = Value(-2.0, label='a')\nb = Value(3.0, label='b')\nd = a*b; d.label = 'd'\ne = a+b; e.label = 'e'\nf = d*e; f.label = 'f'\nf.backward()\ndraw_dot(f, label)\n\n\n\n\n\nAccumulate the gradient\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n            other.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n        \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad # &lt;- Accumulate the gradient\n            other.grad += self.data * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        \n        def _backward():\n            self.grad += (1 - t**2) * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\na = Value(3.0, label='b')\nb = a + a; b.label = 'b'\nb.backward()\ndraw_dot(b, label)\n\n\n\n\n\na = Value(-2.0, label='a')\nb = Value(3.0, label='b')\nd = a*b; d.label = 'd'\ne = a+b; e.label = 'e'\nf = d*e; f.label = 'f'\nf.backward()\ndraw_dot(f, label)\n\n\n\n\n\n\n\nAdd and multiply Value object with constant\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n            other.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n    \n    def __radd__(self, other): # other + self\n        return self + other\n        \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad # &lt;- Accumulate the gradient\n            other.grad += self.data * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n    \n    def __rmul__(self, other): # other * self\n        return self * other\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        \n        def _backward():\n            self.grad += (1 - t**2) * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\na = Value(2.0); a + 1\n\nValue(label= data=3.0)\n\n\n\n\na = Value(2.0); a * 1\n\nValue(label= data=2.0)\n\n\n\n\n2 * a\n\nValue(label= data=4.0)\n\n\n\n\n2 + a\n\nValue(label= data=4.0)\n\n\n\n\n\nImplement tanh\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # ⭠ Add grad\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(label={self.label} data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n            other.grad += 1.0 * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n    \n    def __radd__(self, other): # other + self\n        return self + other\n        \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad # &lt;- Accumulate the gradient\n            other.grad += self.data * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n    \n    def __rmul__(self, other): # other * self\n        return self * other\n    \n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        \n        def _backward():\n            self.grad += other * (self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        \n        return out\n        \n    \n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self,), 'exp')\n        \n        def _backward():\n            self.grad += out.data * out.grad\n        out._backward = _backward\n        return out\n    \n    def __truediv__(self, other): # self / other\n        return self * other**-1\n    \n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n        out = Value(t, (self,), 'tanh')\n        \n        def _backward():\n            self.grad += (1 - t**2) * out.grad # &lt;- Accumulate the gradient\n        out._backward = _backward\n        return out\n\n    def __neg__(self): #-self\n        return -self\n    \n    def __sub__(self, other): # self - other\n        return self + (-other)\n        \n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\na = Value(2.0)\n\n\na.exp()\n\nValue(label= data=7.38905609893065)\n\n\n\n\nb = Value(3.0)\n\n\na/b\n\nValue(label= data=0.6666666666666666)\n\n\n\n\na **4\n\nValue(label= data=16.0)\n\n\n\n\na - 1\n\nValue(label= data=1.0)\n\n\n\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label='x1*w1'\nx2w2 = x2*w2; x2w2.label='x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label='n'\n# -----\ne = (2*n).exp()\no = (e - 1)/(e + 1)\n# -----\no.label = 'o'\no.backward()\ndraw_dot(o, label)"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#x1w1-x2w2-b-with-pytorch",
    "href": "lecture_notes/building_micrograd.html#x1w1-x2w2-b-with-pytorch",
    "title": "Mircrograd from scratch",
    "section": "x1w1 + x2w2 + b with PyTorch",
    "text": "x1w1 + x2w2 + b with PyTorch\n\nimport torch\n\n\nx1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]); b.requires_grad = True\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\nprint(o.data.item())\no.backward()\n\nprint('-----')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n\n0.7071066904050358\n\n\n\n-----\n\n\n\nx2 0.5000001283844369\n\n\n\nw2 0.0\n\n\n\nx1 -1.5000003851533106\n\n\n\nw1 1.0000002567688737\n\n\n\n\ntorch.Tensor([[1, 2, 3], [4, 5, 6]])\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#neural-network-1",
    "href": "lecture_notes/building_micrograd.html#neural-network-1",
    "title": "Mircrograd from scratch",
    "section": "Neural Network",
    "text": "Neural Network\n\nimport random\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n    \n    def __call__(self, x):\n        act =  sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        return act.tanh()\n\n    def parameters(self):\n        return self.w + [self.b]\n    \nx = [2.0, 3.0]\nn = Neuron(2)\nn(x)\n\nValue(label= data=-0.71630401051218)\n\n\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin\n                              ) for _ in range(nout)]\n    \n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n    \n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n\n\nx = [2.0, 3.0]\nn = Layer(2, 3)\nn(x)\n\n[\n    Value(label= data=0.9323923071860208),\n    Value(label= data=-0.6957480842688355),\n    Value(label= data=0.9949508713128399)\n]\n\n\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i\n                       in range(len(nouts))]\n        \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n    \n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\no = n(x)\no.grad = 1\no.backward()\n\n\nn.parameters(), len(n.parameters())\n\n(\n    [\n        Value(label= data=-0.8773320545613115),\n        Value(label= data=0.21854271535158198),\n        Value(label= data=0.13730892829595565),\n        Value(label= data=-0.5436703421639371),\n        Value(label= data=-0.5007041170945776),\n        Value(label= data=0.9789830631658898),\n        Value(label= data=0.8050974151663517),\n        Value(label= data=-0.11016135996456167),\n        Value(label= data=0.22124094253907778),\n        Value(label= data=-0.8692488975746844),\n        Value(label= data=-0.51512083826767),\n        Value(label= data=-0.15884614255298235),\n        Value(label= data=-0.9216734804692623),\n        Value(label= data=0.6165197184222242),\n        Value(label= data=0.33389808347375305),\n        Value(label= data=0.6716163019747723),\n        Value(label= data=0.7479127489965471),\n        Value(label= data=0.6913996844396202),\n        Value(label= data=-0.3719520946883914),\n        Value(label= data=0.0381466491267759),\n        Value(label= data=-0.8036261340897828),\n        Value(label= data=0.14331062776761772),\n        Value(label= data=-0.9904951973594573),\n        Value(label= data=0.23265417282124412),\n        Value(label= data=-0.5441204768729622),\n        Value(label= data=0.09037344168895323),\n        Value(label= data=-0.6263186959547287),\n        Value(label= data=-0.7687145874568115),\n        Value(label= data=0.8067183837857432),\n        Value(label= data=-0.6695236110998573),\n        Value(label= data=-0.4936725683149976),\n        Value(label= data=-0.948783805686829),\n        Value(label= data=-0.362064305878842),\n        Value(label= data=0.71706547232376),\n        Value(label= data=-0.38398098767491895),\n        Value(label= data=-0.854407056637168),\n        Value(label= data=-0.43771644655834585),\n        Value(label= data=-0.8122254391243122),\n        Value(label= data=-0.7849921896499341),\n        Value(label= data=0.7867428242639574),\n        Value(label= data=0.9508849142793219)\n    ],\n    41\n)\n\n\n\n\ndraw_dot(o, label)"
  },
  {
    "objectID": "lecture_notes/building_micrograd.html#tiny-dataset-with-loss-function",
    "href": "lecture_notes/building_micrograd.html#tiny-dataset-with-loss-function",
    "title": "Mircrograd from scratch",
    "section": "Tiny Dataset with loss function",
    "text": "Tiny Dataset with loss function\n\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0], \n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0]\n\n\nypred = [n(x) for x in xs]\nloss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\nloss\n\nValue(label= data=6.145450264034548)\n\n\n\n\nypred\n\n[\n    Value(label= data=0.9613622740038076),\n    Value(label= data=0.5091506010451603),\n    Value(label= data=0.9497390552324829),\n    Value(label= data=0.7451677610062515)\n]\n\n\n\n\nRepeat\n\nloss.backward()\n\n\nn.layers[0].neurons[0].w[0].grad\n\n-0.04473465638916681\n\n\n\n\nn.layers[0].neurons[0].w[0].data\n\n-0.8773320545613115\n\n\n\n\nfor p in n.parameters():\n    p.data += -0.01 * p.grad\n\n\nn.layers[0].neurons[0].w[0].data\n\n-0.8768847079974198\n\n\n\n\nypred = [n(x) for x in xs]\nloss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\nloss\n\nValue(label= data=5.9448254035134465)\n\n\n\n\nypred\n\n[\n    Value(label= data=0.9617265322081271),\n    Value(label= data=0.43565539138090165),\n    Value(label= data=0.9510677256835314),\n    Value(label= data=0.725065694807515)\n]\n\n\n\n\nn.parameters()\n\n[\n    Value(label= data=-0.8768847079974198),\n    Value(label= data=0.21949508647796803),\n    Value(label= data=0.1385938942733024),\n    Value(label= data=-0.5427509639370535),\n    Value(label= data=-0.4989987951971535),\n    Value(label= data=0.9810637753982486),\n    Value(label= data=0.8032841357361882),\n    Value(label= data=-0.10822461638644491),\n    Value(label= data=0.2191073445250642),\n    Value(label= data=-0.8693341479297328),\n    Value(label= data=-0.5150553447581354),\n    Value(label= data=-0.1599712708867693),\n    Value(label= data=-0.9204522149327646),\n    Value(label= data=0.6176978033162941),\n    Value(label= data=0.33248584468999454),\n    Value(label= data=0.6728960760418363),\n    Value(label= data=0.7467247012385713),\n    Value(label= data=0.6928044491564865),\n    Value(label= data=-0.3733392595509354),\n    Value(label= data=0.03928302778692913),\n    Value(label= data=-0.8017417247095591),\n    Value(label= data=0.14178985429307983),\n    Value(label= data=-0.9916730215251183),\n    Value(label= data=0.23454886497577554),\n    Value(label= data=-0.5461943160052498),\n    Value(label= data=0.09193046790217876),\n    Value(label= data=-0.6258347045651111),\n    Value(label= data=-0.768590750645844),\n    Value(label= data=0.8076512515956322),\n    Value(label= data=-0.6702462673395139),\n    Value(label= data=-0.49429632745656293),\n    Value(label= data=-0.9460952730831453),\n    Value(label= data=-0.3596651251112354),\n    Value(label= data=0.7141165005167276),\n    Value(label= data=-0.3806245013086102),\n    Value(label= data=-0.8570897419717581),\n    Value(label= data=-0.41643954968132757),\n    Value(label= data=-0.8282591500009012),\n    Value(label= data=-0.8021962015254895),\n    Value(label= data=0.7707916705043153),\n    Value(label= data=0.9262723943108128)\n]\n\n\n\n\n\nMake the above Repeat section into training loop\n\ndef train(repeats, model, xs, ygt, lr = 0.01):\n    \n    for k in range(repeats):\n        # forward pass\n        ypred = [model(x) for x in xs]\n        loss = sum((yout - ygt)**2 for ygt, yout in zip(ygt, ypred))\n        print(k, loss.data)\n        \n        # backward propagation\n        for p in n.parameters(): p.grad = 0.0 # zero_grad()\n        loss.backward()\n        \n        # update: gradient descent\n        for p in model.parameters(): p.data += -lr * p.grad\n    return ypred\n\n\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0], \n    [1.0, 1.0, -1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0]\n\n\nmodel = MLP(3, [4, 4, 1])\n\n\ntrain(10, model, xs, ys, 0.05)\n\n0 7.220472885146723\n\n\n\n1 6.8361918435934355\n\n\n\n2 5.589046560434221\n\n\n\n3 4.147937671605216\n\n\n\n4 4.875203300452224\n\n\n\n5 4.323610795464504\n\n\n\n6 3.546174609092321\n\n\n\n7 1.7412962395817813\n\n\n\n8 0.5351626826196177\n\n\n\n9 0.21335050879201323\n\n\n\n[\n    Value(label= data=0.5781623601547444),\n    Value(label= data=-0.8870687314798208),\n    Value(label= data=-0.8905626911927673),\n    Value(label= data=0.896687278453952)\n]\n\n\n\nDone!"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html",
    "href": "exercises/building_makemore_mlp_exercise.html",
    "title": "Building Makemore MLP Exercise",
    "section": "",
    "text": "from tqdm import tqdm\nimport numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html#imports",
    "href": "exercises/building_makemore_mlp_exercise.html#imports",
    "title": "Building Makemore MLP Exercise",
    "section": "",
    "text": "from tqdm import tqdm\nimport numpy\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html#setup",
    "href": "exercises/building_makemore_mlp_exercise.html#setup",
    "title": "Building Makemore MLP Exercise",
    "section": "Setup",
    "text": "Setup\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\nlen(words)\n\n32033\n\n\n\ndef generate_training_set(words, block_size, print_disabled=False):\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i+1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i:s for s, i in stoi.items()}\n    \n    X, Y = [], []\n    \n    for w in words:\n        if print_disabled: print(w)\n        \n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            if print_disabled: print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n            context = context[1:] + [ix] # crop and append\n            \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y\n\n\nX, Y = generate_training_set(words, 3)\n\n\nX.shape, Y.shape\n\n(torch.Size([228146, 3]), torch.Size([228146]))\n\n\n\ndef generate_train_valid_test_split(words, block_size=3):\n    random.seed(42)\n    random.shuffle(words)\n    n1 = int(0.8*len(words))\n    n2 = int(0.9*len(words))\n\n    Xtr, Ytr = generate_training_set(words[:n1], block_size)\n    Xdev, Ydev = generate_training_set(words[n1:n2], block_size)\n    Xte, Yte = generate_training_set(words[n2:], block_size)\n    \n    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n\n\nXtr, Ytr, Xdev, Ydev, Xte, Yte = generate_train_valid_test_split(words, block_size=3)\n\n\nXtr.shape, Ytr.shape\n\n(torch.Size([182625, 3]), torch.Size([182625]))\n\n\n\nXdev.shape, Ydev.shape\n\n(torch.Size([22655, 3]), torch.Size([22655]))\n\n\n\nXte.shape, Yte.shape\n\n(torch.Size([22866, 3]), torch.Size([22866]))"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html#e01",
    "href": "exercises/building_makemore_mlp_exercise.html#e01",
    "title": "Building Makemore MLP Exercise",
    "section": "E01",
    "text": "E01\nTune the hyperparameters of the training to beat the validation loss of 2.2\n\nno of neurons in the hidden layer\nembedding size\nno of characters\nepochs\nlearning rate; change/decay it over the epochs\nbatch size\n\n\ndef evaluate_loss(parameters, X, Y, block_size=3, embedding_size=10):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n    return loss\n\n\ndef _regularization_loss(parameters, lambdas):\n    C = parameters[0]\n    W1 = parameters[1]\n    W2 = parameters[3]\n    \n    return lambdas[0]*(C**2).mean() + lambdas[1]*(W1**2).mean() + lambdas[2]*(W2**2).mean()\n\n\ndef train(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g)\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.randn(hidden_neuron, generator=g)\n        W2 = torch.randn((hidden_neuron, 27), generator=g)\n        b2 = torch.randn(27, generator=g)\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\n1st try\n\nparameters, loss = train(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                    | 4/100000 [00:00&lt;1:36:08, 17.34it/s] 10%|████████████████████▊                                                                                                                                                                                           | 10003/100000 [08:59&lt;1:19:38, 18.83it/s] 20%|█████████████████████████████████████████▌                                                                                                                                                                      | 20003/100000 [17:47&lt;1:10:41, 18.86it/s] 30%|██████████████████████████████████████████████████████████████▍                                                                                                                                                 | 30004/100000 [26:35&lt;1:02:27, 18.68it/s] 40%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                              | 40005/100000 [51:47&lt;49:39, 20.13it/s] 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                         | 50005/100000 [59:59&lt;40:59, 20.32it/s] 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                   | 60005/100000 [1:08:05&lt;32:22, 20.58it/s] 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 70003/100000 [1:16:09&lt;24:24, 20.49it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 80005/100000 [1:24:15&lt;16:17, 20.45it/s] 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 90003/100000 [1:32:20&lt;08:05, 20.60it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [1:40:25&lt;00:00, 16.60it/s]\n\n\n0 17.771562576293945\n10000 2.3100812435150146\n20000 2.236790418624878\n30000 2.1661746501922607\n40000 2.145174980163574\n50000 2.1430141925811768\n60000 2.1360814571380615\n70000 2.1251132488250732\n80000 2.1180062294006348\n90000 2.1188645362854004\n\n\n\nloss, evaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\n(2.101083755493164, tensor(2.1680, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\n2nd try\n\nparameters, loss = train(Xtr, Ytr, 300_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.01, parameters=parameters, enable_print=False)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300000/300000 [4:07:07&lt;00:00, 20.23it/s]\n\n\n\nloss, evaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\n(2.1126763820648193, tensor(2.1603, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\n3rd try\n\nparameters, loss = train(Xtr, Ytr, 10_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=1, parameters=parameters, enable_print=True, print_at_every_nth_epoch=1000)\n\n  0%|                                                                                                                                                                                                                       | 4/10000 [00:00&lt;08:24, 19.80it/s] 10%|█████████████████████▎                                                                                                                                                                                              | 1003/10000 [00:49&lt;07:26, 20.17it/s] 20%|██████████████████████████████████████████▍                                                                                                                                                                         | 2003/10000 [01:39&lt;06:38, 20.07it/s] 30%|███████████████████████████████████████████████████████████████▋                                                                                                                                                    | 3005/10000 [02:28&lt;05:46, 20.21it/s] 40%|████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                               | 4004/10000 [03:18&lt;04:57, 20.19it/s] 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                          | 5003/10000 [04:07&lt;04:09, 19.99it/s] 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                    | 6003/10000 [04:56&lt;03:16, 20.39it/s] 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 7004/10000 [05:46&lt;02:29, 20.07it/s] 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 8003/10000 [06:35&lt;01:37, 20.42it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 9004/10000 [07:24&lt;00:48, 20.40it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:13&lt;00:00, 20.24it/s]\n\n\n0 2.1231608390808105\n1000 2.1130003929138184\n2000 2.1557743549346924\n3000 2.136502265930176\n4000 2.142028331756592\n5000 2.1329710483551025\n6000 2.1422650814056396\n7000 2.148254632949829\n8000 2.13120698928833\n9000 2.1335060596466064\n\n\n\nloss, evaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\n(2.190765142440796, tensor(2.1794, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\n4th try\n\nparameters, loss = train(Xtr, Ytr, 10_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, parameters=parameters, enable_print=True, print_at_every_nth_epoch=1000)\n\n  0%|                                                                                                                                                                                                                       | 5/10000 [00:00&lt;08:24, 19.82it/s] 10%|█████████████████████▎                                                                                                                                                                                              | 1005/10000 [00:49&lt;07:20, 20.43it/s] 20%|██████████████████████████████████████████▌                                                                                                                                                                         | 2005/10000 [01:38&lt;06:34, 20.29it/s] 30%|███████████████████████████████████████████████████████████████▋                                                                                                                                                    | 3002/10000 [02:27&lt;05:44, 20.32it/s] 40%|████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                               | 4004/10000 [03:17&lt;04:58, 20.06it/s] 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                          | 5003/10000 [04:07&lt;04:03, 20.56it/s] 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                    | 6003/10000 [04:56&lt;03:15, 20.48it/s] 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 7003/10000 [05:45&lt;02:27, 20.34it/s] 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 8003/10000 [06:35&lt;01:39, 20.12it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 9004/10000 [07:24&lt;00:47, 21.10it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:13&lt;00:00, 20.25it/s]\n\n\n0 2.141832113265991\n1000 2.091341495513916\n2000 2.089855909347534\n3000 2.079847574234009\n4000 2.081550121307373\n5000 2.096187114715576\n6000 2.0649683475494385\n7000 2.0917818546295166\n8000 2.0842249393463135\n9000 2.0907206535339355\n\n\n\nloss, evaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\n(2.090895414352417, tensor(2.1472, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\n5th try\n\nparameters, loss = train(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.01, parameters=parameters, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                    | 5/100000 [00:00&lt;1:23:39, 19.92it/s] 10%|████████████████████▊                                                                                                                                                                                           | 10004/100000 [08:12&lt;1:14:28, 20.14it/s] 20%|█████████████████████████████████████████▌                                                                                                                                                                      | 20005/100000 [16:24&lt;1:05:10, 20.45it/s] 30%|███████████████████████████████████████████████████████████████                                                                                                                                                   | 30005/100000 [24:34&lt;57:04, 20.44it/s] 40%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                              | 40002/100000 [32:45&lt;49:26, 20.22it/s] 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                         | 50005/100000 [40:56&lt;41:04, 20.28it/s] 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                    | 60004/100000 [49:07&lt;32:55, 20.25it/s] 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                               | 70005/100000 [57:18&lt;24:55, 20.05it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 80004/100000 [1:05:29&lt;16:18, 20.43it/s] 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 90003/100000 [1:13:40&lt;08:24, 19.82it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [1:21:51&lt;00:00, 20.36it/s]\n\n\n0 2.0824689865112305\n10000 2.0864546298980713\n20000 2.0779430866241455\n30000 2.0869970321655273\n40000 2.0827417373657227\n50000 2.1026248931884766\n60000 2.0927939414978027\n70000 2.0810811519622803\n80000 2.095008611679077\n90000 2.0829107761383057\n\n\n\nloss, evaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\n(2.0964395999908447, tensor(2.1466, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\n\nTest Loss\n\nloss, evaluate_loss(parameters, Xte, Yte, block_size=3, embedding_size=50)\n\n(2.0964395999908447, tensor(2.1446, grad_fn=&lt;NllLossBackward0&gt;))"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html#e02",
    "href": "exercises/building_makemore_mlp_exercise.html#e02",
    "title": "Building Makemore MLP Exercise",
    "section": "E02",
    "text": "E02\n\nWeight Initialization\n\n\nWhat is the loss you’d get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve?\nCan you tune the initialization to get a starting loss that is much more similar to (1)?\n\nAnswer to (1)\nIf the predicted probabilities were uniform then the probabilities would have been 1/27 of each character prediction\nAnd we would have take the log of the probability which would have been\n\ntorch.tensor(1/27).log()\n\ntensor(-3.2958)\n\n\nto the get the loss it would have been\n\n- torch.tensor(1/27).log()\n\ntensor(3.2958)\n\n\nNo we sum up the losses and divide by the count, (n * (3.2958))/n which is equal to 3.2958\nLets see the initial loss when we train the model with current initialization\n\nparameters, loss = train(Xtr, Ytr, 10, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)\n\n 40%|███████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                  | 4/10 [00:00&lt;00:00, 19.51it/s] 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 9/10 [00:00&lt;00:00, 19.94it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00&lt;00:00, 19.88it/s]\n\n\n0 18.27653694152832\n1 17.431493759155273\n2 16.35456085205078\n3 16.05698585510254\n4 15.747321128845215\n5 15.394339561462402\n6 15.205368995666504\n7 14.835010528564453\n8 14.528204917907715\n9 14.28638744354248\n\n\nThe initial loss is 18.98 which is high comparative to 3.2958\nLets see the probabilities of the output\n\nparameters, loss = train(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 19.37it/s]\n\n\n0 18.204519271850586\n\n\n\ndef compute_probs(parameters, X, block_size=3, embedding_size=50):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    return F.softmax(logits, dim=1)\n\n\ncompute_probs(parameters, Xtr)\n\ntensor([[2.9970e-06, 2.3740e-08, 2.1316e-10,  ..., 1.2648e-13, 8.5370e-04,\n         8.7376e-08],\n        [1.7422e-05, 1.1364e-09, 1.3196e-09,  ..., 3.6301e-13, 3.8613e-06,\n         2.4013e-07],\n        [5.8833e-05, 5.7244e-06, 1.0801e-02,  ..., 9.2642e-07, 2.9683e-06,\n         2.4511e-06],\n        ...,\n        [5.7658e-11, 1.4429e-09, 9.7899e-11,  ..., 1.0416e-11, 8.2188e-09,\n         2.7279e-10],\n        [7.0990e-01, 8.7623e-12, 1.6534e-07,  ..., 3.1374e-09, 3.6852e-06,\n         1.1986e-04],\n        [9.9999e-01, 1.0279e-07, 6.2436e-11,  ..., 1.4053e-10, 6.7408e-14,\n         1.2024e-09]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nLets view a single row of probabilities\n\ncompute_probs(parameters, Xtr)[0]\n\ntensor([2.9970e-06, 2.3740e-08, 2.1316e-10, 1.9171e-08, 3.7981e-04, 2.2313e-02,\n        1.3911e-17, 1.0186e-09, 9.7561e-10, 5.6293e-12, 8.8295e-09, 3.4877e-09,\n        1.2439e-08, 7.9825e-14, 7.3846e-04, 1.0648e-11, 5.4885e-08, 3.0407e-13,\n        2.0024e-02, 9.5325e-01, 1.7357e-03, 2.2441e-08, 6.8103e-04, 2.4685e-05,\n        1.2648e-13, 8.5370e-04, 8.7376e-08], grad_fn=&lt;SelectBackward0&gt;)\n\n\nto get a uniform probability, I think we need to have all logits as equal so that we can get probability of each as 1/27\n\nTry 1\nlets try uniform wieght initialization\n\ndef train_v2(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g)\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.rand(hidden_neuron, generator=g)\n        W2 = torch.rand((hidden_neuron, 27), generator=g)  \n        b2 = torch.rand(27, generator=g)\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\nparameters, loss = train_v2(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 20.50it/s]\n\n\n0 7.901321887969971\n\n\nWith uniform weight initialization the intial loss (6.422) obtained is less than of normal weight initialization (17.7)\n\n\nTry 2\nLets initialize the last layers of weights and biases as zero.\n\ndef train_v3(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g)\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.rand(hidden_neuron)\n        W2 = torch.zeros((hidden_neuron, 27))\n        b2 = torch.zeros(27)\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\nparameters, loss = train_v3(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 21.42it/s]\n\n\n0 3.295837163925171\n\n\nThe initial loss is now 3.2958 (which we wanted).\nLets see how well it trains now\n\nparameters, loss = train_v3(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                               | 0/30000 [00:00&lt;?, ?it/s] 33%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                            | 10005/30000 [07:59&lt;15:49, 21.07it/s] 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 20004/30000 [15:58&lt;08:02, 20.72it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [23:57&lt;00:00, 20.86it/s]\n\n\n0 3.295837163925171\n10000 2.8236281871795654\n20000 2.8253743648529053\n\n\n\nloss\n\n2.8211419582366943\n\n\n\n\nTry 3\nAs we can see the losses are not decreasing faster, lets not initialize weight to zero but close to zero and see …\n\ndef train_v4(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g)\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.rand(hidden_neuron)\n        W2 = torch.rand((hidden_neuron, 27)) * 0.01 # close to zero\n        b2 = torch.zeros(27)\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\nparameters, loss = train_v4(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                       | 3/30000 [00:00&lt;23:51, 20.95it/s] 33%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                            | 10005/30000 [08:00&lt;15:48, 21.08it/s] 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 20003/30000 [16:00&lt;08:02, 20.73it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [24:00&lt;00:00, 20.82it/s]\n\n\n0 3.29825496673584\n10000 2.8132688999176025\n20000 2.826235294342041\n\n\n\n\nTry 4\nLets not try to uniformly initiate all the weights but only the last layers and the rest we can keep as normal initialized\n\ndef train_v5(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g)\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.randn(hidden_neuron)\n        W2 = torch.rand((hidden_neuron, 27)) * 0.01 # close to zero\n        b2 = torch.zeros(27)\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\nparameters, loss = train_v5(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                       | 3/30000 [00:00&lt;24:39, 20.28it/s] 33%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                            | 10003/30000 [08:13&lt;16:37, 20.04it/s] 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 20005/30000 [16:28&lt;08:11, 20.33it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [24:42&lt;00:00, 20.24it/s]\n\n\n0 3.2991583347320557\n10000 2.175701379776001\n20000 2.1791296005249023\n\n\nThe losses are reducing now. Lets train for 100_000 and check\n\nparameters, loss = train_v5(Xtr, Ytr, 200_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                    | 4/200000 [00:00&lt;3:01:12, 18.39it/s]  5%|██████████▍                                                                                                                                                                                                     | 10004/200000 [08:33&lt;2:43:37, 19.35it/s] 10%|████████████████████▊                                                                                                                                                                                           | 20002/200000 [16:56&lt;2:28:24, 20.21it/s] 15%|███████████████████████████████▏                                                                                                                                                                                | 30004/200000 [25:11&lt;2:19:51, 20.26it/s] 20%|█████████████████████████████████████████▌                                                                                                                                                                      | 40005/200000 [33:26&lt;2:12:10, 20.17it/s] 25%|████████████████████████████████████████████████████                                                                                                                                                            | 50003/200000 [41:41&lt;2:04:02, 20.15it/s] 30%|██████████████████████████████████████████████████████████████▍                                                                                                                                                 | 60003/200000 [49:56&lt;1:55:31, 20.20it/s] 35%|████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 70003/200000 [58:23&lt;1:49:37, 19.76it/s] 40%|██████████████████████████████████████████████████████████████████████████████████▍                                                                                                                           | 80004/200000 [1:06:54&lt;1:40:25, 19.92it/s] 45%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                 | 90003/200000 [1:15:17&lt;1:30:28, 20.26it/s] 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                      | 100005/200000 [1:23:47&lt;1:22:01, 20.32it/s] 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 110003/200000 [1:32:17&lt;1:16:43, 19.55it/s] 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                  | 120004/200000 [1:40:49&lt;1:06:07, 20.16it/s] 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                       | 130003/200000 [1:49:13&lt;1:00:42, 19.22it/s] 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                              | 140004/200000 [1:57:51&lt;49:56, 20.03it/s] 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 150004/200000 [2:06:14&lt;40:59, 20.32it/s] 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 160004/200000 [2:14:38&lt;32:47, 20.33it/s] 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 170003/200000 [2:22:59&lt;25:09, 19.87it/s] 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 180003/200000 [2:31:25&lt;16:36, 20.06it/s] 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 190005/200000 [2:39:51&lt;08:15, 20.16it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [2:48:07&lt;00:00, 19.83it/s]\n\n\n0 3.292088270187378\n10000 2.184924364089966\n20000 2.168978452682495\n30000 2.129955768585205\n40000 2.1225433349609375\n50000 2.1296749114990234\n60000 2.1202642917633057\n70000 2.131760358810425\n80000 2.1080808639526367\n90000 2.1024396419525146\n100000 2.0863888263702393\n110000 2.0778346061706543\n120000 2.084108591079712\n130000 2.085371255874634\n140000 2.084995985031128\n150000 2.0778989791870117\n160000 2.0879881381988525\n170000 2.0799689292907715\n180000 2.0731143951416016\n190000 2.0831706523895264\n\n\n\nloss\n\n2.0791072845458984\n\n\nThe losses are getting reduced faster!\n\nevaluate_loss(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\ntensor(2.1343, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "exercises/building_makemore_mlp_exercise.html#e03",
    "href": "exercises/building_makemore_mlp_exercise.html#e03",
    "title": "Building Makemore MLP Exercise",
    "section": "E03",
    "text": "E03\nRead the Bengio et al 2003 paper, implement and try any idea from the paper. Did it work?\nIn the paper there is a mention of direct connection from the word features to output.\nLets implement the direct connection from embedding to output and see the results\n\nDirect connection from embedding to output\n\nC = torch.randn((27, 50), generator=g)\n\n\nC[X].shape; C[X].view(-1, 150).shape\n\ntorch.Size([228146, 150])\n\n\n\ndef evaluate_loss_dir_conn(parameters, X, Y, block_size=3, embedding_size=10):\n    C, W1, b1, W2, W3, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2 + C[X].view(-1, block_size * embedding_size) @ W3\n    loss = F.cross_entropy(logits, Y)\n    return loss\n\n\ndef train_dir_conn(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g)\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g)\n        b1 = torch.randn(hidden_neuron)\n        W2 = torch.rand((hidden_neuron, 27)) * 0.01 # close to zero\n        W3 = torch.rand((block_size * embedding_size, 27)) * 0.01 # close to zero\n        b2 = torch.zeros(27)\n        parameters = [C, W1, b1, W2, W3, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, ))\n\n        loss = evaluate_loss_dir_conn(parameters, X[ix], Y[ix], block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()\n\n\nparameters, loss = train_dir_conn(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)\n\n  0%|                                                                                                                                                                                                                    | 2/100000 [00:00&lt;2:09:58, 12.82it/s] 10%|████████████████████▊                                                                                                                                                                                           | 10002/100000 [11:49&lt;1:43:19, 14.52it/s] 20%|█████████████████████████████████████████▌                                                                                                                                                                      | 20002/100000 [23:20&lt;1:35:02, 14.03it/s] 30%|██████████████████████████████████████████████████████████████▍                                                                                                                                                 | 30002/100000 [35:10&lt;1:21:53, 14.24it/s] 40%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                                                            | 40002/100000 [46:47&lt;1:09:35, 14.37it/s] 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                         | 50002/100000 [58:30&lt;57:41, 14.44it/s] 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                   | 60002/100000 [1:10:02&lt;46:07, 14.45it/s] 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 70002/100000 [1:21:52&lt;35:18, 14.16it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 80002/100000 [1:33:29&lt;22:48, 14.61it/s] 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 90002/100000 [1:44:59&lt;11:26, 14.57it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [1:56:30&lt;00:00, 14.31it/s]\n\n\n0 3.29349684715271\n10000 2.151575803756714\n20000 2.122009515762329\n30000 2.1049506664276123\n40000 2.107222318649292\n50000 2.098936080932617\n60000 2.0728397369384766\n70000 2.1058623790740967\n80000 2.0761640071868896\n90000 2.0695760250091553\n\n\n\nloss\n\n2.0880658626556396\n\n\n\nevaluate_loss_dir_conn(parameters, Xdev, Ydev, block_size=3, embedding_size=50)\n\ntensor(2.1274, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nevaluate_loss_dir_conn(parameters, Xte, Yte, block_size=3, embedding_size=50)\n\ntensor(2.1239, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nThe loss decreased by lot with this direct connection and the above method of weight initialization"
  },
  {
    "objectID": "exercises/building_makemore_mlp2.html",
    "href": "exercises/building_makemore_mlp2.html",
    "title": "Building Makemore MLP 2 Exercise",
    "section": "",
    "text": "from tqdm import tqdm\nimport numpy\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "exercises/building_makemore_mlp2.html#imports",
    "href": "exercises/building_makemore_mlp2.html#imports",
    "title": "Building Makemore MLP 2 Exercise",
    "section": "",
    "text": "from tqdm import tqdm\nimport numpy\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math\n\n\ng = torch.Generator().manual_seed(42)"
  },
  {
    "objectID": "exercises/building_makemore_mlp2.html#setup",
    "href": "exercises/building_makemore_mlp2.html#setup",
    "title": "Building Makemore MLP 2 Exercise",
    "section": "Setup",
    "text": "Setup\n\nwords = open('../data/names.txt', 'r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i+1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s, i in stoi.items()}\n\n\nlen(words)\n\n32033\n\n\n\ndef generate_training_set(words, block_size, print_disabled=False):\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i+1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i:s for s, i in stoi.items()}\n    \n    X, Y = [], []\n    \n    for w in words:\n        if print_disabled: print(w)\n        \n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            if print_disabled: print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n            context = context[1:] + [ix] # crop and append\n            \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y\n\n\nX, Y = generate_training_set(words, 3)\n\n\nX.shape, Y.shape\n\n(torch.Size([228146, 3]), torch.Size([228146]))\n\n\n\ndef generate_train_valid_test_split(words, block_size=3):\n    random.seed(42)\n    random.shuffle(words)\n    n1 = int(0.8*len(words))\n    n2 = int(0.9*len(words))\n\n    Xtr, Ytr = generate_training_set(words[:n1], block_size)\n    Xdev, Ydev = generate_training_set(words[n1:n2], block_size)\n    Xte, Yte = generate_training_set(words[n2:], block_size)\n    \n    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n\n\nXtr, Ytr, Xdev, Ydev, Xte, Yte = generate_train_valid_test_split(words, block_size=3)\n\n\nXtr.shape, Ytr.shape\n\n(torch.Size([182625, 3]), torch.Size([182625]))\n\n\n\nXdev.shape, Ydev.shape\n\n(torch.Size([22655, 3]), torch.Size([22655]))\n\n\n\nXte.shape, Yte.shape\n\n(torch.Size([22866, 3]), torch.Size([22866]))"
  },
  {
    "objectID": "exercises/building_makemore_mlp2.html#e01",
    "href": "exercises/building_makemore_mlp2.html#e01",
    "title": "Building Makemore MLP 2 Exercise",
    "section": "E01",
    "text": "E01\n\nI did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn’t train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance.\n\n\nInspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n\n\ndef train(X, Y, epochs, bs = 32, context_length = 3, vocab_size = 27, embedding_size = 10, n_hidden = 100, print_at_every_epoch=10):\n    \n    C = torch.randn((vocab_size, embedding_size), generator=g)\n    \n    layers = [\n        nn.Linear(context_length * embedding_size, n_hidden), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n        nn.Linear(n_hidden, n_hidden), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n        nn.Linear(n_hidden, n_hidden), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n        nn.Linear(n_hidden, n_hidden), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n        nn.Linear(n_hidden, n_hidden), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n        nn.Linear(n_hidden, vocab_size), nn.BatchNorm1d(vocab_size)\n    ]\n    visualisation = [-1] * len(layers) \n\n    def get_activation(layer_num):\n        def hook_fn(m, i, o): visualisation[layer_num] = o \n        return hook_fn\n        \n    for i, layer in enumerate(layers):\n        layer.register_forward_hook(get_activation(i))\n        if isinstance(layer, nn.Linear):\n            nn.init.zeros_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    print('Total number of layers are : ', len(layers))\n    parameters = [C] + [p for layer in layers for p in layer.parameters()]\n    print('Total number of parameters are :', sum(p.nelement() for p in parameters))\n\n    \n    # set parameters to required grad\n    for p in parameters:\n        p.requires_grad = True\n\n    \n        \n    lossi = []\n    ud = []\n    for i in range(epochs):\n        ix = torch.randint(0, len(X), (bs,),  generator=g)\n        \n        Xb, Yb = X[ix], Y[ix]\n        \n        emb = C[Xb]\n        x = emb.view(emb.shape[0], -1)\n        \n        for layer in layers:\n            x = layer(x)\n            x.retain_grad()\n\n\n        loss = F.cross_entropy(x, Yb)\n\n            \n        for p in parameters:\n            p.grad = None\n            \n        loss.backward()\n        \n        \n        \n        \n        lr = 0.1 if i &lt;= 100_000 else 0.01 # lr decay\n        \n        for p in parameters:\n            p.data += -lr * p.grad\n        \n        if print_at_every_epoch: \n            if i % print_at_every_epoch == 0:\n                print(f'Epoch {i} Loss {loss.item()}')\n        \n        lossi.append(loss.item())\n        \n        with torch.no_grad():\n            for p in parameters:\n                grad_update_data_ratio = (lr * p.grad.std() / p.data.std()).log10().item() \n                ud.append(grad_update_data_ratio)\n        \n    return C, layers, lossi, ud, visualisation\n\n\nC, layers, losses, uds, v = train(Xtr, Ytr, 1000, print_at_every_epoch=100)\n\nTotal number of layers are :  17\nTotal number of parameters are : 47551\nEpoch 0 Loss 3.295837163925171\nEpoch 100 Loss 3.0274267196655273\nEpoch 200 Loss 2.801142930984497\nEpoch 300 Loss 2.839231491088867\nEpoch 400 Loss 2.77958083152771\nEpoch 500 Loss 2.7113969326019287\nEpoch 600 Loss 2.7721636295318604\nEpoch 700 Loss 2.801912784576416\nEpoch 800 Loss 2.7530055046081543\nEpoch 900 Loss 3.0189149379730225\n\n\n\nv[14].shape\n\ntorch.Size([32, 100])\n\n\n\nv[14].grad.shape\n\ntorch.Size([32, 100])\n\n\n\ndef evaluate_loss(model, embedding, X, Y):\n    emb = embedding[X]\n    logits = emb.view(emb.shape[0], -1)\n    for layer in model: logits = layer(logits) \n    return F.cross_entropy(logits, Y)\n\n\nevaluate_loss(layers,C, Xtr, Ytr), evaluate_loss(layers,C, Xdev, Ydev)\n\n(tensor(2.8327, grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.8316, grad_fn=&lt;NllLossBackward0&gt;))\n\n\n\nemb = C[Xdev]\nlogits = emb.view(emb.shape[0], -1)\n\n\nThe loss is not good. The network partially trains\n\n\nLets see the activations and gradients\n\ndef visualize_histograms(layers, C, Xdev, otype='activations'):\n    plot.figure(figsize=(20, 4))\n    legends = []\n    emb = C[Xdev]\n    logits = emb.view(emb.shape[0], -1)\n    for i, layer in enumerate(layers[:-1]):\n        if isinstance(layer, nn.Tanh):\n            t = v[i]\n            if otype == 'gradients': t = v[i].grad\n            print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n            hy, hx = torch.histogram(t, density=True)\n            plot.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f'layer {i} ({layer.__class__.__name__})')\n    plot.legend(legends)\n    plot.title('gradient distribution')\n\n\nvisualize_histograms(layers, C, Xdev)\n\nlayer 2 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 5 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 8 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 11 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 14 (      Tanh): mean +0.000000, std 0.000000e+00\n\n\n\n\n\n\nC, layers, losses, uds, v = train(Xtr, Ytr, 1000, print_at_every_epoch=100)\n\nTotal number of layers are :  17\nTotal number of parameters are : 47551\nEpoch 0 Loss 3.295837163925171\nEpoch 100 Loss 3.063408613204956\nEpoch 200 Loss 2.780902862548828\nEpoch 300 Loss 3.1825616359710693\nEpoch 400 Loss 2.9627223014831543\nEpoch 500 Loss 2.983510971069336\nEpoch 600 Loss 2.810227394104004\nEpoch 700 Loss 3.0428051948547363\nEpoch 800 Loss 2.9010205268859863\nEpoch 900 Loss 2.6343629360198975\n\n\n\nvisualize_histograms(layers, C, Xdev, 'gradients')\n\nlayer 2 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 5 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 8 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 11 (      Tanh): mean +0.000000, std 0.000000e+00\nlayer 14 (      Tanh): mean +0.000000, std 0.000000e+00\n\n\n\n\n\n\nOnly the last two layers’s biases are trained i.e. Linear and Batchnorm layer. It is partially training because the last layer’s tanh activations and gradients are zero. Therefore only the last layers biases are changing due to the loss function’s derivative"
  },
  {
    "objectID": "exercises/building_makemore_mlp2.html#e02",
    "href": "exercises/building_makemore_mlp2.html#e02",
    "title": "Building Makemore MLP 2 Exercise",
    "section": "E02",
    "text": "E02\n\nBatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be “folded into” the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then “fold” the batchnorm gamma/beta into the preceeding Linear layer’s W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n\n\ndef train(X, Y, epochs, bs = 32, context_length = 3, vocab_size = 27, embedding_size = 10, n_hidden = 100, print_at_every_epoch=10):\n    \n    C = torch.randn((vocab_size, embedding_size), generator=g)\n    \n    layers = [\n        nn.Linear(context_length * embedding_size, vocab_size), \n        nn.BatchNorm1d(vocab_size), \n        nn.Tanh()\n    ]\n\n    print('Total number of layers are : ', len(layers))\n    parameters = [C] + [p for layer in layers for p in layer.parameters()]\n    print('Total number of parameters are :', sum(p.nelement() for p in parameters))\n\n    \n    # set parameters to required grad\n    for p in parameters:\n        p.requires_grad = True\n\n    \n        \n    lossi = []\n    for i in range(epochs):\n        ix = torch.randint(0, len(X), (bs,),  generator=g)\n        \n        Xb, Yb = X[ix], Y[ix]\n        \n        emb = C[Xb]\n        x = emb.view(emb.shape[0], -1)\n        \n        for layer in layers:\n            x = layer(x)\n\n        loss = F.cross_entropy(x, Yb)\n   \n        for p in parameters:\n            p.grad = None\n            \n        loss.backward()\n        \n        lr = 0.1 if i &lt;= 100_000 else 0.01 # lr decay\n        \n        for p in parameters:\n            p.data += -lr * p.grad\n        \n        if print_at_every_epoch: \n            if i % print_at_every_epoch == 0:\n                print(f'Epoch {i} Loss {loss.item()}')\n        \n        lossi.append(loss.item())\n        \n    return C, layers, lossi\n\n\nC, layers, losses = train(Xtr, Ytr, 1000, print_at_every_epoch=10)\n\nTotal number of layers are :  3\nTotal number of parameters are : 1161\nEpoch 0 Loss 3.587299108505249\nEpoch 10 Loss 3.2293121814727783\nEpoch 20 Loss 3.0814695358276367\nEpoch 30 Loss 3.2448253631591797\nEpoch 40 Loss 3.101447582244873\nEpoch 50 Loss 3.429870367050171\nEpoch 60 Loss 3.221737861633301\nEpoch 70 Loss 3.0934414863586426\nEpoch 80 Loss 3.1333014965057373\nEpoch 90 Loss 3.0430803298950195\nEpoch 100 Loss 3.222014904022217\nEpoch 110 Loss 2.9680709838867188\nEpoch 120 Loss 3.0302188396453857\nEpoch 130 Loss 2.913753032684326\nEpoch 140 Loss 3.072633743286133\nEpoch 150 Loss 2.923525333404541\nEpoch 160 Loss 2.755462646484375\nEpoch 170 Loss 2.9250504970550537\nEpoch 180 Loss 3.050321578979492\nEpoch 190 Loss 2.914196491241455\nEpoch 200 Loss 3.108175277709961\nEpoch 210 Loss 2.7188565731048584\nEpoch 220 Loss 2.8650524616241455\nEpoch 230 Loss 2.879669189453125\nEpoch 240 Loss 2.9316511154174805\nEpoch 250 Loss 3.002821207046509\nEpoch 260 Loss 2.710664987564087\nEpoch 270 Loss 2.9430744647979736\nEpoch 280 Loss 3.0812506675720215\nEpoch 290 Loss 2.9047486782073975\nEpoch 300 Loss 2.9102120399475098\nEpoch 310 Loss 2.798466205596924\nEpoch 320 Loss 2.9486002922058105\nEpoch 330 Loss 2.7822418212890625\nEpoch 340 Loss 2.922684669494629\nEpoch 350 Loss 3.0438175201416016\nEpoch 360 Loss 2.863776206970215\nEpoch 370 Loss 2.712582588195801\nEpoch 380 Loss 2.9411137104034424\nEpoch 390 Loss 2.966948986053467\nEpoch 400 Loss 2.787348985671997\nEpoch 410 Loss 2.6891531944274902\nEpoch 420 Loss 2.789918899536133\nEpoch 430 Loss 2.846391439437866\nEpoch 440 Loss 2.705667018890381\nEpoch 450 Loss 2.8475072383880615\nEpoch 460 Loss 2.6863129138946533\nEpoch 470 Loss 2.7115252017974854\nEpoch 480 Loss 2.7697083950042725\nEpoch 490 Loss 2.8165030479431152\nEpoch 500 Loss 2.652785539627075\nEpoch 510 Loss 2.7844464778900146\nEpoch 520 Loss 2.5727062225341797\nEpoch 530 Loss 2.773362636566162\nEpoch 540 Loss 2.639413356781006\nEpoch 550 Loss 2.937549352645874\nEpoch 560 Loss 2.733950614929199\nEpoch 570 Loss 2.9230353832244873\nEpoch 580 Loss 2.7119667530059814\nEpoch 590 Loss 2.629343032836914\nEpoch 600 Loss 2.8231914043426514\nEpoch 610 Loss 2.896239995956421\nEpoch 620 Loss 2.754451274871826\nEpoch 630 Loss 2.8002326488494873\nEpoch 640 Loss 2.694410800933838\nEpoch 650 Loss 2.56418776512146\nEpoch 660 Loss 2.7440552711486816\nEpoch 670 Loss 2.738240957260132\nEpoch 680 Loss 2.872488260269165\nEpoch 690 Loss 2.8462133407592773\nEpoch 700 Loss 2.6969306468963623\nEpoch 710 Loss 2.8000383377075195\nEpoch 720 Loss 2.663419246673584\nEpoch 730 Loss 2.8632514476776123\nEpoch 740 Loss 2.5700666904449463\nEpoch 750 Loss 2.537867784500122\nEpoch 760 Loss 2.601379156112671\nEpoch 770 Loss 2.5540926456451416\nEpoch 780 Loss 2.5457212924957275\nEpoch 790 Loss 2.7192153930664062\nEpoch 800 Loss 2.7309796810150146\nEpoch 810 Loss 2.7865402698516846\nEpoch 820 Loss 2.7576744556427\nEpoch 830 Loss 2.7156918048858643\nEpoch 840 Loss 2.634010076522827\nEpoch 850 Loss 2.7768185138702393\nEpoch 860 Loss 2.6386663913726807\nEpoch 870 Loss 2.7992215156555176\nEpoch 880 Loss 2.6494650840759277\nEpoch 890 Loss 2.6424853801727295\nEpoch 900 Loss 2.85687255859375\nEpoch 910 Loss 2.843587636947632\nEpoch 920 Loss 2.7913031578063965\nEpoch 930 Loss 2.5635101795196533\nEpoch 940 Loss 2.5651516914367676\nEpoch 950 Loss 2.635497570037842\nEpoch 960 Loss 2.7413291931152344\nEpoch 970 Loss 2.7723095417022705\nEpoch 980 Loss 2.8175578117370605\nEpoch 990 Loss 2.7129404544830322\n\n\n\nlen(layers)\n\n3\n\n\n\ndef compute_logits(embedding, model, X):\n    emb = embedding[X]\n    logits = emb.view(emb.shape[0], -1)\n    for layer in model: logits = layer(logits) \n    return logits\n\n\no1 = compute_logits(C, layers, Xdev)\n\n11.4 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nLets fold the batchnorm into the previous linear layer\n\n\ndef compute_logits_folded_bn(embedding, model, X):\n    emb = embedding[X]\n    logits = emb.view(emb.shape[0], -1)\n    \n    # linear layer weight and bias\n    w0 = model[0].weight.T\n    b0 = model[0].bias\n    \n    # batchnorm weight and bias\n    w1 = model[1].weight.T\n    b1 = model[1].bias\n    \n    # batchnorm running mean and std\n    running_mean = layers[1].running_mean\n    running_std = torch.sqrt(layers[1].running_var)\n    \n    # new weight and bias\n    w2 = (w1 * w0) / running_std\n    b2 = w1 * (b0 - running_mean) / running_std + b1\n    \n    \n    return F.tanh( logits @ w2 + b2 )\n\n\no2 = compute_logits_folded_bn(C, layers, Xdev)\n\n8.37 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\no1\n\ntensor([[-0.8510,  0.9600,  0.6935,  ..., -0.5150, -0.5210,  0.6189],\n        [-0.8332,  0.4823, -0.3241,  ..., -0.4695,  0.7627, -0.2997],\n        [-0.8381,  0.8900, -0.6281,  ..., -0.7169,  0.8849, -0.6847],\n        ...,\n        [ 0.9905,  0.9764, -0.9539,  ..., -0.9095, -0.9541, -0.9398],\n        [ 0.9542,  0.9854, -0.8611,  ..., -0.9231,  0.6759, -0.9285],\n        [ 0.9966, -0.8805, -0.8403,  ..., -0.9066,  0.3652, -0.8431]],\n       grad_fn=&lt;TanhBackward0&gt;)\n\n\n\no2\n\ntensor([[-0.8547,  0.9598,  0.6821,  ..., -0.5114, -0.5483,  0.6069],\n        [-0.8373,  0.4803, -0.3395,  ..., -0.4656,  0.7448, -0.3119],\n        [-0.8420,  0.8895, -0.6376,  ..., -0.7149,  0.8754, -0.6903],\n        ...,\n        [ 0.9903,  0.9763, -0.9550,  ..., -0.9090, -0.9571, -0.9405],\n        [ 0.9530,  0.9853, -0.8646,  ..., -0.9228,  0.6529, -0.9294],\n        [ 0.9965, -0.8813, -0.8444,  ..., -0.9061,  0.3298, -0.8455]],\n       grad_fn=&lt;TanhBackward0&gt;)\n\n\n\ntorch.nn.functional.mse_loss(o1, o2)\n\ntensor(0.0003, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nthey are close, the little difference might be due to epsilon added to the variance"
  },
  {
    "objectID": "exercises/micrograd_from_scratch_exercise.html#section-1-derivatives",
    "href": "exercises/micrograd_from_scratch_exercise.html#section-1-derivatives",
    "title": "Micrograd from scratch exercise",
    "section": "section 1: derivatives",
    "text": "section 1: derivatives\n\n# here is a mathematical expression that takes 3 inputs and produces one output\nfrom math import sin, cos\n\ndef f(a, b, c):\n    return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n\nprint(f(2, 3, 4))\n\n6.336362190988558\n\n\n\\[ \\frac{\\partial f}{\\partial a} = -3a^2 - 0.5a^{-0.5} \\] \\[ \\frac{\\partial f}{\\partial b} = 3\\cos({3b}) + 2.5b^{1.5} \\] \\[ \\frac{\\partial f}{\\partial c} = \\frac{1.0}{c^2} \\]\n\n# write the function df that returns the analytical gradient of f\n# i.e. use your skills from calculus to take the derivative, then implement the formula\n# if you do not calculus then feel free to ask wolframalpha, e.g.:\n# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n\ndef gradf(a, b, c):\n    return [-3*a**2 - 0.5*a**-0.5, # df/da\n            3*cos(3*b) + 2.5*b**1.5, # df/db\n            1.0/c**2] # df/dc\n\n# expected answer is the list of \nans = [-12.353553390593273, 10.25699027111255, 0.0625]\nyours = gradf(2, 3, 4)\nfor dim in range(3):\n    ok = 'OK' if abs(yours[dim] - ans[dim]) &lt; 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n\nOK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\nOK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\nOK for dim 2: expected 0.0625, yours returns 0.0625\n\n\n\n# now estimate the gradient numerically without any calculus, using\n# the approximation we used in the video.\n# you should not call the function df from the last cell\n\n# -----------\nh = 0.000001\nnumerical_grad = [0, 0, 0] # TODO\na, b, c = 2, 3, 4\nnumerical_grad[0] = (f(a + h, b, c) - f(a, b, c))/h\nnumerical_grad[1] = (f(a, b + h, c) - f(a, b, c))/h\nnumerical_grad[2] = (f(a, b, c + h) - f(a, b, c))/h\n# -----------\n\ndivergence = 0\nfor dim in range(3):\n    ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) &lt; 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n    divergence += abs(numerical_grad[dim] - ans[dim])\nprint(divergence)\n\nOK for dim 0: expected -12.353553390593273, yours returns -12.353559348809995\nOK for dim 1: expected 10.25699027111255, yours returns 10.256991666679482\nOK for dim 2: expected 0.0625, yours returns 0.062499984743169534\n7.369040485372125e-06\n\n\n\n# there is an alternative formula that provides a much better numerical \n# approximation to the derivative of a function.\n# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n# implement it. confirm that for the same step size h this version gives a\n# better approximation.\n\n# -----------\nnumerical_grad2 = [0, 0, 0] # TODO\nnumerical_grad2[0] = (f(a + h, b, c) - f(a - h, b, c))/(2*h)\nnumerical_grad2[1] = (f(a, b + h, c) - f(a, b - h, c))/(2*h)\nnumerical_grad2[2] = (f(a, b, c + h) - f(a, b, c - h))/(2*h)\n# -----------\n\ndivergence = 0\nfor dim in range(3):\n    ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) &lt; 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n    divergence += abs(numerical_grad2[dim] - ans[dim])\nprint(divergence)\n\nOK for dim 0: expected -12.353553390593273, yours returns -12.353553391353245\nOK for dim 1: expected 10.25699027111255, yours returns 10.256990273571631\nOK for dim 2: expected 0.0625, yours returns 0.06250000028629188\n3.505345347321054e-09\n\n\nYes, the symmetric derivative gives better approximation because the above computed divergence of numerical_grad2 is smaller than of numerical_grad"
  },
  {
    "objectID": "exercises/micrograd_from_scratch_exercise.html#section-2-support-for-softmax",
    "href": "exercises/micrograd_from_scratch_exercise.html#section-2-support-for-softmax",
    "title": "Micrograd from scratch exercise",
    "section": "section 2: support for softmax",
    "text": "section 2: support for softmax\n\n# Value class starter code, with many functions taken out\nfrom math import exp, log\n\nclass Value:\n  \n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n  \n    def __add__(self, other): # exactly as in the video\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n    \n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n    \n        return out\n    \n    \n    # ------\n    # re-implement all the other functions needed for the exercises below\n    # your code here\n    # TODO\n    # ------\n    def __radd__(self, other): # other + self\n        return self + other\n    \n    def exp(self):\n        out = Value(exp(self.data), (self,), 'exp')\n        \n        def _backward():\n            self.grad += out.data * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def log(self):\n        out = Value(log(self.data), (self,), 'log')\n        \n        def _backward():\n            self.grad += (1.0/self.data) * out.grad\n        out._backward = _backward\n        \n        return out\n\n    def __truediv__(self, other): # self / other\n        out = Value(self.data/other.data, (self, other), 'div')\n        \n        def _backward():\n            self.grad += (1/other.data) * out.grad\n            other.grad += -(self.data/(other.data ** 2)) * out.grad\n        \n        out._backward = _backward\n        \n        return out\n    \n    def __neg__(self): #-self\n        out = Value(self.data * -1, (self,), 'neg')\n        \n        def _backward():\n            self.grad += -1 * out.grad\n        out._backward = _backward\n        \n        return out\n        \n    def backward(self): # exactly as in video  \n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n    \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\n# without referencing our code/video __too__ much, make this cell work\n# you'll have to implement (in some cases re-implemented) a number of functions\n# of the Value object, similar to what we've seen in the video.\n# instead of the squared error loss this implements the negative log likelihood\n# loss, which is very often used in classification.\n\n# this is the softmax function\n# https://en.wikipedia.org/wiki/Softmax_function\ndef softmax(logits):\n    counts = [logit.exp() for logit in logits]\n    denominator = sum(counts)\n    out = [c / denominator for c in counts]\n    return out\n\n# this is the negative log likelihood loss function, pervasive in classification\nlogits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\nprobs = softmax(logits)\nloss = -probs[3].log() # dim 3 acts as the label for this input example\nloss.backward()\n\nans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\nfor dim in range(4):\n    ok = 'OK' if abs(logits[dim].grad - ans[dim]) &lt; 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n\nOK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\nOK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\nOK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\nOK for dim 3: expected -0.8864503806400986, yours returns -0.886450380640099\n\n\n\n# verify the gradient using the torch library\n# torch should give you the exact same gradient\nimport torch\n\nlogits = torch.Tensor([0.0, 3.0, -2.0, 1.0]); logits.requires_grad = True\nprobs = torch.softmax(logits, dim = 0)\nloss = -probs[3].log(); \nloss.backward()\n\nans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\nfor dim in range(4):\n    ok = 'OK' if abs(logits.grad[dim] - ans[dim]) &lt; 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits.grad[dim]}\")\n\n/Users/anubhavmaity/mambaforge/envs/fastai/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nOK for dim 0: expected 0.041772570515350445, yours returns 0.041772566735744476\nOK for dim 1: expected 0.8390245074625319, yours returns 0.8390244841575623\nOK for dim 2: expected 0.005653302662216329, yours returns 0.005653302650898695\nOK for dim 3: expected -0.8864503806400986, yours returns -0.8864504098892212"
  },
  {
    "objectID": "exercises/wavenet_exercise-1.html",
    "href": "exercises/wavenet_exercise-1.html",
    "title": "Wavenet Hyperparameter Tuning",
    "section": "",
    "text": "# !pip install ray[tune]\n\n\n# !pip install optuna\n\n\nimport numpy as np\nimport torch\n\nimport random\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\n\n\ndevice = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda:0\"\n\n\ndevice\n\n'cuda:0'\n\n\n\ntorch.manual_seed(42);\n\n\nrandom.seed(42)\n\n\n!ls\n\nnames.txt  sample_data\n\n\n\nSetup Data Loader\n\nwords = open('names.txt', 'r').read().splitlines()\n\n\nrandom.shuffle(words)\n\n\ndef build_dataset(words, block_size=8):\n    \n    X, Y = [], []\n    \n    random.seed(42)\n    random.shuffle(words)\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i + 1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i: s for s, i in stoi.items()}\n    vocab_size = len(itos)\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n    \n    X = torch.tensor(X).to(device)\n    Y = torch.tensor(Y).to(device)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\n\nXtr.shape\n\ntorch.Size([182625, 8])\n\n\n\n\nCreate Model\n\n# --- Flatten Consecutive ---\nclass FlattenConsecutive(nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.n = n\n    \n    def forward(self, x):\n        B, T, C = x.shape\n        x = x.reshape(B, T//self.n, C*self.n)\n        if x.shape[1] == 1: \n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n\n# -- SwapDim ---\nclass SwapDim(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return torch.transpose(x, 1, 2)\n\n# -- SwapDimBack -- \nclass SwapDimBack(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        return torch.transpose(x, 1, 2)\n\n\nvocab_size = 27\nn_embd = 24\nn_hidden = 128\nmodel = nn.Sequential(\n    nn.Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), nn.Linear(n_embd*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n    FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n   FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False),  nn.BatchNorm1d(n_hidden), nn.Tanh(),\n#     nn.Linear(n_hidden, vocab_size),\n).to(device)\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb, logits.shape\n\ntorch.Size([4, 8])\n\n\n(tensor([[ 0,  0,  0,  0,  0,  0,  0,  1],\n         [ 0,  0,  0,  0,  0,  0,  0,  0],\n         [ 0,  0,  0,  0,  0, 16,  1, 24],\n         [ 0,  0,  0,  0,  0,  1,  4, 18]], device='cuda:0'),\n torch.Size([4, 128]))\n\n\n\ndef build_model(n_embd, # the dimensionality of the character embedding vectors\n                n_hidden, # the number of neurons in the hidden layer of the MLP \n                last_layer_factor = 0.1 # the factor by to reduce the weights of the last layer\n               ):\n    vocab_size = 27\n    model = nn.Sequential(\n    nn.Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), nn.Linear(n_embd*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n    FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n   FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False),  nn.BatchNorm1d(n_hidden), nn.Tanh(),\n      nn.Linear(n_hidden, vocab_size)\n    ).to(device)\n\n\n    # parameter init\n    with torch.no_grad(): model[-1].weight *= last_layer_factor\n\n    parameters = model.parameters()\n    print(\"No of parameters \", sum(p.nelement() for p in parameters))\n    for p in parameters: p.requires_grad = True\n    return model\n\n\nmodel = build_model(24, 128)\n\nNo of parameters  76579\n\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  3, 15, 18, 20],\n        [ 0,  0,  0,  0,  0,  0,  7,  9],\n        [ 0,  0,  0,  0,  0, 12,  1,  3],\n        [ 0,  0,  0, 17, 21,  9, 14,  3]], device='cuda:0')\n\n\n\nlogits.shape\n\ntorch.Size([4, 27])\n\n\n\ndef train(config, checkpoint_dir=None):\n    \n    n_embd = config['n_embd']\n    n_hidden = config['n_hidden']\n    last_layer_factor = config['last_layer_factor']\n    max_steps = config['max_steps'] \n    lr = config['lr']\n    batch_size = config['batch_size']\n    \n    model = build_model(n_embd, n_hidden, last_layer_factor)\n\n    train_loss = F.cross_entropy(model(Xtr), Ytr)\n    print('Initial loss ', train_loss)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    \n    lossi = []\n    \n    for i in range(max_steps):\n        running_loss = 0.0\n        epoch_steps = 0\n        # minibatch construct\n        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n        Xb, Yb = Xtr[ix], Ytr[ix]\n\n        logits = model(Xb)\n        loss = F.cross_entropy(logits, Yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        # track stats\n        if i % 10_000 == 0:\n            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n        lossi.append(loss.log10().item())\n    \n        \n    return model\n\n\nconfig = {\n        \"n_embd\": 24,\n        \"n_hidden\": 128,\n        \"lr\": 0.001,\n        \"last_layer_factor\": 0.1,\n        \"batch_size\": 32,\n        \"max_steps\": 200_000\n    }\n\n\nm = train(config)\n\nNo of parameters  76579\nInitial loss  tensor(3.2798, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n      0/ 200000: 3.2850\n  10000/ 200000: 1.9604\n  20000/ 200000: 1.9096\n  30000/ 200000: 2.1808\n  40000/ 200000: 1.9603\n  50000/ 200000: 2.0830\n  60000/ 200000: 1.9285\n  70000/ 200000: 1.8355\n  80000/ 200000: 2.1152\n  90000/ 200000: 1.7333\n 100000/ 200000: 2.5383\n 110000/ 200000: 2.5408\n 120000/ 200000: 1.7806\n 130000/ 200000: 1.5074\n 140000/ 200000: 2.2836\n 150000/ 200000: 2.1666\n 160000/ 200000: 2.0499\n 170000/ 200000: 2.4158\n 180000/ 200000: 1.8051\n 190000/ 200000: 1.6264\n\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = m(Xb)\nlogits\n\ntensor([[-0.3831,  5.9946, -0.4334, -3.1496, -1.0150,  3.1747, -3.4749, -3.2213,\n          0.9639,  2.5345, -2.5072, -4.6121, -2.3915, -1.1776, -1.0199,  4.6581,\n         -2.2405, -5.6197,  1.3689, -1.8678, -2.7871,  1.0448, -3.2367, -4.8778,\n         -4.8898,  1.1356, -0.6535],\n        [-1.8927, -1.4403, -2.6883,  2.1526,  0.8893, -1.0265, -1.4148,  0.8529,\n          1.6819, -2.4304,  1.2136,  3.3855,  1.9102,  1.1939,  1.9362, -3.9292,\n         -0.9157, -0.2079,  1.1477,  1.5550,  0.8729, -2.7374,  0.1523,  0.2212,\n         -0.3517, -0.8675, -0.6843],\n        [-1.8927, -1.4403, -2.6883,  2.1526,  0.8893, -1.0265, -1.4148,  0.8529,\n          1.6819, -2.4304,  1.2136,  3.3855,  1.9102,  1.1939,  1.9362, -3.9292,\n         -0.9157, -0.2079,  1.1477,  1.5550,  0.8729, -2.7374,  0.1523,  0.2212,\n         -0.3517, -0.8675, -0.6843],\n        [ 4.3652,  1.3847,  0.9043, -2.2867,  0.5967,  2.3756, -3.3935, -2.3518,\n         -3.2263,  2.7976, -4.1615, -2.9255,  0.9531, -1.9894, -1.8476,  0.9243,\n         -3.5244, -4.1242, -1.3296,  1.9078,  1.0783, -0.3561, -1.0896, -1.9243,\n         -2.5875,  0.5664, -3.5556]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nwith torch.no_grad():\n    train_loss = F.cross_entropy(m(Xtr), Ytr).item() \n    val_loss = F.cross_entropy(m(Xdev), Ydev).item()\n    print(train_loss, val_loss)\n\n1.86216139793396 2.0197317600250244"
  },
  {
    "objectID": "exercises/building_makemore_execise.html",
    "href": "exercises/building_makemore_execise.html",
    "title": "Building makemore exercise",
    "section": "",
    "text": "from collections import defaultdict, Counter\nimport numpy\nimport torch\nfrom matplotlib import pyplot as plt\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split\nimport torch.nn.functional as F\n\n\ng = torch.Generator().manual_seed(2147483647)"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#imports",
    "href": "exercises/building_makemore_execise.html#imports",
    "title": "Building makemore exercise",
    "section": "",
    "text": "from collections import defaultdict, Counter\nimport numpy\nimport torch\nfrom matplotlib import pyplot as plt\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split\nimport torch.nn.functional as F\n\n\ng = torch.Generator().manual_seed(2147483647)"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e01",
    "href": "exercises/building_makemore_execise.html#e01",
    "title": "Building makemore exercise",
    "section": "E01",
    "text": "E01\n\nTrain a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n\n\nCounting\nRead in the data\n\nwith open('../data/names.txt') as f:\n    words = list(map(lambda x: x.strip(), f.readlines()))\n\n\nwords[:10], len(words)\n\n(['emma',\n  'olivia',\n  'ava',\n  'isabella',\n  'sophia',\n  'charlotte',\n  'mia',\n  'amelia',\n  'harper',\n  'evelyn'],\n 32033)\n\n\n\ndef generate_tripling(words):\n    for w in words:\n        chs = ['.'] + list(w) + ['.']\n        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n            yield ch1, ch2, ch3\n\n\nalphabets = '.abcdefghijklmnopqrstuvwxyz'\nstoi = {char: alphabets.index(char) for char in alphabets}\nitos = dict(map(reversed, stoi.items()))\n\n\nfor ch1, ch2, ch3 in generate_tripling(words[:3]): print(ch1, ch2, ch3)\n\n. e m\ne m m\nm m a\nm a .\n. o l\no l i\nl i v\ni v i\nv i a\ni a .\n. a v\na v a\nv a .\n\n\n\nsum(1 for ch1, ch2, ch3 in generate_tripling(words))\n\n196113\n\n\n\ndef generate_tripling_counter(words):\n    tripling_counter = Counter()\n    for ch1, ch2, ch3 in generate_tripling(words):\n        tripling_counter[(ch1, ch2, ch3)] += 1\n    return tripling_counter\n\n\ntripling_counter = generate_tripling_counter(words)\ntripling_counter.most_common(10)\n\n[(('a', 'h', '.'), 1714),\n (('n', 'a', '.'), 1673),\n (('a', 'n', '.'), 1509),\n (('o', 'n', '.'), 1503),\n (('.', 'm', 'a'), 1453),\n (('.', 'j', 'a'), 1255),\n (('.', 'k', 'a'), 1254),\n (('e', 'n', '.'), 1217),\n (('l', 'y', 'n'), 976),\n (('y', 'n', '.'), 953)]\n\n\n\nlen(tripling_counter)\n\n6037\n\n\n\ndef create_matrix():\n    N = torch.zeros((27, 27, 27), dtype=torch.int32)\n    for ch1, ch2, ch3 in generate_tripling(words):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        ix3 = stoi[ch3]\n        N[ix1, ix2, ix3] += 1\n    return N\n\n\nN = create_matrix(); N.shape\n\ntorch.Size([27, 27, 27])\n\n\n\nN[1, 8, 0]\n\ntensor(1714, dtype=torch.int32)\n\n\n\nP = (N+1).float()\nP = P/P.sum(-1, keepdims=True)\n\n\ndef generate_tripling_prob(words):\n    for ch1, ch2, ch3 in generate_tripling(words):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        ix3 = stoi[ch3]\n        prob = P[ix1, ix2, ix3]\n        yield ch1, ch2, ch3, prob\n\n\nfor ch1, ch2, ch3, prob in generate_tripling_prob(words[:3]): \n    print(ch1, ch2, ch3, prob)\n\n. e m tensor(0.1855)\ne m m tensor(0.1269)\nm m a tensor(0.3744)\nm a . tensor(0.0669)\n. o l tensor(0.2494)\no l i tensor(0.1084)\nl i v tensor(0.0219)\ni v i tensor(0.2669)\nv i a tensor(0.1578)\ni a . tensor(0.3657)\n. a v tensor(0.0550)\na v a tensor(0.1882)\nv a . tensor(0.1405)\n\n\n\ndef generate_names(count, P):\n    for i in range(count):\n        out = []\n        ix1, ix2 = 0, 0\n        while True:\n            p = P[ix1, ix2]\n            ix1 = ix2\n            ix2 = torch.multinomial(p, num_samples = 1, replacement = True, generator=g).item()\n            out.append(itos[ix2])\n            if ix2 == 0:\n                break\n        yield ''.join(out)\n\n\nfor name in generate_names(5, P): print(name)\n\nquia.\nyu.\nquinslyntien.\nnolliahi.\nha.\n\n\n\ndef log_likelihood(words):\n    sum_log = 0\n    count = 0\n    for ch1, ch2, ch3, prob in generate_tripling_prob(words):\n        sum_log += torch.log(prob)\n        count += 1\n    return sum_log/count\n\n\nlen(P)\n\n27\n\n\n\nlog_likelihood(words)\n\ntensor(-2.0927)\n\n\nnegative log likelihood\n\n- log_likelihood(words)\n\ntensor(2.0927)\n\n\n\n\nNN\n\ndef generate_training_set(words):\n    xs1 = []\n    xs2 = []\n    ys = []\n    for ch1, ch2, ch3 in generate_tripling(words):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        ix3 = stoi[ch3]\n        xs1.append(ix1)\n        xs2.append(ix2)\n        ys.append(ix3)\n    xs1 = torch.tensor(xs1)\n    xs2 = torch.tensor(xs2)\n    xs = torch.vstack((xs1, xs2)).permute(1, 0)\n    ys = torch.tensor(ys)\n    return xs, ys\n\n\nSample dataset\n\nxs, ys = generate_training_set(words[:1])\n\n\nxs.shape, ys.shape\n\n(torch.Size([4, 2]), torch.Size([4]))\n\n\n\nxenc = F.one_hot(xs, num_classes=27)\n\n\nxenc.shape\n\ntorch.Size([4, 2, 27])\n\n\n\nxenc\n\ntensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0]],\n\n        [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0]],\n\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0]],\n\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0],\n         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0]]])\n\n\n\nxenc_flattened = xenc.view(4, -1).float()\n\n\nW = torch.randn((27*2, 27), generator=g)\nW\n\ntensor([[ 1.7457,  2.1346, -0.8561,  ..., -1.3257,  0.1467,  0.1691],\n        [-1.5397, -0.7276,  1.1491,  ...,  0.0793,  0.9019,  1.2028],\n        [-0.3794, -0.9875, -0.1380,  ..., -0.5608, -0.2000,  0.2345],\n        ...,\n        [-1.1553,  0.0718,  1.3791,  ...,  1.7574, -0.8094,  1.5089],\n        [-1.3766,  0.0908, -0.4618,  ...,  1.4289, -0.3785, -1.1138],\n        [-0.7575,  0.9740, -0.0826,  ..., -0.0362,  1.4447, -1.0328]])\n\n\n\nlogits = xenc_flattened @ W # log counts\n\n\ncounts = logits.exp()\n\n\nprob = counts/counts.sum(1, keepdims=True)\n\n\nprob.shape\n\ntorch.Size([4, 27])\n\n\n\nys\n\ntensor([13, 13,  1,  0])\n\n\n\nprob[0, 13], prob[1, 13], prob[2, 1], prob[3, 0]\n\n(tensor(0.0138), tensor(0.0141), tensor(0.0025), tensor(0.0234))\n\n\n\n-prob[torch.arange(4), ys].log().mean()\n\ntensor(4.5763)\n\n\n\n\nTrain\n\nxs, ys = generate_training_set(words)\n\n\nxs, ys\n\n(tensor([[ 0,  5],\n         [ 5, 13],\n         [13, 13],\n         ...,\n         [26, 25],\n         [25, 26],\n         [26, 24]]),\n tensor([13, 13,  1,  ..., 26, 24,  0]))\n\n\n\nxenc = F.one_hot(xs, num_classes=27)\nxenc_flattened = xenc.view(len(xenc), -1).float()\n\n\nxenc_flattened.dtype\n\ntorch.float32\n\n\n\nxenc_flattened.shape, ys.shape\n\n(torch.Size([196113, 54]), torch.Size([196113]))\n\n\n\ndef train(X, y, epochs, lr):\n    num = X.shape[0]\n    print(num)\n    W = torch.randn((54, 27), requires_grad=True, generator=g)\n    for i in range(epochs):\n        logits = X @ W\n        counts = logits.exp()\n        prob = counts/counts.sum(1, keepdims=True)\n        loss = -prob[torch.arange(num), y].log().mean()\n        print(f'Epoch {i} Loss {loss}')\n        \n        W.grad = None\n        loss.backward()\n        W.data += -lr * W.grad \n    return W\n\n\nxenc_flattened.shape[0]\n\n196113\n\n\n\nmodel = train(xenc_flattened, ys, 100, 50)\n\n196113\nEpoch 0 Loss 4.304508209228516\nEpoch 1 Loss 3.5239951610565186\nEpoch 2 Loss 3.1618173122406006\nEpoch 3 Loss 2.9341208934783936\nEpoch 4 Loss 2.788262128829956\nEpoch 5 Loss 2.6947758197784424\nEpoch 6 Loss 2.6315062046051025\nEpoch 7 Loss 2.5849287509918213\nEpoch 8 Loss 2.5485451221466064\nEpoch 9 Loss 2.519071578979492\nEpoch 10 Loss 2.4946088790893555\nEpoch 11 Loss 2.4739458560943604\nEpoch 12 Loss 2.4562535285949707\nEpoch 13 Loss 2.440938949584961\nEpoch 14 Loss 2.427560329437256\nEpoch 15 Loss 2.415781021118164\nEpoch 16 Loss 2.4053385257720947\nEpoch 17 Loss 2.396023988723755\nEpoch 18 Loss 2.3876688480377197\nEpoch 19 Loss 2.380136728286743\nEpoch 20 Loss 2.3733139038085938\nEpoch 21 Loss 2.3671069145202637\nEpoch 22 Loss 2.3614373207092285\nEpoch 23 Loss 2.356238842010498\nEpoch 24 Loss 2.3514552116394043\nEpoch 25 Loss 2.347038984298706\nEpoch 26 Loss 2.3429486751556396\nEpoch 27 Loss 2.339149236679077\nEpoch 28 Loss 2.3356103897094727\nEpoch 29 Loss 2.3323051929473877\nEpoch 30 Loss 2.3292105197906494\nEpoch 31 Loss 2.3263063430786133\nEpoch 32 Loss 2.3235747814178467\nEpoch 33 Loss 2.321000576019287\nEpoch 34 Loss 2.3185694217681885\nEpoch 35 Loss 2.3162693977355957\nEpoch 36 Loss 2.3140900135040283\nEpoch 37 Loss 2.3120217323303223\nEpoch 38 Loss 2.3100552558898926\nEpoch 39 Loss 2.3081839084625244\nEpoch 40 Loss 2.3064005374908447\nEpoch 41 Loss 2.3046987056732178\nEpoch 42 Loss 2.303072929382324\nEpoch 43 Loss 2.301518201828003\nEpoch 44 Loss 2.30003023147583\nEpoch 45 Loss 2.2986040115356445\nEpoch 46 Loss 2.297236442565918\nEpoch 47 Loss 2.2959237098693848\nEpoch 48 Loss 2.2946624755859375\nEpoch 49 Loss 2.293450355529785\nEpoch 50 Loss 2.292283773422241\nEpoch 51 Loss 2.2911601066589355\nEpoch 52 Loss 2.2900779247283936\nEpoch 53 Loss 2.289034605026245\nEpoch 54 Loss 2.2880282402038574\nEpoch 55 Loss 2.2870562076568604\nEpoch 56 Loss 2.2861175537109375\nEpoch 57 Loss 2.285210371017456\nEpoch 58 Loss 2.2843332290649414\nEpoch 59 Loss 2.28348445892334\nEpoch 60 Loss 2.282663106918335\nEpoch 61 Loss 2.281867504119873\nEpoch 62 Loss 2.2810964584350586\nEpoch 63 Loss 2.2803494930267334\nEpoch 64 Loss 2.2796247005462646\nEpoch 65 Loss 2.278921365737915\nEpoch 66 Loss 2.2782392501831055\nEpoch 67 Loss 2.2775766849517822\nEpoch 68 Loss 2.276932954788208\nEpoch 69 Loss 2.2763075828552246\nEpoch 70 Loss 2.2756996154785156\nEpoch 71 Loss 2.2751083374023438\nEpoch 72 Loss 2.2745330333709717\nEpoch 73 Loss 2.2739734649658203\nEpoch 74 Loss 2.273428440093994\nEpoch 75 Loss 2.272897958755493\nEpoch 76 Loss 2.27238130569458\nEpoch 77 Loss 2.2718777656555176\nEpoch 78 Loss 2.2713873386383057\nEpoch 79 Loss 2.2709085941314697\nEpoch 80 Loss 2.270442247390747\nEpoch 81 Loss 2.269987106323242\nEpoch 82 Loss 2.269543170928955\nEpoch 83 Loss 2.2691097259521484\nEpoch 84 Loss 2.268686532974243\nEpoch 85 Loss 2.2682735919952393\nEpoch 86 Loss 2.2678701877593994\nEpoch 87 Loss 2.2674758434295654\nEpoch 88 Loss 2.2670907974243164\nEpoch 89 Loss 2.266714334487915\nEpoch 90 Loss 2.2663462162017822\nEpoch 91 Loss 2.265986204147339\nEpoch 92 Loss 2.265634298324585\nEpoch 93 Loss 2.2652900218963623\nEpoch 94 Loss 2.2649528980255127\nEpoch 95 Loss 2.2646231651306152\nEpoch 96 Loss 2.2643001079559326\nEpoch 97 Loss 2.263984203338623\nEpoch 98 Loss 2.263674736022949\nEpoch 99 Loss 2.263371467590332\n\n\nHere the loss is less, it is an improve over bigram model\n\nxenc = F.one_hot(torch.tensor([0, 0]), num_classes=27).float()\n\n\nxenc.shape\n\ntorch.Size([2, 27])\n\n\n\n\n\nPrediction\n\ndef generate_words():\n    for i in range(5):\n        out = []\n        ix1, ix2 = 0, 0\n        while True:\n            xenc = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()\n            xenc_flattened = xenc.view(1, -1)\n            logits = xenc_flattened @ model # predict log-counts\n            counts = logits.exp()\n            p = counts/counts.sum(1, keepdims=True)\n            ix1 = ix2\n            ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n            out.append(itos[ix2])\n            if ix2 == 0:\n                break\n        print(''.join(out))\n\n\ngenerate_words()\n\nyanel.\nori.\nyrmynna.\nalanan.\nssonncasmi."
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e02",
    "href": "exercises/building_makemore_execise.html#e02",
    "title": "Building makemore exercise",
    "section": "E02",
    "text": "E02\n\nsplit up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n\n\nxenc_num = xenc_flattened.shape[0]\n\n\nrange(xenc_num)\n\nrange(0, 196113)\n\n\n\ntest_subset, valid_subset, train_subset = random_split(range(xenc_num), [0.1, 0.1, 0.8], \n                          generator=g)\n\n\ntrain_idx = torch.tensor(train_subset)\nvalid_idx = torch.tensor(valid_subset)\ntest_idx = torch.tensor(test_subset)\n\n\ntrain_idx.shape, valid_idx.shape, test_idx.shape\n\n(torch.Size([156890]), torch.Size([19611]), torch.Size([19612]))\n\n\n\nx_train, y_train = xenc_flattened[train_idx], ys[train_idx]\nx_valid, y_valid = xenc_flattened[valid_idx], ys[valid_idx]\nx_test, y_test = xenc_flattened[test_idx], ys[test_idx]\n\n\nmodel_trigram = train(x_train, y_train, 100, 10)\n\n156890\nEpoch 0 Loss 4.504120826721191\nEpoch 1 Loss 4.2687578201293945\nEpoch 2 Loss 4.06620454788208\nEpoch 3 Loss 3.8899307250976562\nEpoch 4 Loss 3.7362024784088135\nEpoch 5 Loss 3.6019790172576904\nEpoch 6 Loss 3.4846436977386475\nEpoch 7 Loss 3.382246255874634\nEpoch 8 Loss 3.293360471725464\nEpoch 9 Loss 3.2166662216186523\nEpoch 10 Loss 3.1506481170654297\nEpoch 11 Loss 3.0936076641082764\nEpoch 12 Loss 3.0438811779022217\nEpoch 13 Loss 3.0000367164611816\nEpoch 14 Loss 2.9609479904174805\nEpoch 15 Loss 2.925764560699463\nEpoch 16 Loss 2.8938498497009277\nEpoch 17 Loss 2.864720106124878\nEpoch 18 Loss 2.838001251220703\nEpoch 19 Loss 2.813394069671631\nEpoch 20 Loss 2.790656566619873\nEpoch 21 Loss 2.7695863246917725\nEpoch 22 Loss 2.7500126361846924\nEpoch 23 Loss 2.73178768157959\nEpoch 24 Loss 2.7147839069366455\nEpoch 25 Loss 2.698887348175049\nEpoch 26 Loss 2.6839985847473145\nEpoch 27 Loss 2.6700284481048584\nEpoch 28 Loss 2.6568961143493652\nEpoch 29 Loss 2.6445319652557373\nEpoch 30 Loss 2.6328697204589844\nEpoch 31 Loss 2.6218528747558594\nEpoch 32 Loss 2.611428737640381\nEpoch 33 Loss 2.601550579071045\nEpoch 34 Loss 2.5921757221221924\nEpoch 35 Loss 2.583266258239746\nEpoch 36 Loss 2.5747880935668945\nEpoch 37 Loss 2.56670880317688\nEpoch 38 Loss 2.559000253677368\nEpoch 39 Loss 2.5516374111175537\nEpoch 40 Loss 2.5445964336395264\nEpoch 41 Loss 2.537855625152588\nEpoch 42 Loss 2.5313961505889893\nEpoch 43 Loss 2.5251998901367188\nEpoch 44 Loss 2.5192503929138184\nEpoch 45 Loss 2.513532876968384\nEpoch 46 Loss 2.508033514022827\nEpoch 47 Loss 2.502739906311035\nEpoch 48 Loss 2.4976401329040527\nEpoch 49 Loss 2.4927237033843994\nEpoch 50 Loss 2.487980604171753\nEpoch 51 Loss 2.4834015369415283\nEpoch 52 Loss 2.478977680206299\nEpoch 53 Loss 2.4747016429901123\nEpoch 54 Loss 2.4705657958984375\nEpoch 55 Loss 2.4665629863739014\nEpoch 56 Loss 2.4626872539520264\nEpoch 57 Loss 2.4589321613311768\nEpoch 58 Loss 2.455292224884033\nEpoch 59 Loss 2.4517619609832764\nEpoch 60 Loss 2.4483370780944824\nEpoch 61 Loss 2.445012331008911\nEpoch 62 Loss 2.4417836666107178\nEpoch 63 Loss 2.4386465549468994\nEpoch 64 Loss 2.4355976581573486\nEpoch 65 Loss 2.4326331615448\nEpoch 66 Loss 2.4297492504119873\nEpoch 67 Loss 2.426943302154541\nEpoch 68 Loss 2.4242119789123535\nEpoch 69 Loss 2.4215521812438965\nEpoch 70 Loss 2.418961524963379\nEpoch 71 Loss 2.4164371490478516\nEpoch 72 Loss 2.4139764308929443\nEpoch 73 Loss 2.411576986312866\nEpoch 74 Loss 2.4092373847961426\nEpoch 75 Loss 2.4069550037384033\nEpoch 76 Loss 2.4047274589538574\nEpoch 77 Loss 2.402553081512451\nEpoch 78 Loss 2.400430202484131\nEpoch 79 Loss 2.3983571529388428\nEpoch 80 Loss 2.396331787109375\nEpoch 81 Loss 2.394352912902832\nEpoch 82 Loss 2.392418622970581\nEpoch 83 Loss 2.3905277252197266\nEpoch 84 Loss 2.388679265975952\nEpoch 85 Loss 2.386870861053467\nEpoch 86 Loss 2.3851022720336914\nEpoch 87 Loss 2.383371353149414\nEpoch 88 Loss 2.3816778659820557\nEpoch 89 Loss 2.380019426345825\nEpoch 90 Loss 2.378396511077881\nEpoch 91 Loss 2.3768067359924316\nEpoch 92 Loss 2.3752498626708984\nEpoch 93 Loss 2.3737244606018066\nEpoch 94 Loss 2.372229814529419\nEpoch 95 Loss 2.370765447616577\nEpoch 96 Loss 2.3693296909332275\nEpoch 97 Loss 2.367922306060791\nEpoch 98 Loss 2.366542339324951\nEpoch 99 Loss 2.3651890754699707\n\n\nLoss on the dev set\n\ndef evaluate_loss(model, x, y):\n    logits = x @ model\n    counts = logits.exp()\n    pred = counts/counts.sum(1, keepdims=True)\n    return - pred[torch.arange(x.shape[0]), y].log().mean().item()\n\n\nevaluate_loss(model_trigram, x_valid, y_valid)\n\n2.3728110790252686\n\n\nLoss on the test set\n\nevaluate_loss(model_trigram, x_test, y_test)\n\n2.3934383392333984\n\n\nThe loss on test and validation dataset are about the same for the trigram model on the training set.\nSame we can for the bigram model that I have computed in nbs/lecture_notes/02_building_makemore.ipynb. The validation and test dataset have about the same losses to the training set.\nThe trigram is better than bigram"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e03",
    "href": "exercises/building_makemore_execise.html#e03",
    "title": "Building makemore exercise",
    "section": "E03",
    "text": "E03\n\nUse the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n\n\ndef train(X, y, epochs, lr, regularization_param, print_at_every_epoch = False, print_at_last=False):\n    num = X.shape[0]\n    W = torch.randn((54, 27), requires_grad=True, generator=g)\n    for i in range(epochs):\n        logits = X @ W\n        counts = logits.exp()\n        prob = counts/counts.sum(1, keepdims=True)\n        loss = -prob[torch.arange(num), y].log().mean()\n        \n        # regularization\n        regularization_loss = regularization_param * (W **2).mean()\n        loss += regularization_loss\n        \n        if print_at_every_epoch: print(f'Epoch {i} Loss {loss}')\n        \n        W.grad = None\n        loss.backward()\n        W.data += -lr * W.grad \n        \n    if print_at_last: print(f'Loss {loss}')\n    return W, loss.item()\n\n\ndef get_reg_param_trend():\n    train_losses = []\n    val_losses = []\n    reg_params = []\n\n    for reg_param in torch.linspace(0, 10, 100):\n        model, train_loss = train(x_train, y_train, 100, 10, reg_param)\n        val_loss = evaluate_loss(model, x_valid, y_valid)\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        reg_params.append(reg_param)\n\n    return train_losses, val_losses, reg_params\n\n\ntrain_losses, val_losses, reg_params = get_reg_param_trend()\n\n\nplt.plot(train_losses)\nplt.plot(val_losses)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('regularization parameter')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n\n\n\n\nAs we increase the regularization strength, we see that both the losses are increasing and the training loss is increasing over validation loss, it means that the model is underfitting. Lets choose the regularization parameter as 0 and evaluate the test set\n\nmodel, train_loss = train(x_train, y_train, 100, 10, 0)\n\n\ntrain_loss\n\n2.3800032138824463\n\n\n\nevaluate_loss(model, x_valid, y_valid)\n\n2.3897788524627686\n\n\n\nevaluate_loss(model, x_test, y_test)\n\n2.407710075378418"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e04",
    "href": "exercises/building_makemore_execise.html#e04",
    "title": "Building makemore exercise",
    "section": "E04",
    "text": "E04\n\nWe saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n\n\nxs.shape\n\ntorch.Size([196113, 2])\n\n\n\nxenc_num\n\n196113\n\n\n\nx_train, y_train = xs[train_idx], ys[train_idx]\nx_valid, y_valid = xs[valid_idx], ys[valid_idx]\nx_test, y_test = xs[test_idx], ys[test_idx]\n\n\ndef train(X, y, epochs, lr, regularization_param, print_at_every_epoch = False, print_at_last=False):\n    assert X.shape[-1] == 2\n    num = X.shape[0]\n    W = torch.randn((54, 27), requires_grad=True, generator=g)\n    for i in range(epochs):\n        \n        #indexing\n        W1 = W[:27]\n        W2 = W[27:]\n        \n        logits = W1[X[:, 0]] + W2[X[:, 1]]\n        \n        counts = logits.exp()\n        prob = counts/counts.sum(1, keepdims=True)\n        loss = -prob[torch.arange(num), y].log().mean()\n        \n        # regularization\n        regularization_loss = regularization_param * (W **2).mean()\n        loss += regularization_loss\n        \n        if print_at_every_epoch: print(f'Epoch {i} Loss {loss}')\n        \n        W.grad = None\n        loss.backward()\n        W.data += -lr * W.grad \n        \n    if print_at_last: print(f'Loss {loss}')\n    return W, loss\n\n\nmodel, train_loss = train(x_train, y_train, 100, 10, 0)\n\n\ntrain_loss\n\ntensor(2.3760, grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndef evaluate_loss(model, x, y):\n    W1 = model[:27]\n    W2 = model[27:]\n    \n    logits = W1[x[:, 0]] + W2[x[:, 1]]\n    \n    counts = logits.exp()\n    prob = counts/counts.sum(1, keepdims=True)\n    return -prob[torch.arange(x.shape[0]), y].log().mean()\n\n\nevaluate_loss(model, x_valid, y_valid)\n\ntensor(2.3836, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nevaluate_loss(model, x_test, y_test)\n\ntensor(2.4045, grad_fn=&lt;NegBackward0&gt;)"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e05",
    "href": "exercises/building_makemore_execise.html#e05",
    "title": "Building makemore exercise",
    "section": "E05",
    "text": "E05\n\nLook up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we’d prefer to use F.cross_entropy instead?\n\n\nx_train, y_train = xenc_flattened[train_idx], ys[train_idx]\nx_valid, y_valid = xenc_flattened[valid_idx], ys[valid_idx]\nx_test, y_test = xenc_flattened[test_idx], ys[test_idx]\n\n\ndef train(X, y, epochs, lr, print_at_every_epoch = False, print_at_last=False):\n    num = X.shape[0]\n    W = torch.randn((54, 27), requires_grad=True, generator=g)\n    for i in range(epochs):\n        logits = X @ W\n        loss = F.cross_entropy(logits, y)\n        \n        if print_at_every_epoch: print(f'Epoch {i} Loss {loss}')\n        \n        W.grad = None\n        loss.backward()\n        W.data += -lr * W.grad \n        \n    if print_at_last: print(f'Loss {loss}')\n    return W, loss\n\n\ndef evaluate_loss(model, x, y):\n    logits = x @ model\n    counts = logits.exp()\n    pred = counts/counts.sum(1, keepdims=True)\n    return - pred[torch.arange(x.shape[0]), y].log().mean().item()\n\n\nmodel, loss = train(x_train, y_train, 100, 10, 0)\n\n\nloss\n\ntensor(2.3776, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nevaluate_loss(model, x_valid, y_valid)\n\n2.387678861618042\n\n\n\nevaluate_loss(model, x_test, y_test)\n\n2.4051289558410645\n\n\nUsing F.cross_entropy, we dont have to compute the counts and probabilities ourselves instead we just have to pass the logits and labels to the function, also we can pass the weight given to each class and label smoothing"
  },
  {
    "objectID": "exercises/building_makemore_execise.html#e06",
    "href": "exercises/building_makemore_execise.html#e06",
    "title": "Building makemore exercise",
    "section": "E06",
    "text": "E06\n\nmeta-exercise! Think of a fun/interesting exercise and complete it.\n\nLets introduce bias, activation function and another layer of weight in the neural network\n\nrelu = torch.nn.ReLU() # activation\n\n\ndef train(X, y, epochs, lr, print_at_every_epoch = False, print_at_last=False):\n    num = X.shape[0]\n    W1 = torch.randn((54, 50), requires_grad=True, generator=g)\n    W2 = torch.randn((50, 27), requires_grad=True, generator=g)\n    b = torch.zeros(50, requires_grad=True)\n    \n    for i in range(epochs):\n        \n        #first layer\n        output = relu(X @ W1 + b)\n        \n        # second layer\n        logits = output @ W2\n        loss = F.cross_entropy(logits, y)\n        \n        if print_at_every_epoch: print(f'Epoch {i} Loss {loss}')\n        \n        W1.grad = None\n        W2.grad = None\n        b.grad = None\n        \n        loss.backward()\n        W1.data += -lr * W1.grad \n        W2.data += -lr * W2.grad \n        b.data += -lr * b.grad\n        \n    if print_at_last: print(f'Loss {loss}')\n    return W1, W2, b, loss\n\n\nparams1, params2, bias,  loss = train(x_train, y_train, 100, 5, print_at_every_epoch = True, print_at_last = True)\n\nEpoch 0 Loss 14.541219711303711\nEpoch 1 Loss 6.2091875076293945\nEpoch 2 Loss 4.395009994506836\nEpoch 3 Loss 3.53914475440979\nEpoch 4 Loss 3.2722010612487793\nEpoch 5 Loss 3.154813766479492\nEpoch 6 Loss 3.0808494091033936\nEpoch 7 Loss 3.0231924057006836\nEpoch 8 Loss 2.9723856449127197\nEpoch 9 Loss 2.929724931716919\nEpoch 10 Loss 2.8861398696899414\nEpoch 11 Loss 2.8448402881622314\nEpoch 12 Loss 2.8097846508026123\nEpoch 13 Loss 2.7789549827575684\nEpoch 14 Loss 2.7526140213012695\nEpoch 15 Loss 2.7295141220092773\nEpoch 16 Loss 2.709247350692749\nEpoch 17 Loss 2.691128730773926\nEpoch 18 Loss 2.6747939586639404\nEpoch 19 Loss 2.6597096920013428\nEpoch 20 Loss 2.6458194255828857\nEpoch 21 Loss 2.6330149173736572\nEpoch 22 Loss 2.621168851852417\nEpoch 23 Loss 2.6100637912750244\nEpoch 24 Loss 2.5997819900512695\nEpoch 25 Loss 2.589761972427368\nEpoch 26 Loss 2.5802347660064697\nEpoch 27 Loss 2.5712060928344727\nEpoch 28 Loss 2.5626959800720215\nEpoch 29 Loss 2.553861379623413\nEpoch 30 Loss 2.545748710632324\nEpoch 31 Loss 2.5381507873535156\nEpoch 32 Loss 2.53104567527771\nEpoch 33 Loss 2.5244433879852295\nEpoch 34 Loss 2.5182859897613525\nEpoch 35 Loss 2.5125017166137695\nEpoch 36 Loss 2.506807804107666\nEpoch 37 Loss 2.501720428466797\nEpoch 38 Loss 2.4966917037963867\nEpoch 39 Loss 2.492325782775879\nEpoch 40 Loss 2.4879019260406494\nEpoch 41 Loss 2.484579086303711\nEpoch 42 Loss 2.480976104736328\nEpoch 43 Loss 2.4792466163635254\nEpoch 44 Loss 2.475902795791626\nEpoch 45 Loss 2.4750518798828125\nEpoch 46 Loss 2.4703636169433594\nEpoch 47 Loss 2.470320463180542\nEpoch 48 Loss 2.4647164344787598\nEpoch 49 Loss 2.4643948078155518\nEpoch 50 Loss 2.458014965057373\nEpoch 51 Loss 2.457747459411621\nEpoch 52 Loss 2.4513306617736816\nEpoch 53 Loss 2.4503233432769775\nEpoch 54 Loss 2.444188117980957\nEpoch 55 Loss 2.442826271057129\nEpoch 56 Loss 2.4379799365997314\nEpoch 57 Loss 2.436905860900879\nEpoch 58 Loss 2.4321563243865967\nEpoch 59 Loss 2.4304089546203613\nEpoch 60 Loss 2.4265809059143066\nEpoch 61 Loss 2.424839973449707\nEpoch 62 Loss 2.421621322631836\nEpoch 63 Loss 2.4200439453125\nEpoch 64 Loss 2.4169461727142334\nEpoch 65 Loss 2.415520429611206\nEpoch 66 Loss 2.412719488143921\nEpoch 67 Loss 2.4114415645599365\nEpoch 68 Loss 2.408984899520874\nEpoch 69 Loss 2.4082932472229004\nEpoch 70 Loss 2.405961275100708\nEpoch 71 Loss 2.4053380489349365\nEpoch 72 Loss 2.403006076812744\nEpoch 73 Loss 2.402623176574707\nEpoch 74 Loss 2.400026559829712\nEpoch 75 Loss 2.399906873703003\nEpoch 76 Loss 2.3976261615753174\nEpoch 77 Loss 2.3984215259552\nEpoch 78 Loss 2.395678997039795\nEpoch 79 Loss 2.396718740463257\nEpoch 80 Loss 2.394043445587158\nEpoch 81 Loss 2.395671844482422\nEpoch 82 Loss 2.3921918869018555\nEpoch 83 Loss 2.392773389816284\nEpoch 84 Loss 2.3891818523406982\nEpoch 85 Loss 2.3900513648986816\nEpoch 86 Loss 2.3863515853881836\nEpoch 87 Loss 2.3863282203674316\nEpoch 88 Loss 2.3832342624664307\nEpoch 89 Loss 2.3832027912139893\nEpoch 90 Loss 2.3800551891326904\nEpoch 91 Loss 2.380153179168701\nEpoch 92 Loss 2.377634048461914\nEpoch 93 Loss 2.3776769638061523\nEpoch 94 Loss 2.375394821166992\nEpoch 95 Loss 2.375596046447754\nEpoch 96 Loss 2.373054265975952\nEpoch 97 Loss 2.3736236095428467\nEpoch 98 Loss 2.3708877563476562\nEpoch 99 Loss 2.3716115951538086\nLoss 2.3716115951538086\n\n\n\ndef evaluate_loss(X, y, W1, W2, b):\n    #first layer\n    output = relu(X @ W1 + b)\n        \n    # second layer\n    logits = output @ W2\n    loss = F.cross_entropy(logits, y)\n    return loss\n\n\nevaluate_loss(x_valid, y_valid, params1, params2, bias)\n\ntensor(2.3825, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nevaluate_loss(x_test, y_test, params1, params2, bias)\n\ntensor(2.3918, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "exercises/wavenet_exercise.html",
    "href": "exercises/wavenet_exercise.html",
    "title": "Wavenet Hyperparameter Tuning",
    "section": "",
    "text": "# !pip install ray[tune]\n\n\n# !pip install optuna\n\n\nimport numpy as np\nimport torch\n\nimport random\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport ray\nfrom ray import tune\nfrom ray.tune import CLIReporter, JupyterNotebookReporter\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.tune.schedulers.pb2 import PB2\nfrom ray.air import session\nfrom ray.tune.suggest.optuna import OptunaSearch\nfrom ray.air import Result\n\n\ndevice = \"cpu\"\nif torch.cuda.is_available():\n    device = \"cuda:0\"\n\n\ndevice\n\n'cuda:0'\n\n\n\ntorch.manual_seed(42);\n\n\nrandom.seed(42)\n\n\nSetup Data Loader\n\nwords = open('names.txt', 'r').read().splitlines()\n\n\nrandom.shuffle(words)\n\n\ndef build_dataset(words, block_size=8):\n    \n    X, Y = [], []\n    \n    random.seed(42)\n    random.shuffle(words)\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i + 1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i: s for s, i in stoi.items()}\n    vocab_size = len(itos)\n    \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]\n    \n    X = torch.tensor(X).to(device)\n    Y = torch.tensor(Y).to(device)\n    return X, Y\n\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\n\nXtr.shape\n\ntorch.Size([182625, 8])\n\n\n\n\nCreate Model\n\n# --- Tanh --- layer\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x).to(device)\n        return self.out\n    \n    def parameters(self):\n        return []\n    \n# --- Linear --- layer\nclass Linear:\n    \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = (torch.randn((fan_in, fan_out)) / fan_in ** 0.5).to(device) # note: kaiming init\n        self.bias = torch.zeros(fan_out).to(device) if bias else None #\n    \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n    \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\n# --- Embedding --- layer\nclass Embedding: \n    def __init__(self, num_embeddings, embedding_dim):\n        self.weight = torch.randn((num_embeddings, embedding_dim)).to(device)\n        \n    def __call__(self, IX):\n        self.out = self.weight[IX]\n        return self.out\n    \n    def parameters(self):\n        return [self.weight]\n\n# --- Sequential ----\nclass Sequential:\n    def __init__(self, layers):\n        self.layers = layers\n        \n    def __call__(self, x):\n        B, T, C = x.shape\n        x = x.view(B, T//self.n, C*self.n)\n        if x.shape[1] == 1: \n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n    \n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]\n\n# --- BatchNorm1d ---\nclass BatchNorm1d:\n    \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim).to(device)\n        self.beta = torch.zeros(dim).to(device)\n        # buffers (trained with a running `momentum update`)\n        self.running_mean = torch.zeros(dim).to(device)\n        self.running_var = torch.ones(dim).to(device)\n    \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            if x.ndim == 2: dim = 0\n            elif x.ndim == 3: dim = (0, 1)\n            xmean = x.mean(dim, keepdim=True)\n            xvar = x.var(dim, keepdim=True)\n        else:\n            # print('In batchnorm call')\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n# --- Flatten Consecutive ---\nclass FlattenConsecutive:\n    def __init__(self, n):\n        self.n = n\n    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n    \n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\ndef build_model(n_embd, # the dimensionality of the character embedding vectors\n                n_hidden, # the number of neurons in the hidden layer of the MLP \n                last_layer_factor = 0.1 # the factor by to reduce the weights of the last layer\n               ):\n    vocab_size = 27\n    model = Sequential([\n        Embedding(vocab_size, n_embd),\n        FlattenConsecutive(2), Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n        FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n        FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n        Linear(n_hidden, vocab_size),\n    ])\n\n\n    # parameter init\n    with torch.no_grad(): model.layers[-1].weight *= last_layer_factor\n\n    parameters = model.parameters()\n    print(\"No of parameters \", sum(p.nelement() for p in parameters))\n    for p in parameters: p.requires_grad = True\n    return model\n\n76579\n\n\n\nmodel = build_model(24, 128)\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  6],\n        [ 0,  0,  0,  0,  0,  0,  0, 12],\n        [ 0,  0,  0,  2, 18,  9,  5, 12]])\n\n\n\nlogits\n\ntensor([[ 0.0273, -0.0539,  0.0103,  0.0468,  0.0906, -0.0791,  0.0134, -0.0599,\n         -0.0407, -0.0617,  0.0134, -0.0230, -0.0095,  0.0769,  0.0231,  0.0755,\n         -0.0331, -0.0292,  0.0256, -0.1219,  0.0147, -0.0233,  0.0642, -0.0846,\n         -0.1325,  0.0057,  0.1157],\n        [ 0.0004,  0.0353, -0.0831,  0.0284, -0.0333,  0.0517, -0.0613, -0.0264,\n          0.0924, -0.0422, -0.0844, -0.0041,  0.0009,  0.0050,  0.0096,  0.0046,\n          0.0754, -0.0220, -0.0776,  0.0863, -0.0258,  0.0097,  0.1024,  0.0146,\n          0.1084, -0.0321, -0.0900],\n        [-0.0469,  0.0274,  0.0240,  0.0247, -0.0269, -0.0069,  0.1197, -0.0066,\n         -0.1216,  0.0030, -0.0689, -0.0530,  0.0465, -0.0710, -0.0207, -0.0524,\n         -0.0485,  0.0697,  0.0468,  0.0067, -0.0114,  0.0317, -0.0701,  0.0484,\n          0.1649,  0.0206, -0.0751],\n        [ 0.0238, -0.0383, -0.0129, -0.1187, -0.0069, -0.0030, -0.0763,  0.0728,\n          0.0896,  0.1063,  0.1267,  0.0320, -0.0092, -0.0483, -0.0477, -0.0461,\n          0.0270,  0.0346,  0.0168,  0.0283,  0.0219, -0.0034, -0.1050,  0.0165,\n         -0.1496, -0.0124,  0.0358]], device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nfor i, ix in enumerate(batch(range(10))):\n    print(i, ix)\n\n0 range(0, 1)\n1 range(1, 2)\n2 range(2, 3)\n3 range(3, 4)\n4 range(4, 5)\n5 range(5, 6)\n6 range(6, 7)\n7 range(7, 8)\n8 range(8, 9)\n9 range(9, 10)\n\n\n\ndef batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\n\nfor i, ix in enumerate(batch(range(10))):\n    print(i, ix)\n\n0 range(0, 1)\n1 range(1, 2)\n2 range(2, 3)\n3 range(3, 4)\n4 range(4, 5)\n5 range(5, 6)\n6 range(6, 7)\n7 range(7, 8)\n8 range(8, 9)\n9 range(9, 10)\n\n\n\n# same optimization as last time\ndef train_no_tune(config, checkpoint_dir=None):\n    \n    n_embd = config['n_embd']\n    n_hidden = config['n_hidden']\n    last_layer_factor = config['last_layer_factor']\n    max_steps = config['max_steps'] \n    lr = config['lr']\n    batch_size = config['batch_size']\n    \n    model = build_model(n_embd, n_hidden, last_layer_factor)\n\n    train_loss = F.cross_entropy(model(Xtr), Ytr)\n    print('Initial loss ', train_loss)\n    \n    lossi = []\n    \n    for i in range(max_steps):\n        running_loss = 0.0\n        epoch_steps = 0\n        # minibatch construct\n        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n        Xb, Yb = Xtr[ix], Ytr[ix]\n\n\n        # forward pass\n        logits = model(Xb)\n        loss = F.cross_entropy(logits, Yb)\n\n        # backward pass\n        for p in model.parameters():\n            p.grad = None\n        loss.backward()\n\n        # update: simple SGD\n        # lr = 0.1 if i &lt; 150_000 else 0.01 # step learning rate decay\n        for p in model.parameters(): \n            p.data += -lr * p.grad\n\n        # track stats\n        if i % 10_000 == 0:\n            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n        lossi.append(loss.log10().item())\n    \n#     print(model)\n        \n    return model\n                \n#     print(train_loss.item(), val_loss.item())\n\n\nconfig = {\n        \"n_embd\": 24,\n        \"n_hidden\": 128,\n        \"lr\": 0.1,\n        \"last_layer_factor\": 0.1,\n        \"batch_size\": 32,\n        \"max_steps\": 150_000\n    }\n\n\nm = train_no_tune(config)\n\n\nix = torch.randint(0, Xtr.shape[0], (4,))\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = m(Xb)\nlogits\n\ntensor([[ 7.3465,  3.0281, -0.8438, -0.6315,  0.1254,  2.3173, -5.4998, -3.1930,\n          1.5278,  1.3920, -0.8801,  0.2029,  0.6476,  1.1801,  3.3868, -0.4111,\n         -3.7620, -2.9908,  2.5343,  2.5573, -1.7074,  1.2317, -2.6614, -0.9130,\n         -1.6869, -1.5149, -0.9145],\n        [ 1.1573,  6.1267, -1.6542, -0.7126, -0.0082,  7.1435, -3.1067, -2.7086,\n         -0.5152,  5.9013, -1.5665, -2.7343, -0.8459, -2.8573,  0.9811,  5.7301,\n         -2.6570, -4.4699,  2.2010, -1.5982, -1.9610,  2.9759,  0.5002, -2.5367,\n         -3.4369,  5.2291, -4.5067],\n        [ 7.1585,  2.8464,  0.2289, -0.9455,  1.5724,  0.8947, -1.6819,  8.0458,\n         -3.1600, -1.8369, -0.4268,  0.1184, -0.3447, -0.5721,  2.6477,  0.2432,\n         -2.7675, -2.0082,  1.5645,  1.6701, -0.3294, -2.4148, -3.6991, -3.9939,\n         -2.7473,  0.7778, -1.0990],\n        [-0.6448,  5.7704, -0.8307,  0.5714, -1.6713,  5.6905, -2.8324, -4.6171,\n          1.4003,  7.0470, -1.9388, -1.7307, -1.1796, -0.9947, -1.5239,  2.1725,\n         -1.7857, -2.5253,  1.9881,  1.3517, -0.3367, -1.3917, -1.4497, -2.4642,\n         -1.0517,  4.1390, -1.2129]], device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nfor layer in m.layers: layer.training = False\ntrain_loss = F.cross_entropy(m(Xtr), Ytr).item() \nval_loss = F.cross_entropy(m(Xdev), Ydev).item()\ntrain_loss, val_loss\n\n(1.858979344367981, 2.039809226989746)\n\n\n\n\nTuning\n\ndef train(config):\n    \n    n_embd = config['n_embd']\n    n_hidden = config['n_hidden']\n    last_layer_factor = config['last_layer_factor']\n    max_steps = config['max_steps'] \n    lr = config['lr']\n    batch_size = config['batch_size']\n    \n    model = build_model(n_embd, n_hidden, last_layer_factor)\n\n    train_loss = F.cross_entropy(model(Xtr), Ytr)\n    print('Initial loss ', train_loss)\n    \n    lossi = []\n    \n    # max_steps = 100_00_000\n    \n    for i in range(max_steps):\n        running_loss = 0.0\n        epoch_steps = 0\n        # minibatch construct\n        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n        Xb, Yb = Xtr[ix], Ytr[ix]\n\n\n        # forward pass\n        logits = model(Xb)\n        loss = F.cross_entropy(logits, Yb)\n\n        # backward pass\n        for p in model.parameters():\n            p.grad = None\n        loss.backward()\n\n        # update: simple SGD\n        for p in model.parameters(): \n            p.data += -lr * p.grad\n\n        # track stats\n        # if i % 10_000 == 0: # print every once in a while\n        #     # print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n            \n        for layer in model.layers: layer.training = False\n        train_loss = F.cross_entropy(model(Xtr), Ytr) \n        val_loss = F.cross_entropy(model(Xdev), Ydev)\n        session.report({'training_iteration':i, 'train_loss':train_loss.item(), 'val_loss':val_loss.item()})\n        for layer in model.layers: layer.training = True\n            \n        lossi.append(loss.log10().item())\n    \n    print('Train Loss ', F.cross_entropy(model(Xtr), Ytr).item())\n    print('Val Loss ', F.cross_entropy(model(Xdev), Ydev).item())\n\n      0/ 300000: 3.2818\n  10000/ 300000: 2.3348\n  20000/ 300000: 2.0034\n  30000/ 300000: 1.9822\n  40000/ 300000: 1.5388\n  50000/ 300000: 1.9057\n  60000/ 300000: 1.6040\n  70000/ 300000: 2.0101\n  80000/ 300000: 2.2422\n  90000/ 300000: 1.6573\n 100000/ 300000: 1.7261\n 110000/ 300000: 1.8587\n 120000/ 300000: 1.5202\n 130000/ 300000: 2.0675\n 140000/ 300000: 1.9777\n 150000/ 300000: 2.0351\n 160000/ 300000: 1.6458\n 170000/ 300000: 1.8868\n 180000/ 300000: 1.7516\n 190000/ 300000: 1.6009\n 200000/ 300000: 1.3999\n 210000/ 300000: 1.7495\n 220000/ 300000: 1.6129\n 230000/ 300000: 1.7232\n 240000/ 300000: 2.2450\n 250000/ 300000: 2.2155\n 260000/ 300000: 1.5074\n 270000/ 300000: 1.6431\n 280000/ 300000: 1.7525\n 290000/ 300000: 1.8047\n\n\n\ndef tuning():\n    search_space = {\n        \"n_embd\": tune.randint(4, 65),\n        \"n_hidden\": tune.randint(100, 500),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"last_layer_factor\": tune.uniform(0.1, 1.01),\n        \"batch_size\": tune.choice([16, 32, 64]),\n        \"max_steps\": tune.randint(100_000, 10_00_000)\n    }\n    \n    initial_params = [{\n        \"n_embd\": 24, \n        \"n_hidden\": 128,\n        \"lr\": 0.1,\n        \"last_layer_factor\": 0.1,\n        \"batch_size\": 32,\n        \"max_steps\": 150_000\n    }]\n    \n    searcher = OptunaSearch(points_to_evaluate=initial_params)\n    trainable_with_cpu_gpu = tune.with_resources(train, {\"cpu\": 30, \"gpu\": 1})\n    \n    tuner = tune.Tuner(\n        trainable_with_cpu_gpu,\n        tune_config=tune.TuneConfig(\n            metric=\"val_loss\",\n            mode=\"min\",\n            search_alg=searcher,\n            num_samples=7,\n        ),\n        param_space=search_space,\n    )\n    \n    results = tuner.fit()\n    \n    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\nray.shutdown(); \nray.init()\n\n2023-03-19 17:02:02,578 INFO worker.py:1553 -- Started a local Ray instance.\n\n\n\n    \n        Ray\n        \n            \n                \n            \n        \n        \n\n\n\nPython version:\n3.8.10\n\n\nRay version:\n2.3.0\n\n\n\n\n    \n\n\n\n\ntuning()\n\n/home/ubuntu/.local/lib/python3.8/site-packages/ray/tune/search/optuna/optuna_search.py:683: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n  return ot.distributions.IntUniformDistribution(\n/home/ubuntu/.local/lib/python3.8/site-packages/ray/tune/search/optuna/optuna_search.py:662: FutureWarning: LogUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n  return ot.distributions.LogUniformDistribution(\n/home/ubuntu/.local/lib/python3.8/site-packages/ray/tune/search/optuna/optuna_search.py:671: FutureWarning: UniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n  return ot.distributions.UniformDistribution(\n[I 2023-03-19 17:02:06,605] A new study created in memory with name: optuna\n/home/ubuntu/.local/lib/python3.8/site-packages/optuna/distributions.py:799: FutureWarning: IntUniformDistribution(high=64, low=4, step=1) is deprecated and internally converted to IntDistribution(high=64, log=False, low=4, step=1). See https://github.com/optuna/optuna/issues/2941.\n  warnings.warn(message, FutureWarning)\n/home/ubuntu/.local/lib/python3.8/site-packages/optuna/distributions.py:799: FutureWarning: IntUniformDistribution(high=499, low=100, step=1) is deprecated and internally converted to IntDistribution(high=499, log=False, low=100, step=1). See https://github.com/optuna/optuna/issues/2941.\n  warnings.warn(message, FutureWarning)\n/home/ubuntu/.local/lib/python3.8/site-packages/optuna/distributions.py:799: FutureWarning: LogUniformDistribution(high=0.1, low=0.0001) is deprecated and internally converted to FloatDistribution(high=0.1, log=True, low=0.0001, step=None). See https://github.com/optuna/optuna/issues/2941.\n  warnings.warn(message, FutureWarning)\n/home/ubuntu/.local/lib/python3.8/site-packages/optuna/distributions.py:799: FutureWarning: UniformDistribution(high=1.01, low=0.1) is deprecated and internally converted to FloatDistribution(high=1.01, log=False, low=0.1, step=None). See https://github.com/optuna/optuna/issues/2941.\n  warnings.warn(message, FutureWarning)\n/home/ubuntu/.local/lib/python3.8/site-packages/optuna/distributions.py:799: FutureWarning: IntUniformDistribution(high=999999, low=100000, step=1) is deprecated and internally converted to IntDistribution(high=999999, log=False, low=100000, step=1). See https://github.com/optuna/optuna/issues/2941.\n  warnings.warn(message, FutureWarning)\n2023-03-19 17:02:06,901 WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (14 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n(pid=88428) /home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n(pid=88428)   from pandas.core.computation.check import NUMEXPR_INSTALLED\n\n\n\n  \n    \n      Tune Status\n      \n\n\n\nCurrent time:\n2023-03-19 23:14:45\n\n\nRunning for:\n06:12:38.49\n\n\nMemory:\n9.4/196.6 GiB\n\n\n\n\n\n\n\nSystem Info\nUsing FIFO scheduling algorithm.\nResources requested: 30.0/30 CPUs, 1.0/1 GPUs, 0.0/125.77 GiB heap, 0.0/57.89 GiB objects (0.0/1.0 accelerator_type:A100)\n\n\n\n\n\nTrial Status\n\n\n\nTrial name\nstatus\nloc\nbatch_size\nlast_layer_factor\nlr\nmax_steps\nn_embd\nn_hidden\niter\ntotal time (s)\ntrain_loss\nval_loss\n\n\n\n\ntrain_f6250653\nRUNNING\n10.19.31.32:88428\n64\n0.99565\n0.000439031\n540060\n24\n456\n302615\n19429.5\n1.85993\n2.00036\n\n\ntrain_abc822a6\nPENDING\n\n16\n0.127438\n0.000668113\n762048\n30\n168\n\n\n\n\n\n\ntrain_e8e76444\nTERMINATED\n10.19.31.32:88428\n32\n0.1\n0.1\n150000\n24\n128\n149999\n2925.94\n1.88007\n2.06663\n\n\n\n\n\n  \n\n\n\n\n(train pid=88428) No of parameters  76579\n(train pid=88428) Initial loss  tensor(3.2965, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n(train pid=88428) Train Loss  1.873051643371582\n(train pid=88428) Val Loss  2.0602710247039795\n(train pid=88428) No of parameters  869355\n(train pid=88428) Initial loss  \n(train pid=88428) tensor(3.4301, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n  Trial Progress\n  \n\n\n\nTrial name\ndate\ndone\nepisodes_total\nexperiment_id\nhostname\niterations_since_restore\nnode_ip\npid\ntime_since_restore\ntime_this_iter_s\ntime_total_s\ntimestamp\ntimesteps_since_restore\ntimesteps_total\ntrain_loss\ntraining_iteration\ntrial_id\nval_loss\nwarmup_time\n\n\n\n\ntrain_e8e76444\n2023-03-19_17-50-55\nTrue\n\n352e72ef24b0485790725bb268154829\n129-146-49-170\n150000\n10.19.31.32\n88428\n2925.94\n0.0204446\n2925.94\n1679248255\n0\n\n1.88007\n149999\ne8e76444\n2.06663\n0.00304294\n\n\ntrain_f6250653\n2023-03-19_23-14-47\nFalse\n\n352e72ef24b0485790725bb268154829\n129-146-49-170\n302648\n10.19.31.32\n88428\n19431.6\n0.0662513\n19431.6\n1679267687\n0\n\n1.85784\n302647\nf6250653\n1.99851\n0.00304294\n\n\n\n\n\n\n\n\n\n\nAnalyze the Ray Tune trials\nAll the trials’ information did not show up above therefore doing the analysis in the folowing cells\n\nexperiment_path = f\"ray_results/train_2023-03-19_17-02-06\"\nprint(f\"Loading results from {experiment_path}...\")\n\nLoading results from ray_results/train_2023-03-19_17-02-06...\n\n\n\nrestored_tuner = tune.Tuner.restore(experiment_path)\nresult_grid = restored_tuner.get_results()\n\n/home/ubuntu/.local/lib/python3.8/site-packages/ray/tune/tuner.py:230: UserWarning: Passing in the experiment's `trainable` will be a required argument to `Tuner.restore` starting from version 2.5. Please specify the trainable to avoid this warning.\n  warnings.warn(warning_message)\n2023-03-21 14:13:11,149 INFO experiment_analysis.py:789 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n\n\n\nresults_df = result_grid.get_dataframe()\nresults_df[[\"training_iteration\", \"val_loss\"]]\n\n\n\n\n\n\n\n\ntraining_iteration\nval_loss\n\n\n\n\n0\n870783\n2.044881\n\n\n1\n423444\n2.018260\n\n\n2\n762047\n2.015049\n\n\n3\n149999\n2.066634\n\n\n4\n199144\n2.051399\n\n\n5\n289118\n2.046853\n\n\n6\n540059\n1.989679\n\n\n\n\n\n\n\n\nbest_result: Result = result_grid.get_best_result()\n\n\nbest_result.config\n\n{'n_embd': 24,\n 'n_hidden': 456,\n 'lr': 0.0004390312584437158,\n 'last_layer_factor': 0.9956496873025195,\n 'batch_size': 64,\n 'max_steps': 540060}\n\n\n\nbest_result.checkpoint\n\ntrain 1.7052044868469238\nval 2.2605273723602295\n\n\n\nbest_result.metrics\n\n{'training_iteration': 540059,\n 'train_loss': 1.7585725784301758,\n 'val_loss': 1.9896790981292725,\n 'time_this_iter_s': 0.06922125816345215,\n 'done': True,\n 'timesteps_total': None,\n 'episodes_total': None,\n 'trial_id': 'f6250653',\n 'experiment_id': '352e72ef24b0485790725bb268154829',\n 'date': '2023-03-20_03-45-17',\n 'timestamp': 1679283917,\n 'time_total_s': 35661.531294584274,\n 'pid': 88428,\n 'hostname': '129-146-49-170',\n 'node_ip': '10.19.31.32',\n 'config': {'n_embd': 24,\n  'n_hidden': 456,\n  'lr': 0.0004390312584437158,\n  'last_layer_factor': 0.9956496873025195,\n  'batch_size': 64,\n  'max_steps': 540060},\n 'time_since_restore': 35661.531294584274,\n 'timesteps_since_restore': 0,\n 'iterations_since_restore': 540060,\n 'warmup_time': 0.003042936325073242,\n 'experiment_tag': '2_batch_size=64,last_layer_factor=0.9956,lr=0.0004,max_steps=540060,n_embd=24,n_hidden=456'}\n\n\n\nbest_result.metrics_dataframe.plot(\"training_iteration\", \"val_loss\")\n\n&lt;AxesSubplot:xlabel='training_iteration'&gt;\n\n\n\n\n\n\nax = None\nfor result in result_grid:\n    label = f\"lr={result.config['lr']:.3f}\"\n    if ax is None:\n        ax = result.metrics_dataframe.plot(\"training_iteration\", \"val_loss\", label=label)\n    else:\n        result.metrics_dataframe.plot(\"training_iteration\", \"val_loss\", ax=ax, label=label)\nax.set_title(\"Mean Accuracy vs. Training Iteration for All Trials\")\nax.set_ylabel(\"Mean Test Accuracy\")\n\nText(0, 0.5, 'Mean Test Accuracy')\n\n\n\n\n\n\n\nTrain and Test with Best Config\n\nbest_result.metrics\n\n{'training_iteration': 540059,\n 'train_loss': 1.7585725784301758,\n 'val_loss': 1.9896790981292725,\n 'time_this_iter_s': 0.06922125816345215,\n 'done': True,\n 'timesteps_total': None,\n 'episodes_total': None,\n 'trial_id': 'f6250653',\n 'experiment_id': '352e72ef24b0485790725bb268154829',\n 'date': '2023-03-20_03-45-17',\n 'timestamp': 1679283917,\n 'time_total_s': 35661.531294584274,\n 'pid': 88428,\n 'hostname': '129-146-49-170',\n 'node_ip': '10.19.31.32',\n 'config': {'n_embd': 24,\n  'n_hidden': 456,\n  'lr': 0.0004390312584437158,\n  'last_layer_factor': 0.9956496873025195,\n  'batch_size': 64,\n  'max_steps': 540060},\n 'time_since_restore': 35661.531294584274,\n 'timesteps_since_restore': 0,\n 'iterations_since_restore': 540060,\n 'warmup_time': 0.003042936325073242,\n 'experiment_tag': '2_batch_size=64,last_layer_factor=0.9956,lr=0.0004,max_steps=540060,n_embd=24,n_hidden=456'}\n\n\n\nconfig = {'n_embd': 24,\n  'n_hidden': 456,\n  'lr': 0.0004390312584437158,\n  'last_layer_factor': 0.9956496873025195,\n  'batch_size': 64,\n  'max_steps': 540060}\n\n\nmodel = train_no_tune(config)\n\nNo of parameters  869355\nInitial loss  tensor(3.4630, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)\n      0/ 540060: 3.4067\n  10000/ 540060: 2.4061\n  20000/ 540060: 2.1215\n  30000/ 540060: 2.2546\n  40000/ 540060: 2.2071\n  50000/ 540060: 2.1789\n  60000/ 540060: 2.5076\n  70000/ 540060: 2.0653\n  80000/ 540060: 1.9867\n  90000/ 540060: 1.9995\n 100000/ 540060: 2.2169\n 110000/ 540060: 2.2312\n 120000/ 540060: 2.0663\n 130000/ 540060: 1.8910\n 140000/ 540060: 1.9463\n 150000/ 540060: 2.0409\n 160000/ 540060: 1.8872\n 170000/ 540060: 2.1610\n 180000/ 540060: 1.8664\n 190000/ 540060: 2.0235\n 200000/ 540060: 1.8428\n 210000/ 540060: 1.8524\n 220000/ 540060: 1.9312\n 230000/ 540060: 1.8659\n 240000/ 540060: 1.7783\n 250000/ 540060: 1.8855\n 260000/ 540060: 2.0965\n 270000/ 540060: 1.8449\n 280000/ 540060: 2.2505\n 290000/ 540060: 2.0819\n 300000/ 540060: 1.9650\n 310000/ 540060: 1.8367\n 320000/ 540060: 1.9641\n 330000/ 540060: 2.1167\n 340000/ 540060: 1.8066\n 350000/ 540060: 1.8251\n 360000/ 540060: 1.5952\n 370000/ 540060: 2.0327\n 380000/ 540060: 1.8862\n 390000/ 540060: 1.8184\n 400000/ 540060: 1.8827\n 410000/ 540060: 1.8935\n 420000/ 540060: 2.1733\n 430000/ 540060: 1.8394\n 440000/ 540060: 2.0016\n 450000/ 540060: 2.0970\n 460000/ 540060: 1.6659\n 470000/ 540060: 1.5410\n 480000/ 540060: 1.9260\n 490000/ 540060: 1.6596\n 500000/ 540060: 1.7006\n 510000/ 540060: 1.7158\n 520000/ 540060: 1.8447\n 530000/ 540060: 1.9106\n 540000/ 540060: 1.7005\n\n\n\nfor layer in model.layers: layer.training = False\n\nwith torch.no_grad():\n    train_loss = F.cross_entropy(model(Xtr), Ytr)\n    val_loss = F.cross_entropy(model(Xdev), Ydev)\n    test_loss = F.cross_entropy(model(Xte), Yte)\n    print('Train loss ', train_loss)\n    print('Val loss ', val_loss)\n    print('Test loss ', test_loss)\n\nTrain loss  tensor(1.7622, device='cuda:0')\nVal loss  tensor(1.9855, device='cuda:0')\nTest loss  tensor(1.9815, device='cuda:0')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "",
    "text": "Neural Networks: Zero to Hero is a course on deep learning fundamentals by the renowned AI researcher and educator Andrej Karpathy. This repository contains my personal lecture notes and exercise solutions for the course, which covers a wide range of topics such as neural networks, backpropagation, wavenet, GPT & more.\nYou can find the lecture notes and exercise here: https://anubhavmaity.github.io/NeuralNetworks-Zero-To-Hero/\nIf you want to play with lecture notes and exercise in Google Colab then head over to the Contents Section.\nThe youtube lectures of the course can be found here: https://karpathy.ai/zero-to-hero.html\nTable of Contents"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "Course Overview",
    "text": "Course Overview\nThe “Neural Networks: Zero to Hero” course covers a range of deep learning topics, including:\n- Neural Networks and Backward propogation\n- Language Modeling\n- MultiLayer Perceptron\n- Activations & Gradients\n- Batch normalizations\n- Wavenet\n- GPT\n- And more!"
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "Contents",
    "text": "Contents\nThis repository contains my personal notes and exercise solutions for each lecture in the course. The lecture notes provide a detailed summary of each lecture, including key concepts, equations, diagrams, and examples. The exercise solutions include code and written explanations, helping you to deepen your understanding of the material and apply it in practice.\n\nLecture 1: The spelled-out intro to neural networks and backpropagation: building micrograd\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLecture 2: The spelled-out intro to language modeling: building makemore\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLecture 3: Building makemore Part 2: MLP\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLecture 4: Building makemore Part 3: Activations & Gradients, BatchNorm\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLecture 5: Building makemore Part 4: Becoming a Backprop Ninja\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note & Exercise\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 6: Building makemore Part 5: Building a WaveNet\n\n\n\nStyle\nColab\nGithub\n\n\n\n\nLecture Note\n\n\n\n\n\n\n\n\n\nExercise"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "Usage",
    "text": "Usage\nYou can use this repository to review and reference the lecture notes, as well as to deepen your understanding of the concepts through the exercise solutions. You can also modify and build upon the code to suit your own needs.\nIf you use this repository in your project, please attribute it to me (Anubhav Maity) and include a link to the original repository:\n\nRepository: https://github.com/anubhavmaity/NeuralNetworks-Zero-To-Hero/\nAuthor: Anubhav Maity"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "Contributing",
    "text": "Contributing\nContributions to this repository are welcome! If you notice any errors or have suggestions for improving the content, feel free to submit a pull request or open an issue.\nThe docs are generated from Jupyter Notebooks using nbdev. To make any changes in the doc, you can make the changes in the Jupyter Notebook and the Github Action will generate the docs from the notebook for you."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "NeuralNetworks-Zero-To-Hero",
    "section": "LICENSE",
    "text": "LICENSE\nThis repository is licensed under the Apache License 2.0. See the LICENSE file in the repo for details."
  }
]