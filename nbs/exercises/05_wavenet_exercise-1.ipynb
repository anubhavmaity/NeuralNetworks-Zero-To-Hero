{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf09f86b",
   "metadata": {},
   "source": [
    "# Wavenet Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd6a19-db09-4a78-9f66-66a416f5cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c252421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0815e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1df47-e95f-430f-96c0-12e5c7969860",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed816f9-a512-42d2-a0dc-f5f0ba0636c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34c512",
   "metadata": {},
   "source": [
    "### Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d76291",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size=8):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "    \n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    vocab_size = len(itos)\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X).to(device)\n",
    "    Y = torch.tensor(Y).to(device)\n",
    "    return X, Y\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f4953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182625, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabedf8a-25b9-4f90-bac5-fb44546fb38a",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flatten Consecutive ---\n",
    "class FlattenConsecutive(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.reshape(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1: \n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "# -- SwapDim ---\n",
    "class SwapDim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, 1, 2)\n",
    "\n",
    "# -- SwapDimBack -- \n",
    "class SwapDimBack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 27\n",
    "n_embd = 24\n",
    "n_hidden = 128\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, n_embd),\n",
    "    FlattenConsecutive(2), nn.Linear(n_embd*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n",
    "    FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n",
    "   FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False),  nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "#     nn.Linear(n_hidden, vocab_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[13,  1, 18,  9, 10,  1, 14,  5],\n",
       "         [ 0,  0,  0,  0,  1,  2,  4, 15],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  8,  1, 26,  1]]),\n",
       " torch.Size([4, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "Xb, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f11c1e-d7fb-4adc-bd47-dc79d9a5c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_embd, # the dimensionality of the character embedding vectors\n",
    "                n_hidden, # the number of neurons in the hidden layer of the MLP \n",
    "                last_layer_factor = 0.1 # the factor by to reduce the weights of the last layer\n",
    "               ):\n",
    "    vocab_size = 27\n",
    "    model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, n_embd),\n",
    "    FlattenConsecutive(2), nn.Linear(n_embd*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n",
    "    FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), SwapDim(), nn.BatchNorm1d(n_hidden), SwapDimBack(), nn.Tanh(),\n",
    "   FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False),  nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "      nn.Linear(n_hidden, vocab_size)\n",
    "    )\n",
    "\n",
    "\n",
    "    # parameter init\n",
    "    with torch.no_grad(): model[-1].weight *= last_layer_factor\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    print(\"No of parameters \", sum(p.nelement() for p in parameters))\n",
    "    for p in parameters: p.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters  76579\n"
     ]
    }
   ],
   "source": [
    "model = build_model(24, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0, 19,  8,  1, 25],\n",
       "        [ 0,  0,  0,  0,  0,  4,  5, 22],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584dfc0-ce59-4211-85c9-b53fcba9fce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640cf2c-f407-4ead-aaf8-1ade48438d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None):\n",
    "    \n",
    "    n_embd = config['n_embd']\n",
    "    n_hidden = config['n_hidden']\n",
    "    last_layer_factor = config['last_layer_factor']\n",
    "    max_steps = config['max_steps'] \n",
    "    lr = config['lr']\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    model = build_model(n_embd, n_hidden, last_layer_factor)\n",
    "\n",
    "    train_loss = F.cross_entropy(model(Xtr), Ytr)\n",
    "    print('Initial loss ', train_loss)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    lossi = []\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "        logits = model(Xb)\n",
    "        loss = F.cross_entropy(logits, Yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track stats\n",
    "        if i % 10_000 == 0:\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daceac7a-c216-4680-b45a-3c7cbb58caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"n_embd\": 24,\n",
    "        \"n_hidden\": 128,\n",
    "        \"lr\": 0.001,\n",
    "        \"last_layer_factor\": 0.1,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_steps\": 200_000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828e4ad-47e6-4770-8460-ec9e358e1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters  76579\n",
      "Initial loss  tensor(3.3022, grad_fn=<NllLossBackward0>)\n",
      "      0/ 200000: 3.3107\n",
      "  10000/ 200000: 2.1195\n",
      "  20000/ 200000: 2.1580\n",
      "  30000/ 200000: 2.0142\n",
      "  40000/ 200000: 2.1674\n",
      "  50000/ 200000: 2.5029\n",
      "  60000/ 200000: 1.7215\n",
      "  70000/ 200000: 2.0961\n",
      "  80000/ 200000: 2.1328\n",
      "  90000/ 200000: 2.2157\n",
      " 100000/ 200000: 2.0725\n",
      " 110000/ 200000: 2.1434\n",
      " 120000/ 200000: 1.8127\n",
      " 130000/ 200000: 1.8254\n"
     ]
    }
   ],
   "source": [
    "m = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c67352-0081-47f5-94fa-aaabd02aca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = m(Xb)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b263806-b786-49ba-8253-1d688b06f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in m: layer.training = False\n",
    "with torch.no_grad():\n",
    "    train_loss = F.cross_entropy(m(Xtr), Ytr).item() \n",
    "    val_loss = F.cross_entropy(m(Xdev), Ydev).item()\n",
    "    print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c003f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhnn",
   "language": "python",
   "name": "zhnn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
