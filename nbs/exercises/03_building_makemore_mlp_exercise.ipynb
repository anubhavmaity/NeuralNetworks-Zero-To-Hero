{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Makemore MLP Exercise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plot\nimport random\nimport math"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/29846070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "g = torch.Generator().manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "words = open('/kaggle/input/nameszhnn/names.txt', 'r').read().splitlines()\nwords[:8]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "32033"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "len(words)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_training_set(words, block_size, print_disabled=False):\n    \n    chars = sorted(list(set(''.join(words))))\n    stoi = {s: i+1 for i, s in enumerate(chars)}\n    stoi['.'] = 0\n    itos = {i:s for s, i in stoi.items()}\n    \n    X, Y = [], []\n    \n    for w in words:\n        if print_disabled: print(w)\n        \n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            if print_disabled: print(''.join(itos[i] for i in context), '--->', itos[ix])\n            context = context[1:] + [ix] # crop and append\n            \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X, Y = generate_training_set(words, 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([228146, 3]), torch.Size([228146]))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "X.shape, Y.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_train_valid_test_split(words, block_size=3):\n    random.seed(42)\n    random.shuffle(words)\n    n1 = int(0.8*len(words))\n    n2 = int(0.9*len(words))\n\n    Xtr, Ytr = generate_training_set(words[:n1], block_size)\n    Xdev, Ydev = generate_training_set(words[n1:n2], block_size)\n    Xte, Yte = generate_training_set(words[n2:], block_size)\n    \n    return Xtr, Ytr, Xdev, Ydev, Xte, Yte"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "Xtr, Ytr, Xdev, Ydev, Xte, Yte = generate_train_valid_test_split(words, block_size=3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([182625, 3]), torch.Size([182625]))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "Xtr.shape, Ytr.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([22655, 3]), torch.Size([22655]))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "Xdev.shape, Ydev.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([22866, 3]), torch.Size([22866]))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "Xte.shape, Yte.shape"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## E01\n\nTune the hyperparameters of the training to beat the validation loss of 2.2\n\n   - no of neurons in the hidden layer\n    \n   - embedding size\n    \n   - no of characters\n    \n   - epochs\n    \n   - learning rate; change/decay it over the epochs\n    \n   - batch size"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_loss(parameters, X, Y, block_size=3, embedding_size=10):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits.cuda(), Y)\n    return loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _regularization_loss(parameters, lambdas):\n    C = parameters[0]\n    W1 = parameters[1]\n    W2 = parameters[3]\n    \n    return lambdas[0]*(C**2).mean() + lambdas[1]*(W1**2).mean() + lambdas[2]*(W2**2).mean()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g).cuda()\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.randn(hidden_neuron, generator=g).cuda()\n        W2 = torch.randn((hidden_neuron, 27), generator=g).cuda()\n        b2 = torch.randn(27, generator=g).cuda()\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 1st try"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 12/100000 [00:00<30:45, 54.19it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 19.391998291015625\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 10%|█         | 10010/100000 [03:03<27:34, 54.40it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.2867367267608643\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 20%|██        | 20009/100000 [06:02<23:37, 56.43it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.210310935974121\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 27%|██▋       | 26950/100000 [08:09<22:06, 55.07it/s]\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/278725407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_neuron\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_at_every_nth_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_23/1581440720.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, epochs, block_size, embedding_size, hidden_neuron, bs, lr, parameters, lambdas, enable_print, print_at_every_nth_epoch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.112903356552124,\n tensor(2.1565, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 2nd try"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 300000/300000 [1:28:47<00:00, 56.31it/s]\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 300_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.01, parameters=parameters, enable_print=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.1061928272247314,\n tensor(2.1500, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 3rd try"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10000/10000 [02:57<00:00, 56.38it/s]\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 10_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=1, parameters=parameters, enable_print=True, print_at_every_nth_epoch=1000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.125706911087036,\n tensor(2.2161, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 4th try"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 12/10000 [00:00<03:03, 54.49it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 2.166624069213867\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 10%|█         | 1008/10000 [00:17<02:38, 56.77it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1000 2.0816526412963867\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 20%|██        | 2010/10000 [00:35<02:20, 57.06it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2000 2.0698986053466797\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 30%|███       | 3012/10000 [00:53<02:03, 56.78it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3000 2.085846424102783\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 40%|████      | 4008/10000 [01:10<01:45, 57.01it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4000 2.0758490562438965\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 50%|█████     | 5010/10000 [01:28<01:33, 53.26it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5000 2.085636615753174\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 60%|██████    | 6012/10000 [01:46<01:10, 56.89it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6000 2.076601505279541\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 70%|███████   | 7008/10000 [02:04<00:52, 56.62it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "7000 2.0770840644836426\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 80%|████████  | 8010/10000 [02:21<00:35, 56.16it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "8000 2.0789082050323486\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 90%|█████████ | 9012/10000 [02:39<00:17, 56.72it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "9000 2.0974490642547607\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10000/10000 [02:57<00:00, 56.49it/s]\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 10_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, parameters=parameters, enable_print=True, print_at_every_nth_epoch=1000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.085941791534424,\n tensor(2.1405, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 5th try"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 7/100000 [00:00<27:16, 61.12it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 2.070305109024048\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 10%|█         | 10011/100000 [02:57<27:57, 53.65it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.0962393283843994\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 20%|██        | 20012/100000 [05:54<23:27, 56.84it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.0833041667938232\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 30%|███       | 30008/100000 [08:50<20:31, 56.83it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "30000 2.074430465698242\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 40%|████      | 40008/100000 [11:47<17:32, 56.99it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "40000 2.087279796600342\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 50%|█████     | 50010/100000 [14:45<14:36, 57.01it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "50000 2.0869252681732178\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 60%|██████    | 60008/100000 [17:42<11:40, 57.07it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "60000 2.0887160301208496\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 70%|███████   | 70010/100000 [20:39<08:49, 56.67it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "70000 2.097712755203247\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 80%|████████  | 80010/100000 [23:36<05:50, 57.03it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "80000 2.0827200412750244\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 90%|█████████ | 90008/100000 [26:33<02:55, 57.04it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "90000 2.0914275646209717\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 100000/100000 [29:30<00:00, 56.50it/s]\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.01, parameters=parameters, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.063863515853882,\n tensor(2.1395, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test Loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2.063863515853882,\n tensor(2.1439, device='cuda:0', grad_fn=<NllLossBackward0>))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss, evaluate_loss(parameters, Xte.cuda(), Yte.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## E02\n- Weight Initialization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "(1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? \n\n(2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Answer to (1)\n\nIf the predicted probabilities were uniform then the probabilities would have been `1/27` of each character prediction\n\nAnd we would have take the log of the probability which would have been"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-3.2958)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "torch.tensor(1/27).log()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "to the get the loss it would have been"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3.2958)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "- torch.tensor(1/27).log()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "No we sum up the losses and divide by the count, `(n * (3.2958))/n`\nwhich is equal to `3.2958`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets see the initial loss when we train the model with current initialization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.59it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 18.98647117614746\n1 18.139089584350586\n2 17.4639949798584\n3 16.95638084411621\n4 16.41069984436035\n5 16.088415145874023\n6 15.858123779296875\n7 15.584877967834473\n8 15.392867088317871\n9 14.91295051574707\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 10, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The initial loss is `18.98` which is high comparative to `3.2958`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets see the probabilities of the output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 1/1 [00:00<00:00, 45.83it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 17.784887313842773\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "parameters, loss = train(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_probs(parameters, X, block_size=3, embedding_size=50):\n    C, W1, b1, W2, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    return F.softmax(logits, dim=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0857e-04, 3.6501e-02, 2.1932e-06,  ..., 2.3620e-11, 3.5701e-07,\n         1.1134e-09],\n        [4.6675e-07, 3.9523e-03, 2.2363e-13,  ..., 9.9556e-17, 1.7200e-13,\n         1.9502e-14],\n        [1.1533e-03, 3.5033e-05, 1.8715e-15,  ..., 1.2193e-10, 1.2248e-08,\n         6.8308e-16],\n        ...,\n        [5.5263e-07, 4.1990e-03, 2.8210e-11,  ..., 1.3669e-13, 7.7488e-08,\n         4.2720e-17],\n        [8.9470e-06, 6.5655e-03, 6.6855e-04,  ..., 5.5212e-08, 6.7759e-13,\n         1.9382e-04],\n        [3.5907e-07, 2.2142e-07, 4.3963e-06,  ..., 1.0096e-06, 9.9789e-01,\n         1.2103e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "compute_probs(parameters, Xtr)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets view a single row of probabilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.0857e-04, 3.6501e-02, 2.1932e-06, 1.3373e-08, 1.1020e-10, 5.3793e-03,\n        3.0107e-08, 2.6294e-14, 9.6476e-12, 1.1091e-11, 1.0275e-08, 1.0968e-10,\n        3.7358e-05, 8.2368e-08, 3.6925e-07, 1.4491e-08, 2.4197e-08, 1.3568e-12,\n        4.5940e-06, 1.9727e-07, 8.8267e-05, 1.2796e-02, 1.9823e-04, 9.4488e-01,\n        2.3620e-11, 3.5701e-07, 1.1134e-09], device='cuda:0',\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "compute_probs(parameters, Xtr)[0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "to get a uniform probability, I think we need to have all logits as equal so that we can get probability of each as `1/27`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Try 1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "lets try uniform wieght initialization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_v2(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g).cuda()\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.rand(hidden_neuron, generator=g).cuda()\n        W2 = torch.rand((hidden_neuron, 27), generator=g).cuda()  \n        b2 = torch.rand(27, generator=g).cuda()\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 1/1 [00:00<00:00, 46.14it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 6.422854900360107\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "parameters, loss = train_v2(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "With uniform weight initialization the intial loss (`6.422`) obtained is less than of normal weight initialization (`17.7`)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Try 2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets initialize the last layers of weights and biases as zero."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_v3(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g).cuda()\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.rand(hidden_neuron).cuda()\n        W2 = torch.zeros((hidden_neuron, 27)).cuda()  \n        b2 = torch.zeros(27).cuda()\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 1/1 [00:00<00:00, 46.01it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.295814037322998\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "parameters, loss = train_v3(Xtr, Ytr, 1, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The initial loss is now `3.2958` (which we wanted). "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets see how well it trains now"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 12/30000 [00:00<09:14, 54.09it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.295814037322998\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 33%|███▎      | 10008/30000 [03:03<06:04, 54.85it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.8334920406341553\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 67%|██████▋   | 20009/30000 [06:08<03:04, 54.04it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.827484369277954\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 30000/30000 [09:11<00:00, 54.35it/s]\n"
    }
   ],
   "source": "parameters, loss = train_v3(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.8184070587158203"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Try 3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As we can see the losses are not decreasing faster, lets not initialize weight to zero but close to zero and see ..."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_v4(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.rand((27, embedding_size), generator=g).cuda()\n        W1 = torch.rand((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.rand(hidden_neuron).cuda()\n        W2 = torch.rand((hidden_neuron, 27)).cuda() * 0.01 # close to zero\n        b2 = torch.zeros(27).cuda()\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 6/30000 [00:00<08:38, 57.86it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.2842519283294678\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 33%|███▎      | 10008/30000 [03:01<05:57, 55.96it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.810333013534546\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 67%|██████▋   | 20010/30000 [06:01<03:02, 54.85it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.8224377632141113\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 30000/30000 [09:05<00:00, 55.03it/s]\n"
    }
   ],
   "source": "parameters, loss = train_v4(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Try 4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Lets not try to uniformly initiate all the weights but only the last layers and the rest we can keep as normal initialized"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_v5(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g).cuda()\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.randn(hidden_neuron).cuda()\n        W2 = torch.rand((hidden_neuron, 27)).cuda() * 0.01 # close to zero\n        b2 = torch.zeros(27).cuda()\n        parameters = [C, W1, b1, W2, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 7/30000 [00:01<58:14,  8.58it/s]  "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.2948546409606934\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 33%|███▎      | 10008/30000 [02:54<05:42, 58.38it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.190589427947998\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 67%|██████▋   | 20010/30000 [05:48<02:53, 57.62it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.175851821899414\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 30000/30000 [08:42<00:00, 57.39it/s]\n"
    }
   ],
   "source": "parameters, loss = train_v5(Xtr, Ytr, 30_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The losses are reducing now. Lets train for 100_000 and check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 12/100000 [00:00<29:50, 55.84it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.297055721282959\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 10%|█         | 10009/100000 [02:58<26:48, 55.94it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.2041165828704834\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 20%|██        | 20008/100000 [05:55<23:44, 56.16it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.1607794761657715\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 30%|███       | 30010/100000 [08:49<20:05, 58.04it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "30000 2.153400182723999\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 40%|████      | 40012/100000 [11:42<17:13, 58.07it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "40000 2.128110408782959\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 50%|█████     | 50008/100000 [14:34<14:20, 58.13it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "50000 2.111949920654297\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 60%|██████    | 60008/100000 [17:27<11:28, 58.10it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "60000 2.116779327392578\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 70%|███████   | 70009/100000 [20:19<08:36, 58.10it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "70000 2.094712972640991\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 80%|████████  | 80011/100000 [23:12<05:43, 58.23it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "80000 2.095874309539795\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 90%|█████████ | 90008/100000 [26:04<02:56, 56.70it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "90000 2.104464530944824\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 100000/100000 [28:59<00:00, 57.47it/s]\n"
    }
   ],
   "source": "parameters, loss = train_v5(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.0862724781036377"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The losses are getting reduced faster!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.1515, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "evaluate_loss(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.1520, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "evaluate_loss(parameters, Xte.cuda(), Yte.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## E03"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Read the Bengio et al 2003 paper, implement and try any idea from the paper. Did it work?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In the paper there is a mention of direct connection from the word features to output. \n\nLets implement the direct connection from embedding to output and see the results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Direct connection from embedding to output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "C = torch.randn((27, 50), generator=g).cuda()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([228146, 150])"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "C[X].shape; C[X].view(-1, 150).shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_loss_dir_conn(parameters, X, Y, block_size=3, embedding_size=10):\n    C, W1, b1, W2, W3, b2 = parameters\n    emb = C[X]\n    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1)\n    logits = h @ W2 + b2 + C[X].view(-1, block_size * embedding_size) @ W3\n    loss = F.cross_entropy(logits.cuda(), Y)\n    return loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_dir_conn(X, \n          Y, \n          epochs, \n          block_size=3, \n          embedding_size=10, \n          hidden_neuron=300, \n          bs=32, \n          lr=0.1, \n          parameters=[], \n          lambdas = [0, 0, 0],\n          enable_print=True,\n          print_at_every_nth_epoch=10000\n         ):\n    \n    if not parameters:\n        C = torch.randn((27, embedding_size), generator=g).cuda()\n        W1 = torch.randn((block_size * embedding_size, hidden_neuron), generator=g).cuda()\n        b1 = torch.randn(hidden_neuron).cuda()\n        W2 = torch.rand((hidden_neuron, 27)).cuda() * 0.01 # close to zero\n        W3 = torch.rand((block_size * embedding_size, 27)).cuda() * 0.01 # close to zero\n        b2 = torch.zeros(27).cuda()\n        parameters = [C, W1, b1, W2, W3, b2]\n\n    \n    for p in parameters: p.requires_grad = True \n        \n    for epoch in tqdm(range(epochs)):\n            \n        ix = torch.randint(0, X.shape[0], (bs, )).cuda()\n\n        loss = evaluate_loss_dir_conn(parameters, X[ix].cuda(), Y[ix].cuda(), block_size, embedding_size)\n        regularization_loss = _regularization_loss(parameters, lambdas)\n        loss += regularization_loss\n\n        for p in parameters:\n            p.grad= None\n        loss.backward()\n\n\n        for p in parameters:\n            p.data += - lr * p.grad\n\n        if enable_print and epoch % print_at_every_nth_epoch == 0: print(epoch, loss.item())\n    \n    return parameters, loss.item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 4/100000 [00:00<51:53, 32.11it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 3.295320987701416\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 10%|█         | 10006/100000 [05:20<47:52, 31.33it/s] "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000 2.150477647781372\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 20%|██        | 20005/100000 [10:40<42:17, 31.53it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20000 2.124218702316284\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 30%|███       | 30006/100000 [16:01<37:17, 31.29it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "30000 2.0882301330566406\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 40%|████      | 40006/100000 [21:21<31:49, 31.41it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "40000 2.1032118797302246\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 50%|█████     | 50007/100000 [26:41<27:06, 30.74it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "50000 2.0811822414398193\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 60%|██████    | 60006/100000 [32:01<21:20, 31.23it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "60000 2.0820119380950928\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 70%|███████   | 70007/100000 [37:22<16:01, 31.18it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "70000 2.064924478530884\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 80%|████████  | 80005/100000 [42:42<10:33, 31.57it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "80000 2.0588784217834473\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 90%|█████████ | 90005/100000 [48:02<05:15, 31.69it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "90000 2.078995704650879\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 100000/100000 [53:26<00:00, 31.19it/s]\n"
    }
   ],
   "source": "parameters, loss = train_dir_conn(Xtr, Ytr, 100_000, block_size=3, embedding_size=50, hidden_neuron=100, bs=16384, lr=0.1, enable_print=True, print_at_every_nth_epoch=10_000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.0455985069274902"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.1238, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "evaluate_loss_dir_conn(parameters, Xdev.cuda(), Ydev.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.1266, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "evaluate_loss_dir_conn(parameters, Xte.cuda(), Yte.cuda(), block_size=3, embedding_size=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The loss decreased by lot with this direct connection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Mixing NN output with the trigram model output\n> Trigram model is the statistical model I implemented in the Lesson's 2 exercise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
